---
title: "编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂"
date: 2025-09-13T00:00:03+08:00
weight: 1
math: true
# draft: true
show_reading_time: true
show_bread_crumbs: true
show_post_nav_links: false # the prev/next after the content
show_code_copy_buttons: true
show_word_count: true
---

# 编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂

最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。

长久以来，我们追求着一个宏大目标：构建一个**统一的多模态模型**，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。

但现实却不尽人意。**理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。** 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。

这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。

今天，我们想分享一个令人兴奋的发现。**我们找到了这样一种催化剂**，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，**将经典的分割任务，重新定义为一次图像编辑**，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。

---

## 困局：16%的分割得分与失控的生成

在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：**生成式分割**。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。

![图示说明：根据指令进行分割。](https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*acrPSp-7qM8AAAAAgCAAAAgAevzJAQ/original)

结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 **16%** 上下。

我们分析，根本原因在于**数据分布的巨大鸿沟**。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。

我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。

---

## 灵感迸发：让分割“穿上色彩的外衣”

我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？

答案显然是后者。

我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是**将分割任务转换成一个色彩编辑任务**。

例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。

![图示说明：左侧为原图香蕉，从生成抽象的黑白掩-码（中），到直接在原图上进行色彩编辑（右三）。这个转换让任务的数据分布回归到了自然图像领域。](https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*-_O6RLOxXKcAAAAAgBAAAAgAevzJAQ/original)

这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。

- **对“理解”的促进**：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。
- **对“创造”的释放**：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。

“左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。

---

## 效果惊人：从16%到72.4%，以及更可控的编辑能力

当我们用这种新方法重新训练模型后，结果超出了所有人的预期。

### 1. SoTA级别的分割能力

首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 **72.4%**！这是一个超过 **350%** 的相对提升。

指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。

![图示说明：我们的模型（右）精准定位并分割了目标主体，Qwen-Image（左二）未能准确定位要分割的目标，Nano-banana（左三）则未能准确分割男士的头部，以及分割的边缘线不够贴合。](https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*koynTZD5vO8AAAAAgDAAAAgAevzJAQ/original)

![图示说明：这个case的指令“please segment the girl with red mask”, 我们的模型（右）精准定位并分割了目标主体，Qwen-Image（左二）未能分割脚部，Nano-banana（左三）则改变了主体尺寸。](https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*5C7KTbk2WZ0AAAAAgBAAAAgAevzJAQ/original)

在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。

在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：

<!-- ![Ming-Lite-Omni1.5 vs Qwen-Image-Edit 差分对比](占位符：请在这里替换为您的图示链接) -->

评估结果显示，我们的模型在分割任务中的表现已达到与专为分割设计的专业模型相当的水平。其中，Qwen-Image-Edit因评估指标明显较低，仅在每个测试子集上随机采样500个样本进行评估。

| 模型类别 | 模型名称 | RefCOCO (val) | RefCOCO+ (val) | RefCOCOg (val) |
| :--- | :--- | :---: | :---: | :---: |
| **Vision Specialist**<br>(专用视觉分割模型) | VLT | 67.5 | 56.3 | 55.0 |
| | CRIS | 70.5 | 62.3 | 59.9 |
| | LAVT | 72.7 | 62.1 | 61.2 |
| | PolyFormer-B | 74.8 | 67.6 | 67.8 |
| **MLLM + SAM**<br>(专用的分割模型) | LISA-7B | 74.1 | 62.4 | 66.4 |
| | PixelLM-7B | 73.0 | 66.3 | 69.3 |
| **MLLM + DiT**<br>(生成式模型做分割) | Qwen-Image-Edit* | 30.3 | 28.8 | 34.0 |
| | **Ming-Lite-Omni1.5** | **72.4** | **62.8** | **64.3** |

### 2. 更精准、更可控的编辑能力

这个方法的魅力在于，它不仅治好了分割的“短板”，还反过来极大地增强了模型的通用编辑能力。

因为模型在成千上万次“精确上色”的练习中，学会了对边界前所未有的尊重。这种对细粒度控制的“肌肉记忆”迁移到了所有编辑任务中。我们的编辑精度可控性指标，在背景改变、颜色修改和材质修改等子项上，均分从7.69提升到8.12。

![图示说明：指令为“消除图中最右侧男士的领结”。我们的模型（右）精准地移除了目标领结，同时保持了背景马匹等元素的一致性。Qwen（左二）错误地移除了多个领结，且马匹和老虎出现了不一致。Nano-banana（左三）同样在衣服款式的一致性和老虎斑纹的一致性上表现不佳。](https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*7PgiRpiJyScAAAAAgCAAAAgAevzJAQ/original)

### 3. 身份的一致性保持

在人像编辑中，一个核心痛点是身份（ID）一致性。我们的模型在这方面也表现出色。无论是改变发型，还是调整表情，模型都能很好地保持人物的核心特征。

**指令：“头转向左侧”**
- Qwen（左）的ID、肤色存在不一致现象。
- Nano-banana（中）人物额头与背景处的行人均发生了改变。
- 我们的模型（右）在转动头部的同时，很好地保持了主体和背景的一致性。

**指令：“微笑”**
- Qwen（左）表情变化的同时人物ID也发生了改变。
- Nano-banana（中）在换表情的同时手部动作出先畸变。
- 我们的模型（右）很好地遵循了指令，同时保持了ID一致性。

**指令：“变换背景”**
- Qwen（左）的ID一致性明显下降，看起来像换了一个人。
- Nano-banana（中）人物ID保持的不错，但画面结构产生了较大差异。
- 我们的模型（右）在准确地更换背景的同时，很好地保持了ID、外表的一致性。

![ID一致性对比图](https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*19ULQZrBWIAAAAAAd5AAAAgAevzJAQ/original)

<!-- **更多一致性 Case：**

<video src="https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/TptZRJDixVUAAAAAhqAAAAgADkliAQFr" width="540px" height="800px" controls></video> -->

<!-- ![更多一致性案例](占位符：请在这里替换为您的图示链接) -->

---

## 诚实的审视：我们的不足与未来方向

尽管取得了令人鼓舞的进展，但我们深知模型仍有很大的提升空间。特别是在以下几个方面：

- **大幅度的动作改变**：实现从站立到奔跑这样的大姿态变换，仍然是一个巨大的挑战。
- **复杂指令的跟随能力**：对于包含多个步骤或条件的复杂指令，模型的理解和执行能力还有待加强。
- **指令多样性的支持**：扩展模型能理解和执行的指令类型，是我们下一步的重点工作。

---

## 结语：寻找下一个“催化剂”

从16%到72.4%，这个故事的核心并非某个复杂的网络结构或海量的新数据，而是一个关于**“任务设计”**的尝试。

我们证明了，与其试图用“胶水”把AI的各种能力勉强粘合在一起，不如去寻找或设计那些本身就是“一体两面”的协同任务。这些任务就像催化剂，能让不同的能力在解决同一个问题的过程中，自然而然地相互促进、共同进化。

“分割即编辑”只是第一个成功的尝试。我们相信，在3D理解、视频生成等更广阔的领域，还隐藏着更多这样的“催化剂”等待我们去发现。

**AI的“左手”与“右手”，终于学会了如何优雅地击掌。而这，仅仅是交响乐的序章。**
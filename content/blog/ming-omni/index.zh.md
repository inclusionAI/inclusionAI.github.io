---
title: "Ming-Omniï¼šä¸€ä¸ªç”¨äºæ„ŸçŸ¥ä¸ç”Ÿæˆçš„ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹"
date: 2025-06-11T00:00:03+08:00
weight: 1
# aliases: ["/first"]
# tags: ["Research"]
# draft: true
# comments: false
# description: "Desc Text."
# disable_share: false
# hide_meta: false
# hide_summary: false # to hide summary in list
# hide_footer: false
math: true
# search_hidden: false # to hide from search page
show_reading_time: true
show_bread_crumbs: true
show_post_nav_links: false # the prev/next after the content
show_code_copy_buttons: true
show_word_count: true
# use_hugo_toc: true
# show_toc: true
# toc_open: true # default expand all
# cover:
#     image: "path"
#     # can also paste direct link from external site
#     # ex. https://i.ibb.co/K0HVPBd/paper-mod-profilemode.png
#     alt: "<alt text>"
#     caption: "<text>"
#     relative: true # To use relative path for cover image, used in hugo Page-bundles
#     responsive_images: true
#     hidden: false
# header:
#   background: "" # background css value
#   background_image: ""
#   gradient: false
#   blur: false
---

<!-- # Ming-Lite-Omni -->

{{< button href="https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify" label="GITHUB" external=true >}} ğŸ“‘ <a href="https://arxiv.org/abs/2506.09344">Technical Report</a>ï½œğŸ“–<a href="https://lucaria-academy.github.io/Ming-Omni/">Project Page</a> ï½œğŸ¤— <a href="https://huggingface.co/inclusionAI/Ming-Lite-Omni">Hugging Face</a>ï½œ ğŸ¤– <a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni">ModelScope</a>

## ä»‹ç»

Ming-lite-omni æ˜¯ Ming-omni çš„è½»é‡ç‰ˆï¼Œæºè‡ª [Ling-lite](https://github.com/inclusionAI/Ling)ï¼Œæ‹¥æœ‰ 28 äº¿æ¿€æ´»å‚æ•°ã€‚Ming-lite-omni æ˜¯ä¸€ä¸ªç»Ÿä¸€çš„å¤šæ¨¡æ€æ¨¡å‹ï¼Œèƒ½å¤Ÿå¤„ç†å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘å’Œè§†é¢‘ï¼Œå¹¶åœ¨è¯­éŸ³å’Œå›¾åƒç”Ÿæˆæ–¹é¢è¡¨ç°å‡ºè¾ƒå¼ºèƒ½åŠ›ã€‚Ming-lite-omni ä½¿ç”¨ä¸“ç”¨ç¼–ç å™¨ä»ä¸åŒæ¨¡æ€æå– tokenï¼Œç„¶åç”± Ling å¤„ç†ï¼ŒLing æ˜¯ä¸€ä¸ª MoE æ¶æ„ï¼Œé…å¤‡äº†æ–°æå‡ºçš„æ¨¡æ€ä¸“ç”¨è·¯ç”±å™¨ã€‚è¯¥è®¾è®¡ä½¿å•ä¸€æ¨¡å‹èƒ½åœ¨ç»Ÿä¸€æ¡†æ¶å†…é«˜æ•ˆå¤„ç†å’Œèåˆå¤šæ¨¡æ€è¾“å…¥ï¼Œä»è€Œæ”¯æŒå¤šæ ·åŒ–ä»»åŠ¡ï¼Œæ— éœ€ä½¿ç”¨å¤šä¸ªæ¨¡å‹ã€ä»»åŠ¡ä¸“ç”¨å¾®è°ƒæˆ–ç»“æ„æ”¹åŠ¨ã€‚é‡è¦çš„æ˜¯ï¼ŒMing-lite-omni è¶…è¶Šä¼ ç»Ÿå¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒéŸ³é¢‘å’Œå›¾åƒç”Ÿæˆã€‚é€šè¿‡é›†æˆå…ˆè¿›çš„éŸ³é¢‘è§£ç å™¨å®ç°è‡ªç„¶è¯­éŸ³ï¼Œä»¥åŠåˆ©ç”¨ Ming-Lite-Uni å®ç°é«˜è´¨é‡å›¾åƒç”Ÿæˆï¼Œæ¨¡å‹è¿˜èƒ½è¿›è¡Œä¸Šä¸‹æ–‡æ„ŸçŸ¥èŠå¤©ã€æ–‡æœ¬è½¬è¯­éŸ³åŠå¤šåŠŸèƒ½å›¾åƒç¼–è¾‘ã€‚æˆ‘ä»¬çš„å®éªŒç»“æœè¡¨æ˜ï¼ŒMing-lite-omni åœ¨æ‰€æœ‰æ¨¡æ€ä¸Šçš„ç»Ÿä¸€æ„ŸçŸ¥ä¸ç”Ÿæˆæ–¹é¢æä¾›äº†å¼ºå¤§è§£å†³æ–¹æ¡ˆã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒMing-lite-omni æ˜¯æˆ‘ä»¬æ‰€çŸ¥é¦–ä¸ªæ¨¡æ€æ”¯æŒä¸ GPT-4o åŒ¹é…çš„å¼€æºæ¨¡å‹ï¼Œä¸”æˆ‘ä»¬å‘å¸ƒäº†å…¨éƒ¨ä»£ç å’Œæ¨¡å‹æƒé‡ï¼Œä»¥ä¿ƒè¿›ç¤¾åŒºè¿›ä¸€æ­¥ç ”ç©¶å’Œå‘å±•ã€‚

## ğŸ“Œ æ›´æ–°

* [2025.06.12] ğŸ”¥ æˆ‘ä»¬çš„[æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2506.09344)å·²å…¬å¼€å‘å¸ƒäº arxivã€‚
* [2025.05.28] ğŸ”¥ Ming-lite-omni å®˜æ–¹ç‰ˆæœ¬å‘å¸ƒï¼Œæ€§èƒ½æ›´ä½³å¹¶æ”¯æŒå›¾åƒç”Ÿæˆã€‚
* [2025.05.04] ğŸ”¥ å‘å¸ƒ Ming-lite-omni æµ‹è¯•ç‰ˆæœ¬ï¼š[Ming-lite-omni-Preview](https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview)ã€‚

## ä¸»è¦ç‰¹æ€§

- **ç»Ÿä¸€å…¨æ¨¡æ€æ„ŸçŸ¥**ï¼šMing-lite-omni åŸºäº [Ling](https://github.com/inclusionAI/Ling)ï¼ˆä¸€ä¸ª MoE æ¶æ„çš„å¤§è¯­è¨€æ¨¡å‹ï¼‰ï¼Œé€šè¿‡æ¨¡æ€ä¸“ç”¨è·¯ç”±å™¨è§£å†³ä»»åŠ¡å†²çªï¼Œç¡®ä¿æ¥è‡ªä¸åŒæ¨¡æ€çš„ token çš„è¿è´¯èåˆã€‚

- **ç»Ÿä¸€æ„ŸçŸ¥ä¸ç”Ÿæˆ**ï¼šMing-lite-omni å®ç°ç»Ÿä¸€çš„ç†è§£ä¸ç”Ÿæˆï¼Œä½¿æ¨¡å‹åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­èƒ½è§£è¯»å¤šæ¨¡æ€æŒ‡ä»¤å’Œç”¨æˆ·æ„å›¾ï¼Œä»è€Œæå‡ç”Ÿæˆè´¨é‡å¹¶å¢å¼ºå¤šä»»åŠ¡ä½¿ç”¨ä¾¿åˆ©æ€§ã€‚

- **åˆ›æ–°çš„ç”Ÿæˆèƒ½åŠ›**ï¼šMing-lite-omni èƒ½æ„ŸçŸ¥æ‰€æœ‰æ¨¡æ€ï¼ŒåŒæ—¶ç”Ÿæˆé«˜è´¨é‡æ–‡æœ¬ã€å®æ—¶è¯­éŸ³å’Œç”ŸåŠ¨å›¾åƒï¼Œå±•ç°å‡ºå“è¶Šçš„è·¨æ¨¡æ€è¡¨ç°ï¼Œæ¶µç›–å›¾åƒæ„ŸçŸ¥ã€è§†å¬äº¤äº’å’Œå›¾åƒç”Ÿæˆç­‰å¤šæ ·ä»»åŠ¡ã€‚

## è¯„æµ‹

Ming-lite-omni åœ¨å›¾åƒæ„ŸçŸ¥ã€è§†å¬äº¤äº’åŠå›¾åƒç”Ÿæˆä»»åŠ¡ä¸­å‡å±•ç°å‡ºä¼˜å¼‚çš„è·¨æ¨¡æ€æ€§èƒ½ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å›¾åƒæ„ŸçŸ¥ä»»åŠ¡ä¸­ï¼ŒMing-lite-omni ä»…æ¿€æ´» 28 äº¿å‚æ•°ï¼Œæ€§èƒ½å·²å¯ä¸ Qwen2.5-VL-7B ç›¸åª²ç¾ã€‚å®ƒåœ¨ç«¯åˆ°ç«¯è¯­éŸ³ç†è§£å’ŒæŒ‡ä»¤æ‰§è¡Œä¸Šè¡¨ç°ä¼˜äº Qwen2.5-Omni å’Œ Kimi-Audioã€‚åŒæ—¶æ”¯æŒåŸç”Ÿåˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆã€ç¼–è¾‘åŠé£æ ¼è¿ç§»ï¼ŒGenEval å¾—åˆ†è¾¾ 0.64ï¼Œä¼˜äºä¸»æµæ¨¡å‹å¦‚ SDXLã€‚åœ¨ FID æŒ‡æ ‡ä¸Šï¼ŒMing-lite-omni è¾¾åˆ° 4.85ï¼Œåˆ·æ–°äº†ç°æœ‰æ–¹æ³•çš„æœ€ä½³æ°´å¹³ã€‚



### Image benchmark
<div align="center">

| Benchmarks        | Ming-lite-omni |    Qwen2.5-VL-7B-Instruct    | InternVL2.5-8B-MPO |
|:------------------|:--------------:|:----------------------------:|:------------------:|
| AI2D              |      83.1      |             84.4             |    <b>84.5</b>     |
| HallusionBench    |  <b>55.0</b>   |             55.8             |        51.7        |
| MMBench_TEST_V11  |      80.8      |         <b>82.8</b>          |        82.0        |
| MMMU              |      56.3      |         <b>56.6</b>          |        54.8        |
| MMStar            |      64.7      |             65.3             |    <b>65.2</b>     |
| MMVet             |      71.3      |             71.6             |        68.1        |
| MathVista         |  <b>71.6</b>   |             68.1             |        67.9        |
| OCRBench          |  <b>88.4</b>   |             87.8             |        88.2        |
| Average           |      71.4      |         <b>71.5</b>          |        70.3        |

</div>


#### Encyclopedia Benchmarks  
<div align="center">

| Object Recognition   | Ming-lite-omni |  Qwen2.5-VL-7B-Instruct  |
|:---------------------|:--------------:|:------------------------:|
| Plants               |   **54.96**    |           47.8           |
| Animals              |    **56.7**    |          50.85           |
| Vehicles             |     41.91      |        **42.29**         |
| Food & Ingredients   |   **62.28**    |          54.09           |
| Dishes               |    **44.3**    |          39.07           |
| General              |     91.08      |        **92.42**         |
| Average              |   **58.54**    |          54.43           |

</div>

### Video benchmark

<div align="center">

| Benchmarks              | Ming-lite-omni | Qwen2.5VL-7B-Instruct |
|:------------------------|:--------------:|:---------------------:|
| VideoMME                |      67.0      |      <b>67.3</b>      |
| MVBench                 |      67.7      |      <b>67.4</b>      |
| Video-MMMU              |      46.3      |      <b>47.4</b>      |
| LongVideoBench          |      56.6      |         54.7          |
| Average                 |  <b>59.4</b>   |         59.2          |

</div>
Note: All models are evaluated based on 128 uniformly sampled frames.

### Audio benchmark
#### SpeechQA

<div align="center">

| Model            |    Average    | AlpacaEval  | CommonEval  |    SD-QA     |     MMSU     |  OpenBookQA  |    IFEval    |   AdvBench    |
|:-----------------|:-------------:|:-----------:|:-----------:|:------------:|:------------:|:------------:|:------------:|:-------------:|
| Qwen2-Audio-chat |     3.545     |    3.69     |    3.40     |    35.35     |    35.43     |    49.01     |    22.57     |     98.85     |
| Baichuan-Audio   |     3.695     |    4.00     |    3.39     |    49.64     |    48.80     |    63.30     |    41.32     |     86.73     |
| GLM-4-Voice      |     3.77      |    4.06     |    3.48     |    43.31     |    40.11     |    52.97     |    24.91     |     88.08     |
| Kimi-Audio       |     4.215     |    4.46     |    3.97     | <b>63.12</b> | <b>62.17</b> | <b>83.52</b> | <b>61.10</b> | <b>100.00</b> |
| Qwen2.5-Omni     |     4.21      |    4.49     |    3.93     |    55.71     |    61.32     |    81.10     |    52.87     |     99.42     |
| Ming-lite-omni   |  <b>4.34</b>  | <b>4.63</b> | <b>4.06</b> |    58.84     |    47.53     |    61.98     |    58.36     |     99.04     |
</div>

#### ASR

<div align="center">

|     Model      | aishell1 | aishell2_android | aishell2_ios | cv15_zh  | fleurs_zh | wenetspeech_meeting | wenetspeech_net | librispeech_test_clean | librispeech_test_other | multilingual_librispeech | cv15_en  | fleurs_en |  voxpopuli_v1.0_en   |
|:--------------:|:--------:|:----------------:|:------------:|:--------:|:---------:|:-------------------:|:---------------:|:----------------------:|:----------------------:|:------------------------:|:--------:|:---------:|:--------------------:|
| Ming-lite-omni |   1.47   |     **2.55**     |   **2.52**   |   6.31   |   2.96    |        5.95         |      5.46       |          1.44          |          2.80          |         **4.15**         | **6.89** | **3.39**  |       **5.80**       |
|  Qwen2.-Omni   |   1.18   |       2.75       |     2.63     | **5.20** |   3.00    |      **5.90**       |      7.70       |          1.80          |          3.40          |           7.56           |   7.60   |   4.10    |       **5.80**       |
|  Qwen2-Audio   |   1.53   |       2.92       |     2.92     |   6.90   |   7.50    |        7.16         |      8.42       |          1.60          |          3.60          |           5.40           |   8.60   |   6.90    |         6.84         |
|   Kimi-Audio   | **0.60** |       2.64       |     2.56     |   7.21   | **2.69**  |        6.28         |    **5.37**     |        **1.28**        |        **2.42**        |           5.88           |  10.31   |   4.44    |         7.97         |

</div>



### Information-Seeking Benchmark
<div align="center">

| Model          | InfoSeek_H-mean | InfoSeek_unseen_question | InfoSeek_unseen_entity |
|:---------------|:---------------:|:------------------------:|:----------------------:|
| GPT-4o         |  <b>36.05</b>   |            -             |           -            |
| PaLI-X         |      22.06      |           23.5           |          20.8          |
| Qwen2.5-vl-32B |      19.35      |          20.55           |         18.28          |
| Ming-lite-omni |      27.7       |         **30.4**         |        **25.4**        |
</div>



### OCR
<div align="center">

| Model              | Ming-lite-omni | Qwen2.5-VL-7B-Instruct  |
|:-------------------|:--------------:|:-----------------------:|
| ChartQA_TEST       |      85.1      |       <b>87.3</b>       |
| DocVQA_TEST        |       93       |       <b>95.7</b>       |
| OCRBenchV2_en/zh   |    53.3/52     |    <b>56.3/57.2</b>     |
| OmniDocBenchâ†“      | 34/<b>34.4</b> |    <b>30.8</b>/39.8     |
| TextVQA_VAL        |      82.8      |       <b>84.9</b>       |
</div>

### GUI
<div align="center">

| Model                      | Ming-lite-omni | InternVL3 8B | Qwen2.5-VL-7B-Instruct | 
|:---------------------------|:--------------:|:------------:|:----------------------:|
| ScreenSpot                 |  <b>82.1</b>   |     79.5     |         78.9*          |
| ScreenSpot-V2              |  <b>84.1</b>   |     81.4     |           -            |
| AITZ(EM)                   |  <b>66.6</b>   |      -       |         57.6*          |
</div>
Note: * denotes the reproduced results.



### Unified Generation Benchmark

<div align="center">

| Model          | single_object | two_object |  counting  |  colors  | position | color_attr | GENEVAL  | DPGBench  |     FIDâ†“      |
|:---------------|:-------------:|:----------:|:----------:|:--------:|:--------:|:----------:|:--------:|:---------:|:-------------:|
| Ming-lite-omni |  **0.9875**   | **0.7727** | **0.6812** |  0.7872  |   0.31   |    0.29    | **0.64** |   81.72   |   **4.85**    |
| Metaquery-XL   |       -       |     -      |     -      |    -     |    -     |     -      |   0.61   | **82.05** |     6.02      |
| SDv2.1         |     0.98      |    0.51    |    0.44    | **0.85** |   0.07   |    0.17    |   0.50   |   68.09   |     26.96     |
| Emu3-Gen       |     0.98      |    0.71    |    0.34    |   0.81   |   0.17   |    0.21    |   0.54   |   80.60   |       -       |
| SDXL           |     0.98      |    0.74    |    0.39    | **0.85** |   0.15   |    0.23    |   0.55   |   74.65   |     8.76      |
| Janus          |     0.97      |    0.68    |    0.30    |   0.84   | **0.46** |  **0.42**  |   0.61   |   79.68   |     10.10     |
| JanusFlow      |       -       |     -      |     -      |    -     |    -     |     -      |   0.63   |   80.09   |     9.51      |

</div>

Please refer to our technical report for more comprehensive evaluation results. 

## æ¨¡å‹ä¸‹è½½

æ‚¨å¯ä»¥ä» Huggingface å’Œ ModelScope ä¸¤ä¸ªå¹³å°ä¸‹è½½æ¨¡å‹ã€‚

<div align="center">

| **æ¨¡å‹**        |   **è¾“å…¥æ¨¡æ€**          | **è¾“å‡ºæ¨¡æ€**     |                                                                     **ä¸‹è½½åœ°å€**                                                                     |
|:---------------| :---------------------: | :---------------: |:----------------------------------------------------------------------------------------------------------------------------------------------------:|
| Ming-Lite-Omni | å›¾åƒã€æ–‡æœ¬ã€è§†é¢‘ã€éŸ³é¢‘  | å›¾åƒã€æ–‡æœ¬ã€éŸ³é¢‘  | [ğŸ¤— HuggingFace](https://huggingface.co/inclusionAI/Ming-Lite-Omni) <br>[ğŸ¤– ModelScope](https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni) |
</div>

å¦‚æœæ‚¨ä½äºä¸­å›½å¤§é™†ï¼Œæˆ‘ä»¬å¼ºçƒˆå»ºè®®æ‚¨ä» ğŸ¤– <a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni">ModelScope</a> ä¸‹è½½æ¨¡å‹ã€‚


## ç¯å¢ƒå‡†å¤‡


### Installation with pip
```shell
pip install -r requirements.txt
# for python 3.10
pip install data/matcha_tts-0.0.5.1-cp310-cp310-linux_x86_64.whl 
# for python 3.8 
# pip install data/matcha_tts-0.0.5.1-cp38-cp38-linux_x86_64.whl
pip install diffusers==0.33.0
pip install nvidia-cublas-cu12==12.4.5.8  # for H20 GPU
```

### Installation with docker

You can also initialize the environment by building the docker image. First clone this repository:
```shell
git clone --depth 1 https://github.com/inclusionAI/Ming.git
cd Ming
```
Then build the docker image with the provided Dockerfile in `docker/docker-py310-cu121`. This step might take a while:
```shell
docker build -t ming:py310-cu121 docker/docker-py310-cu121
```
At last, start the container with the current repo directory mounted:
```shell
docker run -it --gpus all -v "$(pwd)":/workspace/Ming ming:py310-cu121 ming:py310-cu121 /bin/bash
```
You can run the model with python interface. You may download the huggingface model in the repo directory first (`.../Ming/`) or mount the downloaded model path when starting the container.


## ä½¿ç”¨æ ·ä¾‹

We provide a step-by-step running example:

Step 1 - Download the source code
```
git clone https://github.com/inclusionAI/Ming.git 
cd Ming
```
Step 2 - Download the model weights and create a soft link to the source code directory

Download our model following [Model Downloads](#model-downloads)

```shell
mkdir inclusionAI 
ln -s /path/to/inclusionAI/Ming-Lite-Omni inclusionAI/Ming-Lite-Omni
```

Step 3 - Enter the code directory, you can refer to the following codes to run the Ming-Lite-Omni model.
```
jupyter notebook cookbook.ipynb
```

We also provide a simple example on the usage of this repo. For detailed usage, please refer to [cookbook.ipynb](cookbook.ipynb).

```python
import torch
from transformers import AutoProcessor, GenerationConfig
from modeling_bailingmm import BailingMMNativeForConditionalGeneration

# load model
model = BailingMMNativeForConditionalGeneration.from_pretrained(
    "inclusionAI/Ming-Lite-Omni",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=True
).to("cuda")

# build processor
processor = AutoProcessor.from_pretrained("inclusionAI/Ming-Lite-Omni", trust_remote_code=True)

# qa
messages = [
    {
        "role": "HUMAN",
        "content": [
            {"type": "text", "text": "è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚"}
        ],
    },
]

# 1. Format inputs using chat template
text = processor.apply_chat_template(messages, add_generation_prompt=True)

# 2. Extract vision/audio data
image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages)

# 3. Prepare tensor inputs
inputs = processor(
    text=[text],
    images=image_inputs,
    videos=video_inputs,
    audios=audio_inputs,
    return_tensors="pt",
)
inputs = inputs.to(model.device)
for k in inputs.keys():
    if k == "pixel_values" or k == "pixel_values_videos" or k == "audio_feats":
        inputs[k] = inputs[k].to(dtype=torch.bfloat16)

# 4. Configure generation
generation_config = GenerationConfig.from_dict({'no_repeat_ngram_size': 10})
generated_ids = model.generate(
    **inputs,
    max_new_tokens=512,
    use_cache=True,
    eos_token_id=processor.gen_terminator,
    generation_config=generation_config,
)
generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]

# 5. Decode output
output_text = processor.batch_decode(
    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
)[0]
print(output_text)
# Output:

# é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š
# ### 1. **æ –æ¯åœ°**
# é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚
# ### 2. **é¥®é£Ÿ**
# é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚
# ......
```

Note: We test the examples on hardware of NVIDIA H800-80GB/H20-96G with CUDA 12.4. Loading inclusionAI/Ming-Lite-Omni in bfloat16 takes about 62G GPU memory.



## è®¸å¯ä¸æ³•å¾‹å£°æ˜

æœ¬ä»£ç ä»“åº“éµå¾ª [MIT è®¸å¯è¯](./LICENSE)ï¼Œæ³•å¾‹å£°æ˜è§é¡¹ç›®æ ¹ç›®å½•ä¸‹çš„ [LEGAL.md æ–‡ä»¶](./LEGAL.md)ã€‚

## å¼•ç”¨

å¦‚æœæ‚¨è§‰å¾—æˆ‘ä»¬çš„å·¥ä½œå¯¹æ‚¨æœ‰å¸®åŠ©ï¼Œæ¬¢è¿å¼•ç”¨ã€‚


```bibtex

@misc{Mingomni2025,
      title  = {Ming-Omni: A Unified Multimodal Model for Perception and Generation}, 
      author = {Inclusion AI},
      year = {2025},
      eprint = {2506.09344},
      archivePrefix = {arXiv},
      url = {https://arxiv.org/abs/2506.09344}
}
```


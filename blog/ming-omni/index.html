<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Omni: A Unified Multimodal Model for Perception and Generation | INCLUSION AI</title><meta name=keywords content><meta name=description content="
GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction
Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities.
Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-omni/><link crossorigin=anonymous href=/assets/css/stylesheet.419cab9a4e041985806269ccd5fb7e29179062491b1e90454454dcf1af0c9022.css integrity="sha256-QZyrmk4EGYWAYmnM1ft+KReQYkkbHpBFRFTc8a8MkCI=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-omni/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-omni/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Ming-Omni: A Unified Multimodal Model for Perception and Generation"><meta property="og:description" content="
GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction
Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities.
Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-omni/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-11T00:00:03+08:00"><meta property="article:modified_time" content="2025-06-11T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Omni: A Unified Multimodal Model for Perception and Generation"><meta name=twitter:description content="
GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction
Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities.
Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Omni: A Unified Multimodal Model for Perception and Generation","item":"https://inclusionai.github.io/blog/ming-omni/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Omni: A Unified Multimodal Model for Perception and Generation","name":"Ming-Omni: A Unified Multimodal Model for Perception and Generation","description":" GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope\nIntroduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities. Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.\n","keywords":[],"articleBody":" GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope\nIntroduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities. Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.\nğŸ“Œ Updates [2025.06.12] ğŸ”¥ Our Technical Report is in public on arxiv. [2025.05.28] ğŸ”¥ The official version of Ming-lite-omni is released, with better performance and image generation support. [2025.05.04] ğŸ”¥ We release the test version of Ming-lite-omniï¼šMing-lite-omni-Preview. Key Features Unified Omni-Modality Perception: Ming-lite-omni, built on Ling, an MoE architecture LLM, resolves task conflicts and ensures coherent integration of tokens from different modalities through modality-specific routers.\nUnified Perception and Generation: Ming-lite-omni achieves unified understanding and generation, enabling the model to interpret multimodal instructions and user intent during generation, which helps enhance generation quality and improves usability across multiple tasks.\nInnovative Generation Capabilities: Ming-lite-omni can perceive all modalities and generate high-quality text, real-time speech, and vivid images simultaneously, delivering exceptional cross-modal performance across diverse tasks including image perception, audio-visual interaction, and image generation.\nEvaluation Ming-lite-omni delivers exceptional cross-modal performance, as validated across image perception, audio-visual interaction, and image generation tasks. Specifically, in the image perception task, Ming-lite-omni attained performance comparable to that of Qwen2.5-VL-7B by activating only 2.8B parameters. It delivers superior performance in end-to-end speech understanding and instruction following, surpassing Qwen2.5-Omni and Kimi-Audio. It also supports native-resolution image generation, editing, and style transfer, achieving a GenEval score of 0.64, outperforming mainstream models such as SDXL. In terms of FID, Ming-lite-omni reaches 4.85, setting a new SOTA across existing methods.\nImage benchmark Benchmarks Ming-lite-omni Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.1 84.4 84.5 HallusionBench 55.0 55.8 51.7 MMBench_TEST_V11 80.8 82.8 82.0 MMMU 56.3 56.6 54.8 MMStar 64.7 65.3 65.2 MMVet 71.3 71.6 68.1 MathVista 71.6 68.1 67.9 OCRBench 88.4 87.8 88.2 Average 71.4 71.5 70.3 Encyclopedia Benchmarks Object Recognition Ming-lite-omni Qwen2.5-VL-7B-Instruct Plants 54.96 47.8 Animals 56.7 50.85 Vehicles 41.91 42.29 Food \u0026 Ingredients 62.28 54.09 Dishes 44.3 39.07 General 91.08 92.42 Average 58.54 54.43 Video benchmark Benchmarks Ming-lite-omni Qwen2.5VL-7B-Instruct VideoMME 67.0 67.3 MVBench 67.7 67.4 Video-MMMU 46.3 47.4 LongVideoBench 56.6 54.7 Average 59.4 59.2 Note: All models are evaluated based on 128 uniformly sampled frames. Audio benchmark SpeechQA Model Average AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.545 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 3.695 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 3.77 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.215 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.21 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-lite-omni 4.34 4.63 4.06 58.84 47.53 61.98 58.36 99.04 ASR Model aishell1 aishell2_android aishell2_ios cv15_zh fleurs_zh wenetspeech_meeting wenetspeech_net librispeech_test_clean librispeech_test_other multilingual_librispeech cv15_en fleurs_en voxpopuli_v1.0_en Ming-lite-omni 1.47 2.55 2.52 6.31 2.96 5.95 5.46 1.44 2.80 4.15 6.89 3.39 5.80 Qwen2.-Omni 1.18 2.75 2.63 5.20 3.00 5.90 7.70 1.80 3.40 7.56 7.60 4.10 5.80 Qwen2-Audio 1.53 2.92 2.92 6.90 7.50 7.16 8.42 1.60 3.60 5.40 8.60 6.90 6.84 Kimi-Audio 0.60 2.64 2.56 7.21 2.69 6.28 5.37 1.28 2.42 5.88 10.31 4.44 7.97 Information-Seeking Benchmark Model InfoSeek_H-mean InfoSeek_unseen_question InfoSeek_unseen_entity GPT-4o 36.05 - - PaLI-X 22.06 23.5 20.8 Qwen2.5-vl-32B 19.35 20.55 18.28 Ming-lite-omni 27.7 30.4 25.4 OCR Model Ming-lite-omni Qwen2.5-VL-7B-Instruct ChartQA_TEST 85.1 87.3 DocVQA_TEST 93 95.7 OCRBenchV2_en/zh 53.3/52 56.3/57.2 OmniDocBenchâ†“ 34/34.4 30.8/39.8 TextVQA_VAL 82.8 84.9 GUI Model Ming-lite-omni InternVL3 8B Qwen2.5-VL-7B-Instruct ScreenSpot 82.1 79.5 78.9* ScreenSpot-V2 84.1 81.4 - AITZ(EM) 66.6 - 57.6* Note: * denotes the reproduced results. Unified Generation Benchmark Model single_object two_object counting colors position color_attr GENEVAL DPGBench FIDâ†“ Ming-lite-omni 0.9875 0.7727 0.6812 0.7872 0.31 0.29 0.64 81.72 4.85 Metaquery-XL - - - - - - 0.61 82.05 6.02 SDv2.1 0.98 0.51 0.44 0.85 0.07 0.17 0.50 68.09 26.96 Emu3-Gen 0.98 0.71 0.34 0.81 0.17 0.21 0.54 80.60 - SDXL 0.98 0.74 0.39 0.85 0.15 0.23 0.55 74.65 8.76 Janus 0.97 0.68 0.30 0.84 0.46 0.42 0.61 79.68 10.10 JanusFlow - - - - - - 0.63 80.09 9.51 Please refer to our technical report for more comprehensive evaluation results.\nModel Downloads You can download the model from both Huggingface and ModelScope.\nModel Input modality Oput modality Download Ming-Lite-Omni Image,text,viedio,audio Image,text,audio ğŸ¤— HuggingFace ğŸ¤– ModelScope If you're in mainland China, we strongly recommend you to download our model from ğŸ¤– ModelScope. pip install modelscope modelscope download --model inclusionAI/Ming-Lite-Omni --local_dir inclusionAI/Ming-Lite-Omni --revision master Note: This download process will take several minutes to several hours, depending on your network conditions.\nUse Cases Additional demonstration cases are available on our project page.\nEnvironment Preparation Installation with pip pip install -r requirements.txt # for python 3.10 pip install data/matcha_tts-0.0.5.1-cp310-cp310-linux_x86_64.whl # for python 3.8 # pip install data/matcha_tts-0.0.5.1-cp38-cp38-linux_x86_64.whl pip install diffusers==0.33.0 pip install nvidia-cublas-cu12==12.4.5.8 # for H20 GPU Installation with docker You can also initialize the environment by building the docker image. First clone this repository:\ngit clone --depth 1 https://github.com/inclusionAI/Ming.git cd Ming Then build the docker image with the provided Dockerfile in docker/docker-py310-cu121. This step might take a while:\ndocker build -t ming:py310-cu121 docker/docker-py310-cu121 At last, start the container with the current repo directory mounted:\ndocker run -it --gpus all -v \"$(pwd)\":/workspace/Ming ming:py310-cu121 ming:py310-cu121 /bin/bash You can run the model with python interface. You may download the huggingface model in the repo directory first (.../Ming/) or mount the downloaded model path when starting the container.\nExample Usage We provide a step-by-step running example:\nStep 1 - Download the source code\ngit clone https://github.com/inclusionAI/Ming.git cd Ming Step 2 - Download the model weights and create a soft link to the source code directory\nDownload our model following Model Downloads\nmkdir inclusionAI ln -s /path/to/inclusionAI/Ming-Lite-Omni inclusionAI/Ming-Lite-Omni Step 3 - Enter the code directory, you can refer to the following codes to run the Ming-Lite-Omni model.\njupyter notebook cookbook.ipynb We also provide a simple example on the usage of this repo. For detailed usage, please refer to cookbook.ipynb.\nimport torch from transformers import AutoProcessor, GenerationConfig from modeling_bailingmm import BailingMMNativeForConditionalGeneration # load model model = BailingMMNativeForConditionalGeneration.from_pretrained( \"inclusionAI/Ming-Lite-Omni\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).to(\"cuda\") # build processor processor = AutoProcessor.from_pretrained(\"inclusionAI/Ming-Lite-Omni\", trust_remote_code=True) # qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚\"} ], }, ] # 1. Format inputs using chat template text = processor.apply_chat_template(messages, add_generation_prompt=True) # 2. Extract vision/audio data image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages) # 3. Prepare tensor inputs inputs = processor( text=[text], images=image_inputs, videos=video_inputs, audios=audio_inputs, return_tensors=\"pt\", ) inputs = inputs.to(model.device) for k in inputs.keys(): if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\": inputs[k] = inputs[k].to(dtype=torch.bfloat16) # 4. Configure generation generation_config = GenerationConfig.from_dict({'no_repeat_ngram_size': 10}) generated_ids = model.generate( **inputs, max_new_tokens=512, use_cache=True, eos_token_id=processor.gen_terminator, generation_config=generation_config, ) generated_ids_trimmed = [ out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] # 5. Decode output output_text = processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False )[0] print(output_text) # Output: # é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š # ### 1. **æ –æ¯åœ°** # é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚ # ### 2. **é¥®é£Ÿ** # é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚ # ...... Note: We test the examples on hardware of NVIDIA H800-80GB/H20-96G with CUDA 12.4. Loading inclusionAI/Ming-Lite-Omni in bfloat16 takes about 62G GPU memory.\nLicense and Legal Disclaimer This code repository is licensed under the MIT License, and the Legal Disclaimer is located in the LEGAL.md file under the projectâ€™s root directory.\nCitation If you find our work helpful, feel free to give us a cite.\n@misc{Mingomni2025, title = {Ming-Omni: A Unified Multimodal Model for Perception and Generation}, author = {Inclusion AI}, year = {2025}, eprint = {2506.09344}, archivePrefix = {arXiv}, url = {https://arxiv.org/abs/2506.09344} } ","wordCount":"1379","inLanguage":"en","datePublished":"2025-06-11T00:00:03+08:00","dateModified":"2025-06-11T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-omni/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;Â»&nbsp;<a href=https://inclusionai.github.io/blog/>Blog</a></div><h1 class=post-title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</h1><div class=post-meta><span class=post-date title="2025-06-11 00:00:03 +0800 +0800">June 11, 2025</span>
<span class=post-word-count>1379 words</span>
<span class=post-author>inclusionAI, Ant Group</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-omni/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></header><div class=post-content><p><a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify class="btn external" target=_blank>GITHUB</a> ğŸ“‘ <a href=https://arxiv.org/abs/2506.09344>Technical Report</a>ï½œğŸ“–<a href=https://lucaria-academy.github.io/Ming-Omni/>Project Page</a> ï½œğŸ¤— <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>Hugging Face</a>ï½œ ğŸ¤– <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni>ModelScope</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Ming-lite-omni, a light version of Ming-omni, which is derived from <a href=https://github.com/inclusionAI/Ling>Ling-lite</a> and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities.
Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.</p><h2 id=-updates>ğŸ“Œ Updates<a hidden class=anchor aria-hidden=true href=#-updates>#</a></h2><ul><li>[2025.06.12] ğŸ”¥ Our <a href=https://arxiv.org/abs/2506.09344>Technical Report</a> is in public on arxiv.</li><li>[2025.05.28] ğŸ”¥ The official version of Ming-lite-omni is released, with better performance and image generation support.</li><li>[2025.05.04] ğŸ”¥ We release the test version of Ming-lite-omniï¼š<a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview>Ming-lite-omni-Preview</a>.</li></ul><h2 id=key-features>Key Features<a hidden class=anchor aria-hidden=true href=#key-features>#</a></h2><ul><li><p><strong>Unified Omni-Modality Perception</strong>: Ming-lite-omni, built on <a href=https://github.com/inclusionAI/Ling>Ling</a>, an MoE architecture LLM, resolves task conflicts and ensures coherent integration of tokens from different modalities through modality-specific routers.</p></li><li><p><strong>Unified Perception and Generation</strong>: Ming-lite-omni achieves unified understanding and generation, enabling the model to interpret multimodal instructions and user intent during generation, which helps enhance generation quality and improves usability across multiple tasks.</p></li><li><p><strong>Innovative Generation Capabilities</strong>: Ming-lite-omni can perceive all modalities and generate high-quality text, real-time speech, and vivid images simultaneously, delivering exceptional cross-modal performance across diverse tasks including image perception, audio-visual interaction, and image generation.</p></li></ul><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>Ming-lite-omni delivers exceptional cross-modal performance, as validated across image perception, audio-visual interaction, and image generation tasks. Specifically, in the image perception task, Ming-lite-omni attained performance comparable to that of Qwen2.5-VL-7B by activating only 2.8B parameters. It delivers superior performance in end-to-end speech understanding and instruction following, surpassing Qwen2.5-Omni and Kimi-Audio. It also supports native-resolution image generation, editing, and style transfer, achieving a GenEval score of 0.64, outperforming mainstream models such as SDXL. In terms of FID, Ming-lite-omni reaches 4.85, setting a new SOTA across existing methods.</p><h3 id=image-benchmark>Image benchmark<a hidden class=anchor aria-hidden=true href=#image-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th><th style=text-align:center>InternVL2.5-8B-MPO</th></tr></thead><tbody><tr><td style=text-align:left>AI2D</td><td style=text-align:center>83.1</td><td style=text-align:center>84.4</td><td style=text-align:center><b>84.5</b></td></tr><tr><td style=text-align:left>HallusionBench</td><td style=text-align:center><b>55.0</b></td><td style=text-align:center>55.8</td><td style=text-align:center>51.7</td></tr><tr><td style=text-align:left>MMBench_TEST_V11</td><td style=text-align:center>80.8</td><td style=text-align:center><b>82.8</b></td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>MMMU</td><td style=text-align:center>56.3</td><td style=text-align:center><b>56.6</b></td><td style=text-align:center>54.8</td></tr><tr><td style=text-align:left>MMStar</td><td style=text-align:center>64.7</td><td style=text-align:center>65.3</td><td style=text-align:center><b>65.2</b></td></tr><tr><td style=text-align:left>MMVet</td><td style=text-align:center>71.3</td><td style=text-align:center>71.6</td><td style=text-align:center>68.1</td></tr><tr><td style=text-align:left>MathVista</td><td style=text-align:center><b>71.6</b></td><td style=text-align:center>68.1</td><td style=text-align:center>67.9</td></tr><tr><td style=text-align:left>OCRBench</td><td style=text-align:center><b>88.4</b></td><td style=text-align:center>87.8</td><td style=text-align:center>88.2</td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center>71.4</td><td style=text-align:center><b>71.5</b></td><td style=text-align:center>70.3</td></tr></tbody></table></div><h4 id=encyclopedia-benchmarks>Encyclopedia Benchmarks<a hidden class=anchor aria-hidden=true href=#encyclopedia-benchmarks>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Object Recognition</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>Plants</td><td style=text-align:center><strong>54.96</strong></td><td style=text-align:center>47.8</td></tr><tr><td style=text-align:left>Animals</td><td style=text-align:center><strong>56.7</strong></td><td style=text-align:center>50.85</td></tr><tr><td style=text-align:left>Vehicles</td><td style=text-align:center>41.91</td><td style=text-align:center><strong>42.29</strong></td></tr><tr><td style=text-align:left>Food & Ingredients</td><td style=text-align:center><strong>62.28</strong></td><td style=text-align:center>54.09</td></tr><tr><td style=text-align:left>Dishes</td><td style=text-align:center><strong>44.3</strong></td><td style=text-align:center>39.07</td></tr><tr><td style=text-align:left>General</td><td style=text-align:center>91.08</td><td style=text-align:center><strong>92.42</strong></td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center><strong>58.54</strong></td><td style=text-align:center>54.43</td></tr></tbody></table></div><h3 id=video-benchmark>Video benchmark<a hidden class=anchor aria-hidden=true href=#video-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>VideoMME</td><td style=text-align:center>67.0</td><td style=text-align:center><b>67.3</b></td></tr><tr><td style=text-align:left>MVBench</td><td style=text-align:center>67.7</td><td style=text-align:center><b>67.4</b></td></tr><tr><td style=text-align:left>Video-MMMU</td><td style=text-align:center>46.3</td><td style=text-align:center><b>47.4</b></td></tr><tr><td style=text-align:left>LongVideoBench</td><td style=text-align:center>56.6</td><td style=text-align:center>54.7</td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center><b>59.4</b></td><td style=text-align:center>59.2</td></tr></tbody></table></div>Note: All models are evaluated based on 128 uniformly sampled frames.<h3 id=audio-benchmark>Audio benchmark<a hidden class=anchor aria-hidden=true href=#audio-benchmark>#</a></h3><h4 id=speechqa>SpeechQA<a hidden class=anchor aria-hidden=true href=#speechqa>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Average</th><th style=text-align:center>AlpacaEval</th><th style=text-align:center>CommonEval</th><th style=text-align:center>SD-QA</th><th style=text-align:center>MMSU</th><th style=text-align:center>OpenBookQA</th><th style=text-align:center>IFEval</th><th style=text-align:center>AdvBench</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2-Audio-chat</td><td style=text-align:center>3.545</td><td style=text-align:center>3.69</td><td style=text-align:center>3.40</td><td style=text-align:center>35.35</td><td style=text-align:center>35.43</td><td style=text-align:center>49.01</td><td style=text-align:center>22.57</td><td style=text-align:center>98.85</td></tr><tr><td style=text-align:left>Baichuan-Audio</td><td style=text-align:center>3.695</td><td style=text-align:center>4.00</td><td style=text-align:center>3.39</td><td style=text-align:center>49.64</td><td style=text-align:center>48.80</td><td style=text-align:center>63.30</td><td style=text-align:center>41.32</td><td style=text-align:center>86.73</td></tr><tr><td style=text-align:left>GLM-4-Voice</td><td style=text-align:center>3.77</td><td style=text-align:center>4.06</td><td style=text-align:center>3.48</td><td style=text-align:center>43.31</td><td style=text-align:center>40.11</td><td style=text-align:center>52.97</td><td style=text-align:center>24.91</td><td style=text-align:center>88.08</td></tr><tr><td style=text-align:left>Kimi-Audio</td><td style=text-align:center>4.215</td><td style=text-align:center>4.46</td><td style=text-align:center>3.97</td><td style=text-align:center><b>63.12</b></td><td style=text-align:center><b>62.17</b></td><td style=text-align:center><b>83.52</b></td><td style=text-align:center><b>61.10</b></td><td style=text-align:center><b>100.00</b></td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center>4.21</td><td style=text-align:center>4.49</td><td style=text-align:center>3.93</td><td style=text-align:center>55.71</td><td style=text-align:center>61.32</td><td style=text-align:center>81.10</td><td style=text-align:center>52.87</td><td style=text-align:center>99.42</td></tr><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center><b>4.34</b></td><td style=text-align:center><b>4.63</b></td><td style=text-align:center><b>4.06</b></td><td style=text-align:center>58.84</td><td style=text-align:center>47.53</td><td style=text-align:center>61.98</td><td style=text-align:center>58.36</td><td style=text-align:center>99.04</td></tr></tbody></table></div><h4 id=asr>ASR<a hidden class=anchor aria-hidden=true href=#asr>#</a></h4><div align=center><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>aishell1</th><th style=text-align:center>aishell2_android</th><th style=text-align:center>aishell2_ios</th><th style=text-align:center>cv15_zh</th><th style=text-align:center>fleurs_zh</th><th style=text-align:center>wenetspeech_meeting</th><th style=text-align:center>wenetspeech_net</th><th style=text-align:center>librispeech_test_clean</th><th style=text-align:center>librispeech_test_other</th><th style=text-align:center>multilingual_librispeech</th><th style=text-align:center>cv15_en</th><th style=text-align:center>fleurs_en</th><th style=text-align:center>voxpopuli_v1.0_en</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>1.47</td><td style=text-align:center><strong>2.55</strong></td><td style=text-align:center><strong>2.52</strong></td><td style=text-align:center>6.31</td><td style=text-align:center>2.96</td><td style=text-align:center>5.95</td><td style=text-align:center>5.46</td><td style=text-align:center>1.44</td><td style=text-align:center>2.80</td><td style=text-align:center><strong>4.15</strong></td><td style=text-align:center><strong>6.89</strong></td><td style=text-align:center><strong>3.39</strong></td><td style=text-align:center><strong>5.80</strong></td></tr><tr><td style=text-align:center>Qwen2.-Omni</td><td style=text-align:center>1.18</td><td style=text-align:center>2.75</td><td style=text-align:center>2.63</td><td style=text-align:center><strong>5.20</strong></td><td style=text-align:center>3.00</td><td style=text-align:center><strong>5.90</strong></td><td style=text-align:center>7.70</td><td style=text-align:center>1.80</td><td style=text-align:center>3.40</td><td style=text-align:center>7.56</td><td style=text-align:center>7.60</td><td style=text-align:center>4.10</td><td style=text-align:center><strong>5.80</strong></td></tr><tr><td style=text-align:center>Qwen2-Audio</td><td style=text-align:center>1.53</td><td style=text-align:center>2.92</td><td style=text-align:center>2.92</td><td style=text-align:center>6.90</td><td style=text-align:center>7.50</td><td style=text-align:center>7.16</td><td style=text-align:center>8.42</td><td style=text-align:center>1.60</td><td style=text-align:center>3.60</td><td style=text-align:center>5.40</td><td style=text-align:center>8.60</td><td style=text-align:center>6.90</td><td style=text-align:center>6.84</td></tr><tr><td style=text-align:center>Kimi-Audio</td><td style=text-align:center><strong>0.60</strong></td><td style=text-align:center>2.64</td><td style=text-align:center>2.56</td><td style=text-align:center>7.21</td><td style=text-align:center><strong>2.69</strong></td><td style=text-align:center>6.28</td><td style=text-align:center><strong>5.37</strong></td><td style=text-align:center><strong>1.28</strong></td><td style=text-align:center><strong>2.42</strong></td><td style=text-align:center>5.88</td><td style=text-align:center>10.31</td><td style=text-align:center>4.44</td><td style=text-align:center>7.97</td></tr></tbody></table></div><h3 id=information-seeking-benchmark>Information-Seeking Benchmark<a hidden class=anchor aria-hidden=true href=#information-seeking-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>InfoSeek_H-mean</th><th style=text-align:center>InfoSeek_unseen_question</th><th style=text-align:center>InfoSeek_unseen_entity</th></tr></thead><tbody><tr><td style=text-align:left>GPT-4o</td><td style=text-align:center><b>36.05</b></td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>PaLI-X</td><td style=text-align:center>22.06</td><td style=text-align:center>23.5</td><td style=text-align:center>20.8</td></tr><tr><td style=text-align:left>Qwen2.5-vl-32B</td><td style=text-align:center>19.35</td><td style=text-align:center>20.55</td><td style=text-align:center>18.28</td></tr><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center>27.7</td><td style=text-align:center><strong>30.4</strong></td><td style=text-align:center><strong>25.4</strong></td></tr></tbody></table></div><h3 id=ocr>OCR<a hidden class=anchor aria-hidden=true href=#ocr>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>ChartQA_TEST</td><td style=text-align:center>85.1</td><td style=text-align:center><b>87.3</b></td></tr><tr><td style=text-align:left>DocVQA_TEST</td><td style=text-align:center>93</td><td style=text-align:center><b>95.7</b></td></tr><tr><td style=text-align:left>OCRBenchV2_en/zh</td><td style=text-align:center>53.3/52</td><td style=text-align:center><b>56.3/57.2</b></td></tr><tr><td style=text-align:left>OmniDocBenchâ†“</td><td style=text-align:center>34/<b>34.4</b></td><td style=text-align:center><b>30.8</b>/39.8</td></tr><tr><td style=text-align:left>TextVQA_VAL</td><td style=text-align:center>82.8</td><td style=text-align:center><b>84.9</b></td></tr></tbody></table></div><h3 id=gui>GUI<a hidden class=anchor aria-hidden=true href=#gui>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>InternVL3 8B</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>ScreenSpot</td><td style=text-align:center><b>82.1</b></td><td style=text-align:center>79.5</td><td style=text-align:center>78.9*</td></tr><tr><td style=text-align:left>ScreenSpot-V2</td><td style=text-align:center><b>84.1</b></td><td style=text-align:center>81.4</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>AITZ(EM)</td><td style=text-align:center><b>66.6</b></td><td style=text-align:center>-</td><td style=text-align:center>57.6*</td></tr></tbody></table></div>Note: * denotes the reproduced results.<h3 id=unified-generation-benchmark>Unified Generation Benchmark<a hidden class=anchor aria-hidden=true href=#unified-generation-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>single_object</th><th style=text-align:center>two_object</th><th style=text-align:center>counting</th><th style=text-align:center>colors</th><th style=text-align:center>position</th><th style=text-align:center>color_attr</th><th style=text-align:center>GENEVAL</th><th style=text-align:center>DPGBench</th><th style=text-align:center>FIDâ†“</th></tr></thead><tbody><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center><strong>0.9875</strong></td><td style=text-align:center><strong>0.7727</strong></td><td style=text-align:center><strong>0.6812</strong></td><td style=text-align:center>0.7872</td><td style=text-align:center>0.31</td><td style=text-align:center>0.29</td><td style=text-align:center><strong>0.64</strong></td><td style=text-align:center>81.72</td><td style=text-align:center><strong>4.85</strong></td></tr><tr><td style=text-align:left>Metaquery-XL</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>0.61</td><td style=text-align:center><strong>82.05</strong></td><td style=text-align:center>6.02</td></tr><tr><td style=text-align:left>SDv2.1</td><td style=text-align:center>0.98</td><td style=text-align:center>0.51</td><td style=text-align:center>0.44</td><td style=text-align:center><strong>0.85</strong></td><td style=text-align:center>0.07</td><td style=text-align:center>0.17</td><td style=text-align:center>0.50</td><td style=text-align:center>68.09</td><td style=text-align:center>26.96</td></tr><tr><td style=text-align:left>Emu3-Gen</td><td style=text-align:center>0.98</td><td style=text-align:center>0.71</td><td style=text-align:center>0.34</td><td style=text-align:center>0.81</td><td style=text-align:center>0.17</td><td style=text-align:center>0.21</td><td style=text-align:center>0.54</td><td style=text-align:center>80.60</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>SDXL</td><td style=text-align:center>0.98</td><td style=text-align:center>0.74</td><td style=text-align:center>0.39</td><td style=text-align:center><strong>0.85</strong></td><td style=text-align:center>0.15</td><td style=text-align:center>0.23</td><td style=text-align:center>0.55</td><td style=text-align:center>74.65</td><td style=text-align:center>8.76</td></tr><tr><td style=text-align:left>Janus</td><td style=text-align:center>0.97</td><td style=text-align:center>0.68</td><td style=text-align:center>0.30</td><td style=text-align:center>0.84</td><td style=text-align:center><strong>0.46</strong></td><td style=text-align:center><strong>0.42</strong></td><td style=text-align:center>0.61</td><td style=text-align:center>79.68</td><td style=text-align:center>10.10</td></tr><tr><td style=text-align:left>JanusFlow</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>0.63</td><td style=text-align:center>80.09</td><td style=text-align:center>9.51</td></tr></tbody></table></div><p>Please refer to our technical report for more comprehensive evaluation results.</p><h2 id=model-downloads>Model Downloads<a hidden class=anchor aria-hidden=true href=#model-downloads>#</a></h2><p>You can download the model from both Huggingface and ModelScope.</p><div align=center><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:center><strong>Input modality</strong></th><th style=text-align:center><strong>Oput modality</strong></th><th style=text-align:center><strong>Download</strong></th></tr></thead><tbody><tr><td style=text-align:left>Ming-Lite-Omni</td><td style=text-align:center>Image,text,viedio,audio</td><td style=text-align:center>Image,text,audio</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>ğŸ¤— HuggingFace</a><br><a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni>ğŸ¤– ModelScope</a></td></tr></tbody></table></div>If you're in mainland China, we strongly recommend you to download our model from ğŸ¤– <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni>ModelScope</a>.<pre tabindex=0><code>pip install modelscope
modelscope download --model inclusionAI/Ming-Lite-Omni --local_dir inclusionAI/Ming-Lite-Omni  --revision master
</code></pre><p>Note: This download process will take several minutes to several hours, depending on your network conditions.</p><h2 id=use-cases>Use Cases<a hidden class=anchor aria-hidden=true href=#use-cases>#</a></h2><p>Additional demonstration cases are available on our project <a href=https://lucaria-academy.github.io/Ming-Omni/>page</a>.</p><h2 id=environment-preparation>Environment Preparation<a hidden class=anchor aria-hidden=true href=#environment-preparation>#</a></h2><h3 id=installation-with-pip>Installation with pip<a hidden class=anchor aria-hidden=true href=#installation-with-pip>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>pip install -r requirements.txt
</span></span><span class=line><span class=cl><span class=c1># for python 3.10</span>
</span></span><span class=line><span class=cl>pip install data/matcha_tts-0.0.5.1-cp310-cp310-linux_x86_64.whl 
</span></span><span class=line><span class=cl><span class=c1># for python 3.8 </span>
</span></span><span class=line><span class=cl><span class=c1># pip install data/matcha_tts-0.0.5.1-cp38-cp38-linux_x86_64.whl</span>
</span></span><span class=line><span class=cl>pip install <span class=nv>diffusers</span><span class=o>==</span>0.33.0
</span></span><span class=line><span class=cl>pip install nvidia-cublas-cu12<span class=o>==</span>12.4.5.8  <span class=c1># for H20 GPU</span>
</span></span></code></pre></div><h3 id=installation-with-docker>Installation with docker<a hidden class=anchor aria-hidden=true href=#installation-with-docker>#</a></h3><p>You can also initialize the environment by building the docker image. First clone this repository:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>git clone --depth <span class=m>1</span> https://github.com/inclusionAI/Ming.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> Ming
</span></span></code></pre></div><p>Then build the docker image with the provided Dockerfile in <code>docker/docker-py310-cu121</code>. This step might take a while:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker build -t ming:py310-cu121 docker/docker-py310-cu121
</span></span></code></pre></div><p>At last, start the container with the current repo directory mounted:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker run -it --gpus all -v <span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>&#34;</span>:/workspace/Ming ming:py310-cu121 ming:py310-cu121 /bin/bash
</span></span></code></pre></div><p>You can run the model with python interface. You may download the huggingface model in the repo directory first (<code>.../Ming/</code>) or mount the downloaded model path when starting the container.</p><h2 id=example-usage>Example Usage<a hidden class=anchor aria-hidden=true href=#example-usage>#</a></h2><p>We provide a step-by-step running example:</p><p>Step 1 - Download the source code</p><pre tabindex=0><code>git clone https://github.com/inclusionAI/Ming.git 
cd Ming
</code></pre><p>Step 2 - Download the model weights and create a soft link to the source code directory</p><p>Download our model following <a href=/blog/ming-omni/#model-downloads>Model Downloads</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mkdir inclusionAI 
</span></span><span class=line><span class=cl>ln -s /path/to/inclusionAI/Ming-Lite-Omni inclusionAI/Ming-Lite-Omni
</span></span></code></pre></div><p>Step 3 - Enter the code directory, you can refer to the following codes to run the Ming-Lite-Omni model.</p><pre tabindex=0><code>jupyter notebook cookbook.ipynb
</code></pre><p>We also provide a simple example on the usage of this repo. For detailed usage, please refer to <a href=cookbook.ipynb>cookbook.ipynb</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoProcessor</span><span class=p>,</span> <span class=n>GenerationConfig</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>modeling_bailingmm</span> <span class=kn>import</span> <span class=n>BailingMMNativeForConditionalGeneration</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># load model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BailingMMNativeForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>low_cpu_mem_usage</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build processor</span>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. Format inputs using chat template</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. Extract vision/audio data</span>
</span></span><span class=line><span class=cl><span class=n>image_inputs</span><span class=p>,</span> <span class=n>video_inputs</span><span class=p>,</span> <span class=n>audio_inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>process_vision_info</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. Prepare tensor inputs</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>images</span><span class=o>=</span><span class=n>image_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>videos</span><span class=o>=</span><span class=n>video_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>audios</span><span class=o>=</span><span class=n>audio_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>inputs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values_videos&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;audio_feats&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. Configure generation</span>
</span></span><span class=line><span class=cl><span class=n>generation_config</span> <span class=o>=</span> <span class=n>GenerationConfig</span><span class=o>.</span><span class=n>from_dict</span><span class=p>({</span><span class=s1>&#39;no_repeat_ngram_size&#39;</span><span class=p>:</span> <span class=mi>10</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>use_cache</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eos_token_id</span><span class=o>=</span><span class=n>processor</span><span class=o>.</span><span class=n>gen_terminator</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>generation_config</span><span class=o>=</span><span class=n>generation_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids_trimmed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>out_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>in_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>in_ids</span><span class=p>,</span> <span class=n>out_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. Decode output</span>
</span></span><span class=line><span class=cl><span class=n>output_text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_ids_trimmed</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š</span>
</span></span><span class=line><span class=cl><span class=c1># ### 1. **æ –æ¯åœ°**</span>
</span></span><span class=line><span class=cl><span class=c1># é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚</span>
</span></span><span class=line><span class=cl><span class=c1># ### 2. **é¥®é£Ÿ**</span>
</span></span><span class=line><span class=cl><span class=c1># é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚</span>
</span></span><span class=line><span class=cl><span class=c1># ......</span>
</span></span></code></pre></div><p>Note: We test the examples on hardware of NVIDIA H800-80GB/H20-96G with CUDA 12.4. Loading inclusionAI/Ming-Lite-Omni in bfloat16 takes about 62G GPU memory.</p><h2 id=license-and-legal-disclaimer>License and Legal Disclaimer<a hidden class=anchor aria-hidden=true href=#license-and-legal-disclaimer>#</a></h2><p>This code repository is licensed under the <a href=./LICENSE>MIT License</a>, and the Legal Disclaimer is located in the <a href=./LEGAL.md>LEGAL.md file</a> under the project&rsquo;s root directory.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you find our work helpful, feel free to give us a cite.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>Mingomni2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>title</span>  <span class=p>=</span> <span class=s>{Ming-Omni: A Unified Multimodal Model for Perception and Generation}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>      <span class=na>author</span> <span class=p>=</span> <span class=s>{Inclusion AI}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>year</span> <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>eprint</span> <span class=p>=</span> <span class=s>{2506.09344}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>archivePrefix</span> <span class=p>=</span> <span class=s>{arXiv}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>url</span> <span class=p>=</span> <span class=s>{https://arxiv.org/abs/2506.09344}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
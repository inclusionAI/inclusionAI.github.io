<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ü§óHugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Introducing Ming-Lite-Omni V1.5"><meta property="og:description" content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ü§óHugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-21T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-21T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Ming-Lite-Omni V1.5"><meta name=twitter:description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ü§óHugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.
Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Ming-Lite-Omni V1.5","item":"https://inclusionai.github.io/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Ming-Lite-Omni V1.5","name":"Introducing Ming-Lite-Omni V1.5","description":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nWe are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ü§óHugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.\nBuilt on the Ling-lite-1.5 architecture, Ming-lite-omni V1.","keywords":[],"articleBody":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nWe are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ü§óHugging Face). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.\nBuilt on the Ling-lite-1.5 architecture, Ming-lite-omni V1.5 features a total of 20.3B parameters, with 3B active parameters in its Mixture of Experts (MoE) configuration. It has achieved impressive results across various multimodal benchmarks. Below, we highlight the key advancements in performance metrics and model architecture from this latest update.\nPerformance Comparison\nFramework\nIntroduction To achieve these advancements, we integrated our in-house innovations with cutting-edge developments from both academia and the open-source community. We explored several key areas, leading to significant breakthroughs and key findings detailed below.\nImage and Speech Generation Dual-Branch Decoupling for Image Generation: We implemented a dual-branch decoupling strategy that processes the reference image and the noise image with separate weights before they enter the Diffusion Transformer (DiT). This approach effectively increases the model‚Äôs capacity for learning from the reference. To further refine the output, we added two extra transformer layers. ID \u0026 Scene Consistency Loss: To maintain character and scene identity during image editing, we introduced a new ID \u0026 Scene Consistency Loss. This function strategically increases focus on the edited region in the target image, boosts the influence of unedited areas from the reference image, and diminishes the influence of the edited areas from the reference. Perceptual Enhancement Strategy: We introduced a Perceptual Enhancement Strategy by improving the model‚Äôs ability to perceive structures through tasks like segmentation and keypoint detection. This provides greater structural control during generation and editing, leading to significant score improvements on metrics related to object position, structure, and count (see Table A for details). Multi-task Collaborative Learning: A Multi-task Collaborative Learning approach allows generation and editing tasks to synergistically improve one another. By treating segmentation as a colorization-based editing task within a unified training pipeline, we achieved significant gains in segmentation accuracy and enhanced the precision of local edits, resulting in smoother and more natural transitions at the edges of edited areas. New Audio Decoder: On the speech generation front, we developed a novel audio decoder that is fully context-aware, directly processing output features from the LLM. Efficient Speech Generation: To improve prosody and real-time generation capabilities, we applied BPE encoding to the discrete audio codec tokens, which successfully reduced the audio frame rate by 35%. Comprehensive Data Upgrades: Sourced high-quality portrait data, filtered by resolution, facial detail, and face size. Collected and curated a high-quality celebrity dataset with processed face crops. Expanded the model‚Äôs capabilities by building new training subsets for edge maps, segmentation maps, text-in-image, and facial expressions. Image, Text, Video, and Speech Understanding MRoPE for Spatiotemporal Awareness: We integrated MRoPE (Multi-dimensional RoPE), which applies positional encoding across three dimensions (time, height, and width). This equips the model with a native understanding of space and time, enabling highly efficient cross-modal modeling and significantly boosting comprehension accuracy for videos and complex images. Efficient Full-Parameter Training: By optimizing learning rates and the multimodal data mixture, we upgraded our pre-training from a phased approach of freezing/unfreezing LLM layers to a streamlined, efficient full-parameter training regimen. This shortened the training cycle by 26.5% with no loss in performance. Video Understanding via Curriculum Learning: We adopted a curriculum learning strategy for video understanding, progressively increasing the complexity from short-form to long-form videos to enhance the model‚Äôs capabilities. Complex Document Understanding with CoT: For complex document analysis, we incorporated a Chain-of-Thought (CoT) strategy to build structured reasoning paths, effectively improving the model‚Äôs problem-solving abilities on intricate queries. Comprehensive Data Upgrades: Pre-training: Added structured text entity data to fill knowledge graph gaps. Expanded with high-quality product data to enhance general knowledge. Instruction Fine-Tuning: Improved data precision for fine-grained visual perception (e.g., object counting, color, and scene recognition). Increased data depth for specialized domain recognition (e.g., flora/fauna, vehicles, and ingredients). Enhanced cross-disciplinary reasoning in complex visual-text tasks through targeted data optimization. For speech understanding, we infused instruction text with metadata like domain, topic, and language (including dialects). This boosted model performance and enabled comprehensive support for Mandarin, English, Cantonese, Sichuanese, Shanghainese, and Minnan. Aligning with User Preferences To ensure that improvements on standard benchmarks translate directly to a superior real-world user experience, we created our own human evaluation benchmark and conducted internal, head-to-head evaluations against other models. Thanks to our high-quality preference alignment data, Ming-lite-omni V1.5 demonstrates a distinct advantage over leading models in the accuracy (low hallucination rate), relevance, formatting, and fluency of its visual question-answering responses. On our internal adversarial test set, Ming-lite-omni V1.5 achieved an 87.07% win rate against Ming-lite-omni V1, marking a significant enhancement in user experience.\nBenchmark Dimension Qwen2.5-VL-7B Ming-Omni-Lite V1.5 Relevance 4.308 4.5 Fluency 4.765 4.91 Richness 3.828 3.69 Formatting 4.727 4.8 Accuracy 3.741 3.92 Average 4.274 4.365 Demos Image Editing To address the challenge of maintaining character and scene identity during image editing, we‚Äôve introduced the ID \u0026 Scene Consistency Loss. This function strategically increases the weight of the edited region in the target image, boosts the influence of unedited areas from the reference image, and reduces the influence of the edited areas within that same reference.\nGenerative Image Segmentation Next User Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. Ming-Lite-Omni V1.5 User Please segment different classes in this image. Ming-Lite-Omni V1.5 User Please segment different instances in this image. Ming-Lite-Omni V1.5 Image Generation We introduced a Perceptual Enhancement Strategy by optimizing the model‚Äôs structural awareness through tasks like segmentation and keypoint detection. This improves its understanding of image details and spatial relationships, giving us greater structural control over the generation and editing process. The result is a significant boost in scores on the GenEval benchmark, particularly for metrics related to position, structure, and count: 1-Obj 2-Obj Counting Colors Position Color Attr Avg. Ming-lite-omni 0.99 0.77 0.68 0.78 0.46 0.42 0.64 Ming-lite-omni V1.5 0.99 0.93 0.86 0.87 0.90 0.66 0.87 Original Image Generated Depth Map Generated Bounding Boxes Generated Edge Map Visual Dialogue Next User This picture looks very design-oriented. Can you describe its various design elements in detail? Ming-Lite-Omni This image shows a high-end gaming mouse with a modern and technological design. The mouse body is black with a smooth metallic finish. There is a blue glowing logo on the top of the mouse, and below the logo is the mouse wheel and left and right button area. The side of the mouse has a blue light decoration, which increases the visual appeal. The bottom of the mouse has a non-slip texture design to ensure stability during use. The overall design is simple and futuristic, suitable for gamers. Ming-Lite-Omni V1.5 This image showcases a gaming mouse with a striking design, featuring the following design elements: 1. Color and Material: - The main body of the mouse is black, with a smooth surface and a metallic finish. - The edges and button areas of the mouse have blue LED light strips, adding a high-tech feel and visual appeal. 2. Shape and Structure: - The mouse has an overall streamlined design, with a slightly sloped top that allows for natural hand placement. - The sides of the mouse feature pronounced curves, designed ergonomically to provide a comfortable grip. 3. Buttons and Scroll Wheel: - There is a scroll wheel on the left side of the mouse, with a blue LED light strip above it. - There are additional buttons on the right side of the mouse, which also have blue LED light strips, enhancing both functionality and aesthetics. 4. Brand Logo: - In the center of the mouse‚Äôs palm rest, there is an illuminated ‚ÄúG‚Äù logo, which is a signature design element of the brand. 5. Lighting Effects: - The blue LED light strips on the mouse stand out prominently against the dark background, enhancing its high-tech and futuristic feel. 6. Texture and Details: - The top of the mouse has a textured design, likely for an anti-slip effect. - The sides of the mouse feature fine, elongated textured patterns, which add a sense of visual depth. Overall, the design of this mouse combines a modern, high-tech aesthetic with ergonomic design, making it both beautiful and practical, and well-suited for gamers. User Why is the calligraphy in the picture good? Ming-Lite-Omni This calligraphy work showcases superb calligraphic skills and profound cultural heritage. The brushwork is fluid and natural, the character structures are rigorous, and the layout is well-proportioned, reflecting the calligrapher‚Äôs deep skill and artistic cultivation. Ming-Lite-Omni V1.5 This piece of calligraphy displays several notable characteristics that make it an excellent work of art: 1. Exquisite Brushwork: The calligraphy‚Äôs brushwork is fluid and natural, with rich variations in line thickness, demonstrating the calligrapher‚Äôs profound skill and proficient technique. 2. Rigorous Structure: The structure of each character is meticulously designed, with natural connections between strokes, maintaining a good balance and harmony between characters. 3. Composition and Layout: The overall composition of the piece is well-arranged, with even line and character spacing, creating a sense of order and neatness. 4. Ink Tone Variation: The variations in ink tone, from dark to light, are natural, adding layers and a three-dimensional feel to the work. 5. Elegant Content: The content of the calligraphy is a beautiful classical text, rich in philosophical and literary value, which enhances the cultural depth of the work. 6. Seal Embellishment: The red seals on the work not only add a decorative element but also serve to embellish and balance the composition. Together, these features constitute the high artistic value of this calligraphy piece, making it a masterpiece worthy of appreciation and study. User Is the phone in the picture suitable to buy for parents? Ming-Lite-Omni This phone has a simple and stylish appearance, suitable for parents who prefer a minimalist style. The camera configuration is also quite advanced, making it suitable for users who enjoy taking photos. Ming-Lite-Omni V1.5 This phone is suitable for buying for parents for the following reasons: 1. Appearance Design: The phone features a simple and stylish design. The white body looks clean and neat, aligning with modern aesthetics. 2. Camera Configuration: The rear camera has a unique and neat arrangement, which may provide a good photography experience. 3. Brand: The Honor (Ëç£ËÄÄ) brand has a good reputation in the smartphone market, offering stable and reliable products. 4. Features: Phones of this type are typically equipped with a large-capacity battery and long battery life, suitable for daily use. 5. Ease of Use: It may offer a simple user interface and easy-to-operate functions, suitable for users who are not familiar with complex operations. 6. Value for Money: Honor phones are generally known for their high cost-performance ratio, making them suitable for users on a limited budget who still want a good experience. Of course, whether it is truly suitable depends on the actual needs and preferences of the parents. User How to buy a plane ticket using the software in the picture? Please list the specific steps Ming-Lite-Omni The specific steps to buy a plane ticket using the software in the picture are as follows: 1. Open the Ctrip app. 2. Click on the ‚ÄúFlights‚Äù option. 3. On the flight page, select the departure city, destination, departure date, and return date. 4. Click the search button to view available flight information. 5. After selecting a suitable flight, click the ‚ÄúBook‚Äù button. 6. Fill in passenger information and choose a payment method. 7. Confirm the order details and submit the payment to complete the purchase. Ming-Lite-Omni V1.5 The specific steps to purchase a plane ticket using the Ctrip app are as follows: 1. Download and Install the Ctrip App: - If you haven‚Äôt installed the Ctrip App, you can search for ‚ÄúCtrip Travel‚Äù (Êê∫Á®ãÊóÖË°å) in the app store (like the Apple App Store or Android app markets) and download it. 2. Open the Ctrip App: - After installation is complete, open the Ctrip app. 3. Select the ‚ÄúFlights‚Äù Option: - On the homepage navigation bar, tap the ‚ÄúFlights‚Äù option. 4. Enter Departure and Destination Cities: - On the flight booking page, enter your departure and destination cities. 5. Select Departure and Return Dates: - Choose your departure date and return date (if booking a round trip). 6. Select Passenger Information: - Enter the number of passengers and their types (adult, child, infant). 7. Choose a Flight: - Browse and select your preferred flight. 8. Fill in Passenger Information: - Enter the passenger‚Äôs name, ID number, contact information, etc. 9. Select a Payment Method: - Choose your preferred payment method (e.g., Alipay, WeChat Pay, bank card, etc.). 10. Confirm Order and Pay: - After confirming that the order information is correct, tap the ‚ÄúPay‚Äù button to complete the payment. 11. Receive Your E-ticket: - After successful payment, you will receive an e-ticket, which you can view or download from the ‚ÄúMy Orders‚Äù section within the app. By following these steps, you can easily purchase plane tickets using the Ctrip App. User Based on the chart, under which primer is the Œîct difference between samples the smallest? | Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer. (Image source: Publicly available online - ËàíÊ°êÁßëÊäÄÂÖ¨‰ºóÂè∑) Ming-Lite-Omni Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer. Ming-Lite-Omni V1.5 The difference in Œîct between samples is smallest under the EGFP-cGAS monoclonal primer. Calculation Process: 1. Understand the meaning of Œîct: Œîct is the difference in relative expression between the target gene (cGAS-qF+qR or EGFP-cGAS) and the reference gene (cTubulin). A smaller Œîct indicates a smaller difference in expression between the target and reference genes. 2. Compare Œîct under different primers: - cGAS-qF+qR primer: - Wild type: Œîct = 8.06 - EGFP-cGAS monoclonal: Œîct = -0.15 - Difference: 8.06 - (-0.15) = 8.21 - EGFP-cGAS monoclonal primer: - Wild type: Œîct = 10.44 - EGFP-cGAS monoclonal: Œîct = -2.9 - Difference: 10.44 - (-2.9) = 13.34 - EGFP-cGAS-qF+qR primer: - Wild type: Œîct = 21.43 - EGFP-cGAS monoclonal: Œîct = -13.45 - Difference: 21.43 - (-13.45) = 34.88 3. Conclusion: Under the EGFP-cGAS monoclonal primer, the difference in Œîct between samples is the smallest (Œîct difference is 13.34), indicating the smallest difference in expression between samples under this primer. Summary: The difference in Œîct between samples is smallest under the EGFP-cGAS monoclonal primer. Speech Generation Text Inputs Ming-lite-omni Ming-lite-omni V1.5 Tyler, Lucy, Michelle, we‚Äôre going to space! what is the forecast for California for rain Television can make you dumb, but it can also be good education. OCR Next User In true/false question 95, what is the candidate‚Äôs handwritten answer? If the handwritten answer is ‚àö, output True. If the handwritten answer is √ó, output False. Please output True or False Ming-Lite-Omni True Ming-Lite-Omni V1.5 False User Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters. Ming-Lite-Omni 9743 Ming-Lite-Omni V1.5 9742 User What is the Latex tag for mathematical expression in images? Ming-Lite-Omni A C = C B \\therefore C D / / B E Ming-Lite-Omni V1.5 A C = C B \\because C D / / B E User ‰ªéÂõæ‰∏≠ÊèêÂèñ: Êó•Êúü Ming-Lite-Omni Êó•ÊúüÔºö2019Âπ¥12Êúà03Êó•05Êó∂08ÂàÜ ‰ªéÂõæ‰∏≠ÊèêÂèñ: ÈáåÁ®ã ÈáåÁ®ãÔºö23.9ÂÖ¨Èáå ‰ªéÂõæ‰∏≠ÊèêÂèñ: ÈáëÈ¢ù ÈáëÈ¢ùÔºö52.6ÂÖÉ ‰ªéÂõæ‰∏≠ÊèêÂèñ: Áä∂ÊÄÅ Áä∂ÊÄÅÔºö ‰ªéÂõæ‰∏≠ÊèêÂèñ: ÂÆ¢Á•®Âç∞Âä° ÂÆ¢Á•®Âç∞Âä°2019Âπ¥12ÊúàÂç∞900000Âç∑ ‰ªéÂõæ‰∏≠ÊèêÂèñ: Âè∑Á†Å Âè∑Á†Å‰ªé00000001Ëá≥90000000Ê≠¢ Ming-Lite-Omni V1.5 Êó•Êúü: 2021Âπ¥03Êúà05Êó• User What is the difference in value between Lamb and Corn?‚Äô You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \\boxed{}.' Ming-Lite-Omni 0.37 Ming-Lite-Omni V1.5 [think]The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.[/think]\\boxed{0.57} Video Understanding Users What is the little girl doing in the video? Ming-Lite-Omni The scene in the video takes place in a store, possibly a supermarket or large retail store, with shelves filled with various products including snacks and household items. The little girl in the video is pulling a display rack, apparently trying to move or play with it. Price tags are clearly visible, with one prominently displaying '18,690' in blue and white. The store is well-lit with fluorescent lights illuminating the entire area. If you need more detailed analysis or other assistance, please let me know! Ming-Lite-Omni V1.5 The little girl is sleeping in a shopping cart. Users Describe the protagonist's clothing in the video. Ming-Lite-Omni In the video, the protagonist is wearing a red top and carrying a bag with colorful patterns on their shoulder. Ming-Lite-Omni V1.5 The protagonist in the video is wearing a red turtleneck sweater. Users Guess the time and location where this video was filmed. Ming-Lite-Omni Judging from the scenes in the video, this was likely filmed in New York's Central Park. The video shows iconic buildings and landscapes of Central Park, such as its lakes, bridges, and statues. Additionally, there are tourists walking, taking photos, and enjoying outdoor activities in the park, which further confirms that the filming location is New York's Central Park. Ming-Lite-Omni V1.5 From the scenes in the video, the filming location appears to be New York's Central Park. The video showcases multiple attractions within the park, including lakes, bridges, statues, and fountains. The filming time seems to be during the daytime, with bright sunshine and clear weather, making it suitable for outdoor activities. Get Started with Ming-lite-omni v1.5 The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we‚Äôre excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!\nGithub: https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5 ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5 ","wordCount":"3021","inLanguage":"en","datePublished":"2025-07-21T00:00:03+08:00","dateModified":"2025-07-21T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Introducing Ming-Lite-Omni V1.5</h1><div class=post-meta><span title='2025-07-21 00:00:03 +0800 +0800'>July 21, 2025</span>&nbsp;¬∑&nbsp;15 min&nbsp;¬∑&nbsp;3021 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> ü§ó <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>ÔΩú ü§ñ <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><p>We are excited to introduce Ming-lite-omni V1.5, a comprehensive upgrade that significantly enhances the omni-modal capabilities of the original Ming-lite-omni model (find it on ü§ó<a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>Hugging Face</a>). This new version delivers remarkable improvements across a wide range of tasks, including image and text understanding, document analysis, video comprehension, speech understanding and synthesis, as well as image generation and editing.</p><p>Built on the Ling-lite-1.5 architecture, Ming-lite-omni V1.5 features a total of 20.3B parameters, with 3B active parameters in its Mixture of Experts (MoE) configuration. It has achieved impressive results across various multimodal benchmarks. Below, we highlight the key advancements in performance metrics and model architecture from this latest update.</p><div style=text-align:center;margin:auto;width:70%><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/dv7TTL8MW5EAAAAAWtAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>Performance Comparison</p></div><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/A_LZTJcsc3EAAAAAXuAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>Framework</p></div><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>To achieve these advancements, we integrated our in-house innovations with cutting-edge developments from both academia and the open-source community. We explored several key areas, leading to significant breakthroughs and key findings detailed below.</p><h3 id=image-and-speech-generation>Image and Speech Generation<a hidden class=anchor aria-hidden=true href=#image-and-speech-generation>#</a></h3><ol><li><strong>Dual-Branch Decoupling for Image Generation:</strong> We implemented a <strong>dual-branch decoupling strategy</strong> that processes the reference image and the noise image with separate weights before they enter the Diffusion Transformer (DiT). This approach effectively increases the model&rsquo;s capacity for learning from the reference. To further refine the output, we added two extra transformer layers.</li><li><strong>ID & Scene Consistency Loss:</strong> To maintain character and scene identity during image editing, we introduced a new <strong>ID & Scene Consistency Loss</strong>. This function strategically increases focus on the edited region in the target image, boosts the influence of unedited areas from the reference image, and diminishes the influence of the edited areas from the reference.</li><li><strong>Perceptual Enhancement Strategy:</strong> We introduced a <strong>Perceptual Enhancement Strategy</strong> by improving the model&rsquo;s ability to perceive structures through tasks like segmentation and keypoint detection. This provides greater structural control during generation and editing, leading to significant score improvements on metrics related to object position, structure, and count (see <a href=#table1>Table A</a> for details).</li><li><strong>Multi-task Collaborative Learning:</strong> A <strong>Multi-task Collaborative Learning</strong> approach allows generation and editing tasks to synergistically improve one another. By treating segmentation as a colorization-based editing task within a unified training pipeline, we achieved significant gains in segmentation accuracy and enhanced the precision of local edits, resulting in smoother and more natural transitions at the edges of edited areas.</li><li><strong>New Audio Decoder:</strong> On the speech generation front, we developed a novel audio decoder that is fully context-aware, directly processing output features from the LLM.</li><li><strong>Efficient Speech Generation:</strong> To improve prosody and real-time generation capabilities, we applied BPE encoding to the discrete audio codec tokens, which successfully reduced the audio frame rate by 35%.</li><li><strong>Comprehensive Data Upgrades:</strong><ul><li>Sourced high-quality portrait data, filtered by resolution, facial detail, and face size.</li><li>Collected and curated a high-quality celebrity dataset with processed face crops.</li><li>Expanded the model&rsquo;s capabilities by building new training subsets for edge maps, segmentation maps, text-in-image, and facial expressions.</li></ul></li></ol><h3 id=image-text-video-and-speech-understanding>Image, Text, Video, and Speech Understanding<a hidden class=anchor aria-hidden=true href=#image-text-video-and-speech-understanding>#</a></h3><ol><li><strong>MRoPE for Spatiotemporal Awareness:</strong> We integrated MRoPE (Multi-dimensional RoPE), which applies positional encoding across three dimensions (time, height, and width). This equips the model with a native understanding of space and time, enabling highly efficient cross-modal modeling and significantly boosting comprehension accuracy for videos and complex images.</li><li><strong>Efficient Full-Parameter Training:</strong> By optimizing learning rates and the multimodal data mixture, we upgraded our pre-training from a phased approach of <strong>freezing/unfreezing LLM layers</strong> to a streamlined, <strong>efficient full-parameter training</strong> regimen. This shortened the training cycle by 26.5% with no loss in performance.</li><li><strong>Video Understanding via Curriculum Learning:</strong> We adopted a curriculum learning strategy for video understanding, progressively increasing the complexity from short-form to long-form videos to enhance the model&rsquo;s capabilities.</li><li><strong>Complex Document Understanding with CoT:</strong> For complex document analysis, we incorporated a Chain-of-Thought (CoT) strategy to build structured reasoning paths, effectively improving the model&rsquo;s problem-solving abilities on intricate queries.</li><li><strong>Comprehensive Data Upgrades:</strong><ul><li><strong>Pre-training:</strong><ul><li>Added structured text entity data to fill knowledge graph gaps.</li><li>Expanded with high-quality product data to enhance general knowledge.</li></ul></li><li><strong>Instruction Fine-Tuning:</strong><ul><li>Improved data precision for fine-grained visual perception (e.g., object counting, color, and scene recognition).</li><li>Increased data depth for specialized domain recognition (e.g., flora/fauna, vehicles, and ingredients).</li><li>Enhanced cross-disciplinary reasoning in complex visual-text tasks through targeted data optimization.</li><li>For speech understanding, we infused instruction text with metadata like domain, topic, and language (including dialects). This boosted model performance and enabled comprehensive support for Mandarin, English, Cantonese, Sichuanese, Shanghainese, and Minnan.</li></ul></li></ul></li></ol><h3 id=aligning-with-user-preferences>Aligning with User Preferences<a hidden class=anchor aria-hidden=true href=#aligning-with-user-preferences>#</a></h3><p>To ensure that improvements on standard benchmarks translate directly to a superior real-world user experience, we created our own <strong>human evaluation benchmark</strong> and conducted internal, head-to-head evaluations against other models. Thanks to our high-quality preference alignment data, Ming-lite-omni V1.5 demonstrates a distinct advantage over leading models in the accuracy (low hallucination rate), relevance, formatting, and fluency of its visual question-answering responses. On our internal adversarial test set, Ming-lite-omni V1.5 achieved an <strong>87.07% win rate</strong> against Ming-lite-omni V1, marking a significant enhancement in user experience.</p><div style=text-align:center;margin:auto;width:80%;line-height:1.1><table style="margin:0 auto"><thead><tr><th style=text-align:center;padding:5px>Benchmark Dimension</th><th style=text-align:center;padding:5px>Qwen2.5-VL-7B</th><th style=text-align:center;padding:5px>Ming-Omni-Lite V1.5</th></tr></thead><tbody><tr><td style=text-align:center;padding:2px>Relevance</td><td style=text-align:center;padding:2px>4.308</td><td style=text-align:center;padding:2px>4.5</td></tr><tr><td style=text-align:center;padding:2px>Fluency</td><td style=text-align:center;padding:2px>4.765</td><td style=text-align:center;padding:2px>4.91</td></tr><tr><td style=text-align:center;padding:2px>Richness</td><td style=text-align:center;padding:2px>3.828</td><td style=text-align:center;padding:2px>3.69</td></tr><tr><td style=text-align:center;padding:2px>Formatting</td><td style=text-align:center;padding:2px>4.727</td><td style=text-align:center;padding:2px>4.8</td></tr><tr><td style=text-align:center;padding:2px>Accuracy</td><td style=text-align:center;padding:2px>3.741</td><td style=text-align:center;padding:2px>3.92</td></tr><tr><td style=text-align:center;padding:2px><strong>Average</strong></td><td style=text-align:center;padding:2px><strong>4.274</strong></td><td style=text-align:center;padding:2px><strong>4.365</strong></td></tr></tbody></table></div><br><h2 id=demos>Demos<a hidden class=anchor aria-hidden=true href=#demos>#</a></h2><h3 id=image-editing>Image Editing<a hidden class=anchor aria-hidden=true href=#image-editing>#</a></h3><p>To address the challenge of maintaining character and scene identity during image editing, we&rsquo;ve introduced the <strong>ID & Scene Consistency Loss</strong>. This function strategically increases the weight of the edited region in the target image, boosts the influence of unedited areas from the reference image, and reduces the influence of the edited areas within that same reference.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/n66PSYmtwHcAAAAAY_AAAAgAeuUHAQFr/fmt.webp alt="Image description">
<video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/UoqbRYQnZYEAAAAAgCAAAAgAeuUHAQFr controls></video></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>Generative Image Segmentation</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp height=256px></figure>Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different classes in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different instances in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp height=256px></figure></div></div></div></div><h3 id=image-generation>Image Generation<a hidden class=anchor aria-hidden=true href=#image-generation>#</a></h3><p>We introduced a <strong>Perceptual Enhancement Strategy</strong> by optimizing the model&rsquo;s structural awareness through tasks like segmentation and keypoint detection. This improves its understanding of image details and spatial relationships, giving us greater structural control over the generation and editing process. The result is a significant boost in scores on the GenEval benchmark, particularly for metrics related to position, structure, and count:
<a id=table1></a></p><table><thead><tr><th></th><th style=text-align:center>1-Obj</th><th style=text-align:center>2-Obj</th><th style=text-align:center>Counting</th><th style=text-align:center>Colors</th><th style=text-align:center>Position</th><th style=text-align:center>Color Attr</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td>Ming-lite-omni</td><td style=text-align:center>0.99</td><td style=text-align:center>0.77</td><td style=text-align:center>0.68</td><td style=text-align:center>0.78</td><td style=text-align:center>0.46</td><td style=text-align:center>0.42</td><td style=text-align:center>0.64</td></tr><tr><td>Ming-lite-omni V1.5</td><td style=text-align:center>0.99</td><td style=text-align:center>0.93</td><td style=text-align:center>0.86</td><td style=text-align:center>0.87</td><td style=text-align:center>0.90</td><td style=text-align:center>0.66</td><td style=text-align:center>0.87</td></tr></tbody></table><table><thead><tr><th style=text-align:center>Original Image</th><th style=text-align:center>Generated Depth Map</th><th style=text-align:center>Generated Bounding Boxes</th><th style=text-align:center>Generated Edge Map</th></tr></thead><tbody><tr><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>Visual Dialogue</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/2.jpg height=256px></figure>This picture looks very design-oriented. Can you describe its various design elements in detail?</div><div class=role>Ming-Lite-Omni</div><div class=content>This image shows a high-end gaming mouse with a modern and technological design. The mouse body is black with a smooth metallic finish. There is a blue glowing logo on the top of the mouse, and below the logo is the mouse wheel and left and right button area. The side of the mouse has a blue light decoration, which increases the visual appeal. The bottom of the mouse has a non-slip texture design to ensure stability during use. The overall design is simple and futuristic, suitable for gamers.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>This image showcases a gaming mouse with a striking design, featuring the following design elements:<br>1. <strong>Color and Material</strong>: - The main body of the mouse is black, with a smooth surface and a metallic finish. - The edges and button areas of the mouse have blue LED light strips, adding a high-tech feel and visual appeal.<br>2. <strong>Shape and Structure</strong>: - The mouse has an overall streamlined design, with a slightly sloped top that allows for natural hand placement. - The sides of the mouse feature pronounced curves, designed ergonomically to provide a comfortable grip.<br>3. <strong>Buttons and Scroll Wheel</strong>: - There is a scroll wheel on the left side of the mouse, with a blue LED light strip above it. - There are additional buttons on the right side of the mouse, which also have blue LED light strips, enhancing both functionality and aesthetics.<br>4. <strong>Brand Logo</strong>: - In the center of the mouse&rsquo;s palm rest, there is an illuminated &ldquo;G&rdquo; logo, which is a signature design element of the brand.<br>5. <strong>Lighting Effects</strong>: - The blue LED light strips on the mouse stand out prominently against the dark background, enhancing its high-tech and futuristic feel.<br>6. <strong>Texture and Details</strong>: - The top of the mouse has a textured design, likely for an anti-slip effect. - The sides of the mouse feature fine, elongated textured patterns, which add a sense of visual depth.<br>Overall, the design of this mouse combines a modern, high-tech aesthetic with ergonomic design, making it both beautiful and practical, and well-suited for gamers.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/3.webp height=256px></figure>Why is the calligraphy in the picture good?</div><div class=role>Ming-Lite-Omni</div><div class=content>This calligraphy work showcases superb calligraphic skills and profound cultural heritage. The brushwork is fluid and natural, the character structures are rigorous, and the layout is well-proportioned, reflecting the calligrapher&rsquo;s deep skill and artistic cultivation.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>This piece of calligraphy displays several notable characteristics that make it an excellent work of art:<br>1. <strong>Exquisite Brushwork</strong>: The calligraphy&rsquo;s brushwork is fluid and natural, with rich variations in line thickness, demonstrating the calligrapher&rsquo;s profound skill and proficient technique.<br>2. <strong>Rigorous Structure</strong>: The structure of each character is meticulously designed, with natural connections between strokes, maintaining a good balance and harmony between characters.<br>3. <strong>Composition and Layout</strong>: The overall composition of the piece is well-arranged, with even line and character spacing, creating a sense of order and neatness.<br>4. <strong>Ink Tone Variation</strong>: The variations in ink tone, from dark to light, are natural, adding layers and a three-dimensional feel to the work.<br>5. <strong>Elegant Content</strong>: The content of the calligraphy is a beautiful classical text, rich in philosophical and literary value, which enhances the cultural depth of the work.<br>6. <strong>Seal Embellishment</strong>: The red seals on the work not only add a decorative element but also serve to embellish and balance the composition.<br>Together, these features constitute the high artistic value of this calligraphy piece, making it a masterpiece worthy of appreciation and study.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/4.webp height=256px></figure>Is the phone in the picture suitable to buy for parents?</div><div class=role>Ming-Lite-Omni</div><div class=content>This phone has a simple and stylish appearance, suitable for parents who prefer a minimalist style. The camera configuration is also quite advanced, making it suitable for users who enjoy taking photos.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>This phone is suitable for buying for parents for the following reasons:<br>1. <strong>Appearance Design</strong>: The phone features a simple and stylish design. The white body looks clean and neat, aligning with modern aesthetics.<br>2. <strong>Camera Configuration</strong>: The rear camera has a unique and neat arrangement, which may provide a good photography experience.<br>3. <strong>Brand</strong>: The Honor (Ëç£ËÄÄ) brand has a good reputation in the smartphone market, offering stable and reliable products.<br>4. <strong>Features</strong>: Phones of this type are typically equipped with a large-capacity battery and long battery life, suitable for daily use.<br>5. <strong>Ease of Use</strong>: It may offer a simple user interface and easy-to-operate functions, suitable for users who are not familiar with complex operations.<br>6. <strong>Value for Money</strong>: Honor phones are generally known for their high cost-performance ratio, making them suitable for users on a limited budget who still want a good experience.<br>Of course, whether it is truly suitable depends on the actual needs and preferences of the parents.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/6.webp height=256px></figure>How to buy a plane ticket using the software in the picture? Please list the specific steps</div><div class=role>Ming-Lite-Omni</div><div class=content>The specific steps to buy a plane ticket using the software in the picture are as follows:<br>1. Open the Ctrip app. 2. Click on the &ldquo;Flights&rdquo; option. 3. On the flight page, select the departure city, destination, departure date, and return date. 4. Click the search button to view available flight information. 5. After selecting a suitable flight, click the &ldquo;Book&rdquo; button. 6. Fill in passenger information and choose a payment method. 7. Confirm the order details and submit the payment to complete the purchase.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>The specific steps to purchase a plane ticket using the Ctrip app are as follows:<br>1. <strong>Download and Install the Ctrip App</strong>: - If you haven&rsquo;t installed the Ctrip App, you can search for &ldquo;Ctrip Travel&rdquo; (Êê∫Á®ãÊóÖË°å) in the app store (like the Apple App Store or Android app markets) and download it.<br>2. <strong>Open the Ctrip App</strong>: - After installation is complete, open the Ctrip app.<br>3. <strong>Select the &ldquo;Flights&rdquo; Option</strong>: - On the homepage navigation bar, tap the &ldquo;Flights&rdquo; option.<br>4. <strong>Enter Departure and Destination Cities</strong>: - On the flight booking page, enter your departure and destination cities.<br>5. <strong>Select Departure and Return Dates</strong>: - Choose your departure date and return date (if booking a round trip).<br>6. <strong>Select Passenger Information</strong>: - Enter the number of passengers and their types (adult, child, infant).<br>7. <strong>Choose a Flight</strong>: - Browse and select your preferred flight.<br>8. <strong>Fill in Passenger Information</strong>: - Enter the passenger&rsquo;s name, ID number, contact information, etc.<br>9. <strong>Select a Payment Method</strong>: - Choose your preferred payment method (e.g., Alipay, WeChat Pay, bank card, etc.).<br>10. <strong>Confirm Order and Pay</strong>: - After confirming that the order information is correct, tap the &ldquo;Pay&rdquo; button to complete the payment.<br>11. <strong>Receive Your E-ticket</strong>: - After successful payment, you will receive an e-ticket, which you can view or download from the &ldquo;My Orders&rdquo; section within the app.<br>By following these steps, you can easily purchase plane tickets using the Ctrip App.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/7.webp height=128px></figure>Based on the chart, under which primer is the Œîct difference between samples the smallest? | Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer. (Image source: Publicly available online - ËàíÊ°êÁßëÊäÄÂÖ¨‰ºóÂè∑)</div><div class=role>Ming-Lite-Omni</div><div class=content>Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>The difference in Œîct between samples is smallest under the <strong>EGFP-cGAS monoclonal</strong> primer.<br><br><strong>Calculation Process:</strong><br><strong>1. Understand the meaning of Œîct:</strong> Œîct is the difference in relative expression between the target gene (cGAS-qF+qR or EGFP-cGAS) and the reference gene (cTubulin). A smaller Œîct indicates a smaller difference in expression between the target and reference genes.<br><strong>2. Compare Œîct under different primers:</strong><br>- <strong>cGAS-qF+qR primer:</strong> - Wild type: Œîct = 8.06 - EGFP-cGAS monoclonal: Œîct = -0.15 - Difference: 8.06 - (-0.15) = 8.21<br>- <strong>EGFP-cGAS monoclonal primer:</strong> - Wild type: Œîct = 10.44 - EGFP-cGAS monoclonal: Œîct = -2.9 - Difference: 10.44 - (-2.9) = 13.34<br>- <strong>EGFP-cGAS-qF+qR primer:</strong> - Wild type: Œîct = 21.43 - EGFP-cGAS monoclonal: Œîct = -13.45 - Difference: 21.43 - (-13.45) = 34.88<br><strong>3. Conclusion:</strong> Under the <strong>EGFP-cGAS monoclonal</strong> primer, the difference in Œîct between samples is the smallest (Œîct difference is 13.34), indicating the smallest difference in expression between samples under this primer.<br><br><strong>Summary:</strong> The difference in Œîct between samples is smallest under the <strong>EGFP-cGAS monoclonal</strong> primer.</div></div></div></div><h3 id=speech-generation>Speech Generation<a hidden class=anchor aria-hidden=true href=#speech-generation>#</a></h3><table><thead><tr><th>Text Inputs</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Ming-lite-omni V1.5</th></tr></thead><tbody><tr><td>Tyler, Lucy, Michelle, we&rsquo;re going to space!</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.5.wav type=audio/wav></audio></td></tr><tr><td>what is the forecast for California for rain</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.5.wav type=audio/wav></audio></td></tr><tr><td>Television can make you dumb, but it can also be good education.</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.5.wav type=audio/wav></audio></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>OCR</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/9.webp height=320px></figure>In true/false question 95, what is the candidate&rsquo;s handwritten answer? If the handwritten answer is ‚àö, output True. If the handwritten answer is √ó, output False. Please output True or False</div><div class=role>Ming-Lite-Omni</div><div class=content>True</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>False</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/10.webp height=64px></figure>Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters.</div><div class=role>Ming-Lite-Omni</div><div class=content>9743</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>9742</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/11.webp height=64px></figure>What is the Latex tag for mathematical expression in images?</div><div class=role>Ming-Lite-Omni</div><div class=content>A C = C B \therefore C D / / B E</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>A C = C B \because C D / / B E</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/12.webp height=320px></figure>‰ªéÂõæ‰∏≠ÊèêÂèñ: Êó•Êúü</div><div class=role>Ming-Lite-Omni</div><div class=content>Êó•ÊúüÔºö2019Âπ¥12Êúà03Êó•05Êó∂08ÂàÜ ‰ªéÂõæ‰∏≠ÊèêÂèñ: ÈáåÁ®ã ÈáåÁ®ãÔºö23.9ÂÖ¨Èáå ‰ªéÂõæ‰∏≠ÊèêÂèñ: ÈáëÈ¢ù ÈáëÈ¢ùÔºö52.6ÂÖÉ ‰ªéÂõæ‰∏≠ÊèêÂèñ: Áä∂ÊÄÅ Áä∂ÊÄÅÔºö ‰ªéÂõæ‰∏≠ÊèêÂèñ: ÂÆ¢Á•®Âç∞Âä° ÂÆ¢Á•®Âç∞Âä°2019Âπ¥12ÊúàÂç∞900000Âç∑ ‰ªéÂõæ‰∏≠ÊèêÂèñ: Âè∑Á†Å Âè∑Á†Å‰ªé00000001Ëá≥90000000Ê≠¢</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>Êó•Êúü: 2021Âπ¥03Êúà05Êó•</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/13.webp height=320px></figure>What is the difference in value between Lamb and Corn?&rsquo; You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \boxed{}.'</div><div class=role>Ming-Lite-Omni</div><div class=content>0.37</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>[think]<br>The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.<br>[/think]<br>\boxed{0.57}</div></div></div></div><h3 id=video-understanding>Video Understanding<a hidden class=anchor aria-hidden=true href=#video-understanding>#</a></h3><table style="border-collapse:collapse;width:100%;margin:20px auto;border:none;font-size:14.4px"><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Users</td><td style=padding:10px;text-align:left;vertical-align:top;border:none><video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/tMqiS4N2yPAAAAAAgEAAAAgAeuUHAQFr height=256px controls></video><br>What is the little girl doing in the video?</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>The scene in the video takes place in a store, possibly a supermarket or large retail store, with shelves filled with various products including snacks and household items. The little girl in the video is pulling a display rack, apparently trying to move or play with it. Price tags are clearly visible, with one prominently displaying '18,690' in blue and white. The store is well-lit with fluorescent lights illuminating the entire area. If you need more detailed analysis or other assistance, please let me know!</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni V1.5</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>The little girl is sleeping in a shopping cart.</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Users</td><td style=padding:10px;text-align:left;vertical-align:top;border:none><video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/NGX_T5E13IAAAAAAgIAAAAgAfoeUAQBr height=192px controls></video><br>Describe the protagonist's clothing in the video.</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>In the video, the protagonist is wearing a red top and carrying a bag with colorful patterns on their shoulder.</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni V1.5</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>The protagonist in the video is wearing a red turtleneck sweater.</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Users</td><td style=padding:10px;text-align:left;vertical-align:top;border:none><video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/KD6LTqp9VTsAAAAAgMAAAAgAfoeUAQBr height=192px controls></video><br>Guess the time and location where this video was filmed.</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>Judging from the scenes in the video, this was likely filmed in New York's Central Park. The video shows iconic buildings and landscapes of Central Park, such as its lakes, bridges, and statues. Additionally, there are tourists walking, taking photos, and enjoying outdoor activities in the park, which further confirms that the filming location is New York's Central Park.</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni V1.5</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>From the scenes in the video, the filming location appears to be New York's Central Park. The video showcases multiple attractions within the park, including lakes, bridges, statues, and fountains. The filming time seems to be during the daytime, with bright sunshine and clear weather, making it suitable for outdoor activities.</td></tr></table><br><h2 id=get-started-with-ming-lite-omni-v15>Get Started with Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#get-started-with-ming-lite-omni-v15>#</a></h2><p>The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we&rsquo;re excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!</p><ul><li>Github: <a href=https://github.com/inclusionAI/Ming>https://github.com/inclusionAI/Ming</a></li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5</a></li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
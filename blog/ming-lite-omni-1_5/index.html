<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview
Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.419cab9a4e041985806269ccd5fb7e29179062491b1e90454454dcf1af0c9022.css integrity="sha256-QZyrmk4EGYWAYmnM1ft+KReQYkkbHpBFRFTc8a8MkCI=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Introducing Ming-Lite-Omni V1.5"><meta property="og:description" content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview
Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-21T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-21T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Ming-Lite-Omni V1.5"><meta name=twitter:description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview
Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Ming-Lite-Omni V1.5","item":"https://inclusionai.github.io/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Ming-Lite-Omni V1.5","name":"Introducing Ming-Lite-Omni V1.5","description":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nOverview Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models.\n","keywords":[],"articleBody":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nOverview Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models.\nPerformance Comparison\nIntroduce Ming-lite-omni v1.5 Controllable Image Generation: Pixel-Level Control, Infinite Creativity Ming-lite-omni v1.5 significantly optimizes Scene Consistency and ID Consistency (Character / Style Consistency) in image editing. When editing human figures, it demonstrates a clear advantage in maintaining scene and character ID. Furthermore, it expands support for perceptual tasks such as generative segmentation, depth prediction, object detection, and edge contour generation.\nGenerative Image Segmentation Next User Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. Ming-lite-omni v1.5 User Please segment different classes in this image. Ming-lite-omni v1.5 User Please segment different instances in this image. Ming-lite-omni v1.5 ** Depth and Edge Detection**\nOriginal Image Generated Depth Map Generated Bounding Boxes Generated Edge Contours Audio-Video Interactive Understanding Experience Alignment Optimization: Smarter Interaction That Understands You Better Visual Dialogue Next User This picture looks very design-oriented. Can you describe its various design elements in detail? Ming-Lite-Omni This image shows a high-end gaming mouse with a modern and technological design. The mouse body is black with a smooth metallic finish. There is a blue glowing logo on the top of the mouse, and below the logo is the mouse wheel and left and right button area. The side of the mouse has a blue light decoration, which increases the visual appeal. The bottom of the mouse has a non-slip texture design to ensure stability during use. The overall design is simple and futuristic, suitable for gamers. Ming-Lite-Omni V1.5 This image showcases a gaming mouse with a striking design, featuring the following design elements: 1. Color and Material: - The main body of the mouse is black, with a smooth surface and a metallic finish. - The edges and button areas of the mouse have blue LED light strips, adding a high-tech feel and visual appeal. 2. Shape and Structure: - The mouse has an overall streamlined design, with a slightly sloped top that allows for natural hand placement. - The sides of the mouse feature pronounced curves, designed ergonomically to provide a comfortable grip. 3. Buttons and Scroll Wheel: - There is a scroll wheel on the left side of the mouse, with a blue LED light strip above it. - There are additional buttons on the right side of the mouse, which also have blue LED light strips, enhancing both functionality and aesthetics. 4. Brand Logo: - In the center of the mouse‚Äôs palm rest, there is an illuminated ‚ÄúG‚Äù logo, which is a signature design element of the brand. 5. Lighting Effects: - The blue LED light strips on the mouse stand out prominently against the dark background, enhancing its high-tech and futuristic feel. 6. Texture and Details: - The top of the mouse has a textured design, likely for an anti-slip effect. - The sides of the mouse feature fine, elongated textured patterns, which add a sense of visual depth. Overall, the design of this mouse combines a modern, high-tech aesthetic with ergonomic design, making it both beautiful and practical, and well-suited for gamers. User Why is the calligraphy in the picture good? Ming-Lite-Omni This calligraphy work showcases superb calligraphic skills and profound cultural heritage. The brushwork is fluid and natural, the character structures are rigorous, and the layout is well-proportioned, reflecting the calligrapher‚Äôs deep skill and artistic cultivation. Ming-Lite-Omni V1.5 This piece of calligraphy displays several notable characteristics that make it an excellent work of art: 1. Exquisite Brushwork: The calligraphy‚Äôs brushwork is fluid and natural, with rich variations in line thickness, demonstrating the calligrapher‚Äôs profound skill and proficient technique. 2. Rigorous Structure: The structure of each character is meticulously designed, with natural connections between strokes, maintaining a good balance and harmony between characters. 3. Composition and Layout: The overall composition of the piece is well-arranged, with even line and character spacing, creating a sense of order and neatness. 4. Ink Tone Variation: The variations in ink tone, from dark to light, are natural, adding layers and a three-dimensional feel to the work. 5. Elegant Content: The content of the calligraphy is a beautiful classical text, rich in philosophical and literary value, which enhances the cultural depth of the work. 6. Seal Embellishment: The red seals on the work not only add a decorative element but also serve to embellish and balance the composition. Together, these features constitute the high artistic value of this calligraphy piece, making it a masterpiece worthy of appreciation and study. User Is the phone in the picture suitable to buy for parents? Ming-Lite-Omni This phone has a simple and stylish appearance, suitable for parents who prefer a minimalist style. The camera configuration is also quite advanced, making it suitable for users who enjoy taking photos. Ming-Lite-Omni V1.5 This phone is suitable for buying for parents for the following reasons: 1. Appearance Design: The phone features a simple and stylish design. The white body looks clean and neat, aligning with modern aesthetics. 2. Camera Configuration: The rear camera has a unique and neat arrangement, which may provide a good photography experience. 3. Brand: The Honor (Ëç£ËÄÄ) brand has a good reputation in the smartphone market, offering stable and reliable products. 4. Features: Phones of this type are typically equipped with a large-capacity battery and long battery life, suitable for daily use. 5. Ease of Use: It may offer a simple user interface and easy-to-operate functions, suitable for users who are not familiar with complex operations. 6. Value for Money: Honor phones are generally known for their high cost-performance ratio, making them suitable for users on a limited budget who still want a good experience. Of course, whether it is truly suitable depends on the actual needs and preferences of the parents. User How to buy a plane ticket using the software in the picture? Please list the specific steps Ming-Lite-Omni The specific steps to buy a plane ticket using the software in the picture are as follows: 1. Open the Ctrip app. 2. Click on the ‚ÄúFlights‚Äù option. 3. On the flight page, select the departure city, destination, departure date, and return date. 4. Click the search button to view available flight information. 5. After selecting a suitable flight, click the ‚ÄúBook‚Äù button. 6. Fill in passenger information and choose a payment method. 7. Confirm the order details and submit the payment to complete the purchase. Ming-Lite-Omni V1.5 The specific steps to purchase a plane ticket using the Ctrip app are as follows: 1. Download and Install the Ctrip App: - If you haven‚Äôt installed the Ctrip App, you can search for ‚ÄúCtrip Travel‚Äù (Êê∫Á®ãÊóÖË°å) in the app store (like the Apple App Store or Android app markets) and download it. 2. Open the Ctrip App: - After installation is complete, open the Ctrip app. 3. Select the ‚ÄúFlights‚Äù Option: - On the homepage navigation bar, tap the ‚ÄúFlights‚Äù option. 4. Enter Departure and Destination Cities: - On the flight booking page, enter your departure and destination cities. 5. Select Departure and Return Dates: - Choose your departure date and return date (if booking a round trip). 6. Select Passenger Information: - Enter the number of passengers and their types (adult, child, infant). 7. Choose a Flight: - Browse and select your preferred flight. 8. Fill in Passenger Information: - Enter the passenger‚Äôs name, ID number, contact information, etc. 9. Select a Payment Method: - Choose your preferred payment method (e.g., Alipay, WeChat Pay, bank card, etc.). 10. Confirm Order and Pay: - After confirming that the order information is correct, tap the ‚ÄúPay‚Äù button to complete the payment. 11. Receive Your E-ticket: - After successful payment, you will receive an e-ticket, which you can view or download from the ‚ÄúMy Orders‚Äù section within the app. By following these steps, you can easily purchase plane tickets using the Ctrip App. User Based on the chart, under which primer is the Œîct difference between samples the smallest? | Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer. (Image source: Publicly available online - ËàíÊ°êÁßëÊäÄÂÖ¨‰ºóÂè∑) Ming-Lite-Omni Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer. Ming-Lite-Omni V1.5 The difference in Œîct between samples is smallest under the EGFP-cGAS monoclonal primer. Calculation Process: 1. Understand the meaning of Œîct: Œîct is the difference in relative expression between the target gene (cGAS-qF+qR or EGFP-cGAS) and the reference gene (cTubulin). A smaller Œîct indicates a smaller difference in expression between the target and reference genes. 2. Compare Œîct under different primers: - cGAS-qF+qR primer: - Wild type: Œîct = 8.06 - EGFP-cGAS monoclonal: Œîct = -0.15 - Difference: 8.06 - (-0.15) = 8.21 - EGFP-cGAS monoclonal primer: - Wild type: Œîct = 10.44 - EGFP-cGAS monoclonal: Œîct = -2.9 - Difference: 10.44 - (-2.9) = 13.34 - EGFP-cGAS-qF+qR primer: - Wild type: Œîct = 21.43 - EGFP-cGAS monoclonal: Œîct = -13.45 - Difference: 21.43 - (-13.45) = 34.88 3. Conclusion: Under the EGFP-cGAS monoclonal primer, the difference in Œîct between samples is the smallest (Œîct difference is 13.34), indicating the smallest difference in expression between samples under this primer. Summary: The difference in Œîct between samples is smallest under the EGFP-cGAS monoclonal primer. Model Architecture Upgrade and Capability Evaluation The Ming-lite-omni v1.5 model architecture is outlined below. The core design references the structure of Ming-lite-omni V1. However, a key distinction is the upgrade of the Vision head to support reference image feature input, specifically to enhance character and scene consistency in image editing.\nMode Architecture\nThe model‚Äôs capabilities have been significantly optimized and upgraded across three key areas: enhanced Omni-modal comprehension, precise visual editing control, and improved user experience.\nEnhanced Omni-Modal Comprehension Thanks to optimized data quality, Ming-lite-omni v1.5 shows significant improvements in tasks such as vision-text comprehension (including image-text, document, and video understanding) and speech understanding. It has reached an industry-leading level for models of comparable scale.\nVision-text Comprehension\nTask Type Dataset Qwen2.5-VL-7B Ming-lite-omni Ming-lite-omni v1.5 Image-text Understanding AI2D 84.36 83.1 84.91 HallusionBench 55.77 55.0 54.59 MMBench_TEST_V11 82.75 80.8 80.73 MMMU 56.56 56.3 54.33 MMStar 65.27 64.7 65.07 MMVet 71.61 71.3 73.99 MathVista 68.10 71.6 72.00 OCRBench 87.80 88.4 88.90 Average 71.5 71.4 71.8 Video Understanding VideoMME(w/o subs) 65.10 63.4 67.07 VideoMME(w/ subs) 71.60 66.01 72.59 VideoMME(avg) 68.35 67.7 69.83 MVBench 69.60 67.7 69.43 LongVideoBench 56.00 56.6 59.54 OvOBench 51.10 48.48 52.17 Average 61.26 58.89 62.74 Document Understanding ChartQA_test 87.24 85.1 88.84 DocVQA_test 95.57 93 93.68 TextVQA_val 85.06 82.8 82.27 OCRBench 87.8 88.4 88.9 Average 88.91 87.32 88.42 Speech Understanding\nModel Average(Open-ended QA) AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Ming-lite-omni v1.5 4.474 4.648 4.3 61.16 45.77 65.934 55.599 98.076 Ming-lite-omni 4.34 4.63 4.06 58.84 47.53 61.98 58.36 99.04 MiniCPM-o 4.285 4.42 4.15 50.72 54.78 78.02 49.25 97.69 Kimi-Audio 4.215 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.21 4.49 3.93 55.71 61.32 81.10 52.87 99.42 GLM-4-Voice 3.77 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Precise Visual Editing Control Ming-lite-omni v1.5 employs the following optimization strategies to address the issues of character ID and scene ID consistency during image editing:\nID and Scene Consistency Loss: This is achieved by increasing the weight of the edited region in the target image and the reference strength of the non-edited region in the reference image, while simultaneously decreasing the reference strength of the edited region in the reference image. This approach enhances image editing consistency. Incorporating Generative Detection and Segmentation Tasks to Boost Perceptual Capabilities: By supporting generative segmentation and keypoint detection, the model‚Äôs understanding of image details and spatial relationships is improved. This enhances the structural controllability of the editing and generation processes, leading to significant increases in evaluation metrics related to position, structure, and quantity. Multi-Task Collaborative Learning Strategy: Through a joint training pipeline, generation and editing mutually reinforce each other. Segmentation tasks are transformed into colorization editing tasks, which significantly improves segmentation metrics and the precision and controllability of local image editing, resulting in smoother edges for edited regions. Based on these optimizations, Ming-lite-omni v1.5 shows a significant improvement in image editing capabilities, achieving a GenEval score of 0.87.\n1-Obj 2-Obj Counting Colors Position Color Attr Avg. Ming-lite-omni 0.99 0.77 0.68 0.78 0.46 0.42 0.64 Ming-lite-omni v1.5 0.99 0.93 0.86 0.87 0.90 0.66 0.87 Optimized User Experience Thanks to the construction of high-quality alignment preference data, Ming-lite-omni v1.5 demonstrates a certain advantage over leading models in terms of correctness, relevance, format aesthetics, and fluency of expression for image-text Q\u0026A. Ming-lite-omni v1.5 achieved a win rate of 87.07% against Ming-lite-omni V1 on internal adversarial evaluation sets, indicating a significant optimization in user experience.\nEvaluation Dimension Qwen2.5-VL-7B Ming-lite-omni V1.5 Relevance 4.308 4.5 Fluency 4.765 4.91 Richness of Content 3.828 3.69 Format aesthetics 4.727 4.8 Correctness 3.741 3.92 Average Score 4.274 4.365 Get Started with Ming-lite-omni v1.5 The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we‚Äôre excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!\nGithub: https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5 ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5 ","wordCount":"2277","inLanguage":"en","datePublished":"2025-07-21T00:00:03+08:00","dateModified":"2025-07-21T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;¬ª&nbsp;<a href=https://inclusionai.github.io/blog/>Blog</a></div><h1 class=post-title>Introducing Ming-Lite-Omni V1.5</h1><div class=post-meta><span class=post-date title="2025-07-21 00:00:03 +0800 +0800">July 21, 2025</span>
<span class=post-word-count>2277 words</span>
<span class=post-author>inclusionAI, Ant Group</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></header><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> ü§ó <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>ÔΩú ü§ñ <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(<a href=https://github.com/inclusionAI/Ming/tree/v1.0>Github</a>). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models.</p><div style=text-align:center;margin:auto;width:100%><img src=https://mdn.alipayobjects.com/huamei_drbxn1/afts/img/tXyrSrRR09UAAAAAgCAAAAgADkliAQFr/original alt="Image description"><p style=font-size:14px;color:gray>Performance Comparison</p></div><h2 id=introduce-ming-lite-omni-v15>Introduce Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#introduce-ming-lite-omni-v15>#</a></h2><h3 id=controllable-image-generation-pixel-level-control-infinite-creativity>Controllable Image Generation: Pixel-Level Control, Infinite Creativity<a hidden class=anchor aria-hidden=true href=#controllable-image-generation-pixel-level-control-infinite-creativity>#</a></h3><p>Ming-lite-omni v1.5 significantly optimizes Scene Consistency and ID Consistency (Character / Style Consistency) in image editing. When editing human figures, it demonstrates a clear advantage in maintaining scene and character ID. Furthermore, it expands support for perceptual tasks such as generative segmentation, depth prediction, object detection, and edge contour generation.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/n66PSYmtwHcAAAAAY_AAAAgAeuUHAQFr/fmt.webp alt="Image description">
<video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/UoqbRYQnZYEAAAAAgCAAAAgAeuUHAQFr controls></video></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>Generative Image Segmentation</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp height=256px></figure>Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</div><div class=role>Ming-lite-omni v1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different classes in this image.</div><div class=role>Ming-lite-omni v1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different instances in this image.</div><div class=role>Ming-lite-omni v1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp height=256px></figure></div></div></div></div><p>** Depth and Edge Detection**</p><table><thead><tr><th style=text-align:center>Original Image</th><th style=text-align:center>Generated Depth Map</th><th style=text-align:center>Generated Bounding Boxes</th><th style=text-align:center>Generated Edge Contours</th></tr></thead><tbody><tr><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><h3 id=audio-video-interactive-understanding>Audio-Video Interactive Understanding<a hidden class=anchor aria-hidden=true href=#audio-video-interactive-understanding>#</a></h3><p><video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/TptZRJDixVUAAAAAhqAAAAgADkliAQFr width=540px height=800px controls></video></p><h3 id=experience-alignment-optimization-smarter-interaction-that-understands-you-better>Experience Alignment Optimization: Smarter Interaction That Understands You Better<a hidden class=anchor aria-hidden=true href=#experience-alignment-optimization-smarter-interaction-that-understands-you-better>#</a></h3><div class="full-width-container example-container"><div class=example-content><div class=title><span>Visual Dialogue</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/2.jpg height=256px></figure>This picture looks very design-oriented. Can you describe its various design elements in detail?</div><div class=role>Ming-Lite-Omni</div><div class=content>This image shows a high-end gaming mouse with a modern and technological design. The mouse body is black with a smooth metallic finish. There is a blue glowing logo on the top of the mouse, and below the logo is the mouse wheel and left and right button area. The side of the mouse has a blue light decoration, which increases the visual appeal. The bottom of the mouse has a non-slip texture design to ensure stability during use. The overall design is simple and futuristic, suitable for gamers.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>This image showcases a gaming mouse with a striking design, featuring the following design elements:<br>1. <strong>Color and Material</strong>: - The main body of the mouse is black, with a smooth surface and a metallic finish. - The edges and button areas of the mouse have blue LED light strips, adding a high-tech feel and visual appeal.<br>2. <strong>Shape and Structure</strong>: - The mouse has an overall streamlined design, with a slightly sloped top that allows for natural hand placement. - The sides of the mouse feature pronounced curves, designed ergonomically to provide a comfortable grip.<br>3. <strong>Buttons and Scroll Wheel</strong>: - There is a scroll wheel on the left side of the mouse, with a blue LED light strip above it. - There are additional buttons on the right side of the mouse, which also have blue LED light strips, enhancing both functionality and aesthetics.<br>4. <strong>Brand Logo</strong>: - In the center of the mouse&rsquo;s palm rest, there is an illuminated &ldquo;G&rdquo; logo, which is a signature design element of the brand.<br>5. <strong>Lighting Effects</strong>: - The blue LED light strips on the mouse stand out prominently against the dark background, enhancing its high-tech and futuristic feel.<br>6. <strong>Texture and Details</strong>: - The top of the mouse has a textured design, likely for an anti-slip effect. - The sides of the mouse feature fine, elongated textured patterns, which add a sense of visual depth.<br>Overall, the design of this mouse combines a modern, high-tech aesthetic with ergonomic design, making it both beautiful and practical, and well-suited for gamers.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/3.webp height=256px></figure>Why is the calligraphy in the picture good?</div><div class=role>Ming-Lite-Omni</div><div class=content>This calligraphy work showcases superb calligraphic skills and profound cultural heritage. The brushwork is fluid and natural, the character structures are rigorous, and the layout is well-proportioned, reflecting the calligrapher&rsquo;s deep skill and artistic cultivation.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>This piece of calligraphy displays several notable characteristics that make it an excellent work of art:<br>1. <strong>Exquisite Brushwork</strong>: The calligraphy&rsquo;s brushwork is fluid and natural, with rich variations in line thickness, demonstrating the calligrapher&rsquo;s profound skill and proficient technique.<br>2. <strong>Rigorous Structure</strong>: The structure of each character is meticulously designed, with natural connections between strokes, maintaining a good balance and harmony between characters.<br>3. <strong>Composition and Layout</strong>: The overall composition of the piece is well-arranged, with even line and character spacing, creating a sense of order and neatness.<br>4. <strong>Ink Tone Variation</strong>: The variations in ink tone, from dark to light, are natural, adding layers and a three-dimensional feel to the work.<br>5. <strong>Elegant Content</strong>: The content of the calligraphy is a beautiful classical text, rich in philosophical and literary value, which enhances the cultural depth of the work.<br>6. <strong>Seal Embellishment</strong>: The red seals on the work not only add a decorative element but also serve to embellish and balance the composition.<br>Together, these features constitute the high artistic value of this calligraphy piece, making it a masterpiece worthy of appreciation and study.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/4.webp height=256px></figure>Is the phone in the picture suitable to buy for parents?</div><div class=role>Ming-Lite-Omni</div><div class=content>This phone has a simple and stylish appearance, suitable for parents who prefer a minimalist style. The camera configuration is also quite advanced, making it suitable for users who enjoy taking photos.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>This phone is suitable for buying for parents for the following reasons:<br>1. <strong>Appearance Design</strong>: The phone features a simple and stylish design. The white body looks clean and neat, aligning with modern aesthetics.<br>2. <strong>Camera Configuration</strong>: The rear camera has a unique and neat arrangement, which may provide a good photography experience.<br>3. <strong>Brand</strong>: The Honor (Ëç£ËÄÄ) brand has a good reputation in the smartphone market, offering stable and reliable products.<br>4. <strong>Features</strong>: Phones of this type are typically equipped with a large-capacity battery and long battery life, suitable for daily use.<br>5. <strong>Ease of Use</strong>: It may offer a simple user interface and easy-to-operate functions, suitable for users who are not familiar with complex operations.<br>6. <strong>Value for Money</strong>: Honor phones are generally known for their high cost-performance ratio, making them suitable for users on a limited budget who still want a good experience.<br>Of course, whether it is truly suitable depends on the actual needs and preferences of the parents.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/6.webp height=256px></figure>How to buy a plane ticket using the software in the picture? Please list the specific steps</div><div class=role>Ming-Lite-Omni</div><div class=content>The specific steps to buy a plane ticket using the software in the picture are as follows:<br>1. Open the Ctrip app. 2. Click on the &ldquo;Flights&rdquo; option. 3. On the flight page, select the departure city, destination, departure date, and return date. 4. Click the search button to view available flight information. 5. After selecting a suitable flight, click the &ldquo;Book&rdquo; button. 6. Fill in passenger information and choose a payment method. 7. Confirm the order details and submit the payment to complete the purchase.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>The specific steps to purchase a plane ticket using the Ctrip app are as follows:<br>1. <strong>Download and Install the Ctrip App</strong>: - If you haven&rsquo;t installed the Ctrip App, you can search for &ldquo;Ctrip Travel&rdquo; (Êê∫Á®ãÊóÖË°å) in the app store (like the Apple App Store or Android app markets) and download it.<br>2. <strong>Open the Ctrip App</strong>: - After installation is complete, open the Ctrip app.<br>3. <strong>Select the &ldquo;Flights&rdquo; Option</strong>: - On the homepage navigation bar, tap the &ldquo;Flights&rdquo; option.<br>4. <strong>Enter Departure and Destination Cities</strong>: - On the flight booking page, enter your departure and destination cities.<br>5. <strong>Select Departure and Return Dates</strong>: - Choose your departure date and return date (if booking a round trip).<br>6. <strong>Select Passenger Information</strong>: - Enter the number of passengers and their types (adult, child, infant).<br>7. <strong>Choose a Flight</strong>: - Browse and select your preferred flight.<br>8. <strong>Fill in Passenger Information</strong>: - Enter the passenger&rsquo;s name, ID number, contact information, etc.<br>9. <strong>Select a Payment Method</strong>: - Choose your preferred payment method (e.g., Alipay, WeChat Pay, bank card, etc.).<br>10. <strong>Confirm Order and Pay</strong>: - After confirming that the order information is correct, tap the &ldquo;Pay&rdquo; button to complete the payment.<br>11. <strong>Receive Your E-ticket</strong>: - After successful payment, you will receive an e-ticket, which you can view or download from the &ldquo;My Orders&rdquo; section within the app.<br>By following these steps, you can easily purchase plane tickets using the Ctrip App.</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/7.webp height=128px></figure>Based on the chart, under which primer is the Œîct difference between samples the smallest? | Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer. (Image source: Publicly available online - ËàíÊ°êÁßëÊäÄÂÖ¨‰ºóÂè∑)</div><div class=role>Ming-Lite-Omni</div><div class=content>Based on the chart, the Œîct difference between samples is smallest under the cGAS-qF+qR primer.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>The difference in Œîct between samples is smallest under the <strong>EGFP-cGAS monoclonal</strong> primer.<br><br><strong>Calculation Process:</strong><br><strong>1. Understand the meaning of Œîct:</strong> Œîct is the difference in relative expression between the target gene (cGAS-qF+qR or EGFP-cGAS) and the reference gene (cTubulin). A smaller Œîct indicates a smaller difference in expression between the target and reference genes.<br><strong>2. Compare Œîct under different primers:</strong><br>- <strong>cGAS-qF+qR primer:</strong> - Wild type: Œîct = 8.06 - EGFP-cGAS monoclonal: Œîct = -0.15 - Difference: 8.06 - (-0.15) = 8.21<br>- <strong>EGFP-cGAS monoclonal primer:</strong> - Wild type: Œîct = 10.44 - EGFP-cGAS monoclonal: Œîct = -2.9 - Difference: 10.44 - (-2.9) = 13.34<br>- <strong>EGFP-cGAS-qF+qR primer:</strong> - Wild type: Œîct = 21.43 - EGFP-cGAS monoclonal: Œîct = -13.45 - Difference: 21.43 - (-13.45) = 34.88<br><strong>3. Conclusion:</strong> Under the <strong>EGFP-cGAS monoclonal</strong> primer, the difference in Œîct between samples is the smallest (Œîct difference is 13.34), indicating the smallest difference in expression between samples under this primer.<br><br><strong>Summary:</strong> The difference in Œîct between samples is smallest under the <strong>EGFP-cGAS monoclonal</strong> primer.</div></div></div></div><h2 id=model-architecture-upgrade-and-capability-evaluation>Model Architecture Upgrade and Capability Evaluation<a hidden class=anchor aria-hidden=true href=#model-architecture-upgrade-and-capability-evaluation>#</a></h2><p>The Ming-lite-omni v1.5 model architecture is outlined below. The core design references the structure of Ming-lite-omni V1. However, a key distinction is the upgrade of the Vision head to support reference image feature input, specifically to enhance character and scene consistency in image editing.</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/A_LZTJcsc3EAAAAAXuAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>Mode Architecture</p></div><p>The model&rsquo;s capabilities have been significantly optimized and upgraded across three key areas: enhanced Omni-modal comprehension, precise visual editing control, and improved user experience.</p><h3 id=enhanced-omni-modal-comprehension>Enhanced Omni-Modal Comprehension<a hidden class=anchor aria-hidden=true href=#enhanced-omni-modal-comprehension>#</a></h3><p>Thanks to optimized data quality, Ming-lite-omni v1.5 shows significant improvements in tasks such as vision-text comprehension (including image-text, document, and video understanding) and speech understanding. It has reached an industry-leading level for models of comparable scale.</p><p><strong>Vision-text Comprehension</strong></p><table><thead><tr><th>Task Type</th><th>Dataset</th><th>Qwen2.5-VL-7B</th><th>Ming-lite-omni</th><th>Ming-lite-omni v1.5</th></tr></thead><tbody><tr><td>Image-text Understanding</td><td>AI2D</td><td>84.36</td><td>83.1</td><td><strong>84.91</strong></td></tr><tr><td></td><td>HallusionBench</td><td><strong>55.77</strong></td><td>55.0</td><td>54.59</td></tr><tr><td></td><td>MMBench_TEST_V11</td><td><strong>82.75</strong></td><td>80.8</td><td>80.73</td></tr><tr><td></td><td>MMMU</td><td><strong>56.56</strong></td><td>56.3</td><td>54.33</td></tr><tr><td></td><td>MMStar</td><td><strong>65.27</strong></td><td>64.7</td><td>65.07</td></tr><tr><td></td><td>MMVet</td><td>71.61</td><td>71.3</td><td><strong>73.99</strong></td></tr><tr><td></td><td>MathVista</td><td>68.10</td><td>71.6</td><td><strong>72.00</strong></td></tr><tr><td></td><td>OCRBench</td><td>87.80</td><td>88.4</td><td><strong>88.90</strong></td></tr><tr><td></td><td><strong>Average</strong></td><td>71.5</td><td>71.4</td><td><strong>71.8</strong></td></tr><tr><td>Video Understanding</td><td>VideoMME(w/o subs)</td><td>65.10</td><td>63.4</td><td><strong>67.07</strong></td></tr><tr><td></td><td>VideoMME(w/ subs)</td><td>71.60</td><td>66.01</td><td><strong>72.59</strong></td></tr><tr><td></td><td>VideoMME(avg)</td><td>68.35</td><td>67.7</td><td><strong>69.83</strong></td></tr><tr><td></td><td>MVBench</td><td><strong>69.60</strong></td><td>67.7</td><td>69.43</td></tr><tr><td></td><td>LongVideoBench</td><td>56.00</td><td>56.6</td><td><strong>59.54</strong></td></tr><tr><td></td><td>OvOBench</td><td>51.10</td><td>48.48</td><td><strong>52.17</strong></td></tr><tr><td></td><td><strong>Average</strong></td><td>61.26</td><td>58.89</td><td><strong>62.74</strong></td></tr><tr><td>Document Understanding</td><td>ChartQA_test</td><td>87.24</td><td>85.1</td><td><strong>88.84</strong></td></tr><tr><td></td><td>DocVQA_test</td><td><strong>95.57</strong></td><td>93</td><td>93.68</td></tr><tr><td></td><td>TextVQA_val</td><td><strong>85.06</strong></td><td>82.8</td><td>82.27</td></tr><tr><td></td><td>OCRBench</td><td>87.8</td><td>88.4</td><td><strong>88.9</strong></td></tr><tr><td></td><td><strong>Average</strong></td><td><strong>88.91</strong></td><td>87.32</td><td>88.42</td></tr></tbody></table><p><strong>Speech Understanding</strong></p><table><thead><tr><th>Model</th><th>Average(Open-ended QA)</th><th>AlpacaEval</th><th>CommonEval</th><th>SD-QA</th><th>MMSU</th><th>OpenBookQA</th><th>IFEval</th><th>AdvBench</th></tr></thead><tbody><tr><td>Ming-lite-omni v1.5</td><td>4.474</td><td>4.648</td><td>4.3</td><td>61.16</td><td>45.77</td><td>65.934</td><td>55.599</td><td>98.076</td></tr><tr><td>Ming-lite-omni</td><td>4.34</td><td>4.63</td><td>4.06</td><td>58.84</td><td>47.53</td><td>61.98</td><td>58.36</td><td>99.04</td></tr><tr><td>MiniCPM-o</td><td>4.285</td><td>4.42</td><td>4.15</td><td>50.72</td><td>54.78</td><td>78.02</td><td>49.25</td><td>97.69</td></tr><tr><td>Kimi-Audio</td><td>4.215</td><td>4.46</td><td>3.97</td><td>63.12</td><td>62.17</td><td>83.52</td><td>61.10</td><td>100.00</td></tr><tr><td>Qwen2.5-Omni</td><td>4.21</td><td>4.49</td><td>3.93</td><td>55.71</td><td>61.32</td><td>81.10</td><td>52.87</td><td>99.42</td></tr><tr><td>GLM-4-Voice</td><td>3.77</td><td>4.06</td><td>3.48</td><td>43.31</td><td>40.11</td><td>52.97</td><td>24.91</td><td>88.08</td></tr></tbody></table><h3 id=precise-visual-editing-control>Precise Visual Editing Control<a hidden class=anchor aria-hidden=true href=#precise-visual-editing-control>#</a></h3><p>Ming-lite-omni v1.5 employs the following optimization strategies to address the issues of character ID and scene ID consistency during image editing:</p><ol><li>ID and Scene Consistency Loss: This is achieved by increasing the weight of the edited region in the target image and the reference strength of the non-edited region in the reference image, while simultaneously decreasing the reference strength of the edited region in the reference image. This approach enhances image editing consistency.</li><li>Incorporating Generative Detection and Segmentation Tasks to Boost Perceptual Capabilities: By supporting generative segmentation and keypoint detection, the model&rsquo;s understanding of image details and spatial relationships is improved. This enhances the structural controllability of the editing and generation processes, leading to significant increases in evaluation metrics related to position, structure, and quantity.</li><li>Multi-Task Collaborative Learning Strategy: Through a joint training pipeline, generation and editing mutually reinforce each other. Segmentation tasks are transformed into colorization editing tasks, which significantly improves segmentation metrics and the precision and controllability of local image editing, resulting in smoother edges for edited regions.</li></ol><p>Based on these optimizations, Ming-lite-omni v1.5 shows a significant improvement in image editing capabilities, achieving a GenEval score of 0.87.</p><table><thead><tr><th style=text-align:center></th><th style=text-align:center>1-Obj</th><th style=text-align:center>2-Obj</th><th style=text-align:center>Counting</th><th style=text-align:center>Colors</th><th style=text-align:center>Position</th><th style=text-align:center>Color Attr</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>0.99</td><td style=text-align:center>0.77</td><td style=text-align:center>0.68</td><td style=text-align:center>0.78</td><td style=text-align:center>0.46</td><td style=text-align:center>0.42</td><td style=text-align:center>0.64</td></tr><tr><td style=text-align:center>Ming-lite-omni v1.5</td><td style=text-align:center>0.99</td><td style=text-align:center>0.93</td><td style=text-align:center>0.86</td><td style=text-align:center>0.87</td><td style=text-align:center>0.90</td><td style=text-align:center>0.66</td><td style=text-align:center>0.87</td></tr></tbody></table><h3 id=optimized-user-experience>Optimized User Experience<a hidden class=anchor aria-hidden=true href=#optimized-user-experience>#</a></h3><p>Thanks to the construction of high-quality alignment preference data, Ming-lite-omni v1.5 demonstrates a certain advantage over leading models in terms of correctness, relevance, format aesthetics, and fluency of expression for image-text Q&amp;A. Ming-lite-omni v1.5 achieved a win rate of 87.07% against Ming-lite-omni V1 on internal adversarial evaluation sets, indicating a significant optimization in user experience.</p><table><thead><tr><th style=text-align:center>Evaluation Dimension</th><th style=text-align:center>Qwen2.5-VL-7B</th><th style=text-align:center>Ming-lite-omni V1.5</th></tr></thead><tbody><tr><td style=text-align:center>Relevance</td><td style=text-align:center>4.308</td><td style=text-align:center>4.5</td></tr><tr><td style=text-align:center>Fluency</td><td style=text-align:center>4.765</td><td style=text-align:center>4.91</td></tr><tr><td style=text-align:center>Richness of Content</td><td style=text-align:center>3.828</td><td style=text-align:center>3.69</td></tr><tr><td style=text-align:center>Format aesthetics</td><td style=text-align:center>4.727</td><td style=text-align:center>4.8</td></tr><tr><td style=text-align:center>Correctness</td><td style=text-align:center>3.741</td><td style=text-align:center>3.92</td></tr><tr><td style=text-align:center><strong>Average Score</strong></td><td style=text-align:center>4.274</td><td style=text-align:center><strong>4.365</strong></td></tr></tbody></table><h2 id=get-started-with-ming-lite-omni-v15>Get Started with Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#get-started-with-ming-lite-omni-v15>#</a></h2><p>The model and code for Ming-lite-omni v1.5 are now open-source, and we invite everyone to try it out, share feedback, and join the discussion. Looking ahead, we&rsquo;re excited to announce that a quantized and accelerated version is on the way. This future release will not only further enhance omni-modal performance but also make the model even more lightweight, all while strengthening its multimodal reasoning and generation capabilities. Stay tuned for more updates!</p><ul><li>Github: <a href=https://github.com/inclusionAI/Ming>https://github.com/inclusionAI/Ming</a></li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5</a></li></ul></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
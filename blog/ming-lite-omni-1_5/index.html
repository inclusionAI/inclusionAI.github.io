<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni V1.5 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB üöÄ Exciting News We&rsquo;ve just rolled out Ming-Lite-Omni V1.5, a major upgrade from the original version. This release keeps its core strengths while expanding into more multimodal tasks with optimized performance across the board. Experience enhancements like advanced document comprehension, MRoPE-based spatiotemporal position encoding, generative image segmentation, and improved image generation with ID retention. All these amazing features stem from one unified multimodal model, showcasing the power of modality unification."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Lite-Omni V1.5"><meta property="og:description" content="GITHUB üöÄ Exciting News We&rsquo;ve just rolled out Ming-Lite-Omni V1.5, a major upgrade from the original version. This release keeps its core strengths while expanding into more multimodal tasks with optimized performance across the board. Experience enhancements like advanced document comprehension, MRoPE-based spatiotemporal position encoding, generative image segmentation, and improved image generation with ID retention. All these amazing features stem from one unified multimodal model, showcasing the power of modality unification."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-15T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-15T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni V1.5"><meta name=twitter:description content="GITHUB üöÄ Exciting News We&rsquo;ve just rolled out Ming-Lite-Omni V1.5, a major upgrade from the original version. This release keeps its core strengths while expanding into more multimodal tasks with optimized performance across the board. Experience enhancements like advanced document comprehension, MRoPE-based spatiotemporal position encoding, generative image segmentation, and improved image generation with ID retention. All these amazing features stem from one unified multimodal model, showcasing the power of modality unification."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni V1.5","item":"https://inclusionai.github.io/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni V1.5","name":"Ming-Lite-Omni V1.5","description":"GITHUB üöÄ Exciting News We\u0026rsquo;ve just rolled out Ming-Lite-Omni V1.5, a major upgrade from the original version. This release keeps its core strengths while expanding into more multimodal tasks with optimized performance across the board. Experience enhancements like advanced document comprehension, MRoPE-based spatiotemporal position encoding, generative image segmentation, and improved image generation with ID retention. All these amazing features stem from one unified multimodal model, showcasing the power of modality unification.","keywords":[],"articleBody":"GITHUB üöÄ Exciting News We‚Äôve just rolled out Ming-Lite-Omni V1.5, a major upgrade from the original version. This release keeps its core strengths while expanding into more multimodal tasks with optimized performance across the board. Experience enhancements like advanced document comprehension, MRoPE-based spatiotemporal position encoding, generative image segmentation, and improved image generation with ID retention. All these amazing features stem from one unified multimodal model, showcasing the power of modality unification. Stay tuned for detailed insights into each capability, and thank you for your support. We invite everyone to explore, share, and connect! üåü\nComplex Document Understanding The performance of the Ming-Omni-Lite model was systematically evaluated across a diverse set of benchmarks for Optical Character Recognition (OCR), chart analysis, and document understanding.\n\u003c!DOCTYPE html\u003e Tasks Datasets Qwen2.5-VL-7B InternVL3-8B Ming-Omni-Lite OCR-related Understanding ChartQA_test 87.24 86.60 88.84 DocVQA_test 95.57 92.70 93.68 TextVQA_val 85.06 80.20 82.27 OCRBench 87.8 88.00 88.90 average 88.91 86.87 88.42 OCR-related Parsing OmniDocBench‚Üì en/zh 30.8/39.8 -- 34.9/34.9 OCR-related Comprehensive OCRBenchV2 en/zh 56.3/57.2 -- 52.1/55.2 In OCR-related understanding tasks, Ming-Omni-Lite achieved an average score of 88.42, demonstrating performance comparable to the state-of-the-art 7B-parameter model, Qwen2.5VL-7B-Instruct (88.91). Notably, the model delivered state-of-the-art results on the challenging OCRBench (which assesses Text-Associated Vision Capability) and ChartQA benchmarks (which demand sophisticated visual analysis and logical reasoning of charts). This superior performance is attributed to its innovative training strategy, which integrates the Chain-of-Thought (CoT) paradigm to enable structured, multi-step reasoning pathways for complex problem-solving.\nIn the domain of OCR-related parsing, evaluated on the OmniDocBench benchmark for for multi-scene, multilingual, and various built-in (handwriting, tables, charts, chemical formulas, and mathematical expressions) documents. Ming-Omni-Lite exhibited exceptional capabilities. The model‚Äôs strong performance in both English (34.9) and Chinese (34.9) contexts is a direct result of its training on a meticulously constructed, multi-source heterogeneous dataset.\nText-Image Understanding and Human Experience Text-Image Understanding The new Ming-lite-omni v1.5 integrates 3D positional encoding, significantly enhancing its perception of image structure and dynamic information. Combined with enhanced training strategies and comprehensively upgraded, high-quality data, v1.5 achieves substantial performance leaps across key tasks including general image understanding, image object recognition, and vertical scene recognition. This provides a more powerful foundational capability for a wide range of visual applications.\nSpatio-Temporal Position Encoding. We introducing MRoPE, this novel 3D block-wise position encoding across time, height, and width dimensions endows models with spatio-temporal awareness, enabling efficient cross-modal joint modeling and enhancing understanding accuracy in video and complex image scenarios. Efficient Training Strategy. Through strategic optimization of learning rates and multimodal data mixtures, we transform traditional step-wise LLM freezing/unfreezing procedures into efficient full-parameter training. This approach reduces training cycles and computational costs without compromising model performance. Comprehensive Data Improvement. a. In the pre-training phase, we expand knowledge breadth and data quality by: incorporating structured text-entity data to fill knowledge graph gaps; introducing original-description constraints to suppress hallucinations in generation tasks; and expanding vertical-domain corpora to deepen domain understanding. b. In the instruction fine-tuning phase, we strengthen core tasks and advanced understanding by: enhancing fine-grained visual perception (object counting, color identification, scene recognition); deepening vertical category recognition (animals, plants, vehicles, ingredients); and optimizing cross-disciplinary text-image reasoning. Task Type Evaluation Benchmark Qwen2.5-VL-7B Ming-Omni-Lite General AI2D 84.36 84.91 HallusionBench 55.77 54.59 MMBench_TEST_V11 82.75 80.73 MMMU 56.56 54.33 MMStar 65.27 65.07 MMVet 71.61 73.99 MathVista 68.10 72.00 OCRBench 87.80 88.90 Object Detection RefCOCO_val 90.00 91.40 RefCOCO+_val 84.20 86.30 In-house Benchmarks General Knowledge 92.42 92.53 Vertical Categories 47.79 54.27 Human Experience Thanks to the construction of high-quality alignment preference data and the fine-tuning of DPO training hyperparameter search/data sampling strategy configurations, our model achieves SOTA (state-of-the-art) performance on an in-house human experience preference dataset compared to open-source models of equivalent inference parameter size. It surpasses Qwen2.5VL-7B-Instruct by 0.09/5 points in average score. Our model exhibits significant advantages in terms of content accuracy (low hallucination rate), relevance, format aesthetics, and fluency of expression in multimodal question-answering tasks. This means the model provides a superior comprehensive experience for users during the question-answering interaction process.\nBenchmark\n(In-house) Evaluation Dimensions Qwen2.5-VL-7B Ming-Omni-Lite Human Preference Average Score 4.274 4.365 Relevance 4.308 4.5 Fluency 4.765 4.91 Richness 3.828 3.69 Appropriateness 4.727 4.8 Correctness 3.741 3.92 Advancing the SOTA in Video Understanding The pursuit of Artificial General Intelligence (AGI) necessitates robust video comprehension capabilities within Multimodal Large Language Models (MLLMs). Real-world information is inherently dynamic and sequential, with video conveying significantly richer spatiotemporal semantics than static images. We report that Ming-Omni-Lite has achieved notable advancements across several core video understanding benchmarks.\nPerformance Overview: Comprehensive Benchmark Results Ming-Omni-Lite was rigorously evaluated against leading same-scale models (Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B) on prominent and challenging video understanding benchmarks. The results demonstrate Ming-Omni-Lite‚Äôs superior performance:\nBenchmark Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 Technical Foundations: Architectural and Training Innovations The advancements in video comprehension, particularly for long-form content, were achieved through key innovations in model architecture and training methodology:\nEfficient Spatiotemporal Modeling: Incorporates MRoPE to effectively capture intra-frame (spatial) and inter-frame (temporal) dependencies, enabling precise extraction of critical dynamic information. High-Quality, Diverse Video-Text Alignment Data: We have built a large-scale dataset of long/short video-text pairs and TPO (task-perference optimization) data covering a wide range of scenarios and tasks, including temporal retrieval and video tracking. We have performed detailed cleaning to ensure that the model learns accurate alignment capabilities. Innovative Training Objectives and Curriculum Learning: Combines video-specific pretraining and instruction fine-tuning objectives, incorporating curriculum learning strategies that progressively increase temporal complexity from short to long videos. Conclusion: Enabling Advanced Video Interaction Ming-Omni-Lite demonstrates superior performance compared to same-scale SOTA models on video understanding benchmarks. This validates its strong capability to process complex, long-duration, information-dense video content, laying a solid foundation for applications including video summarization, long-form video QA, intelligent tutoring, content moderation, and human-computer interaction.\nWe remain committed to advancing Ming-Omni-Lite‚Äôs capabilities in video and multimodal domains, striving to develop intelligent agents capable of genuine comprehension, reasoning, and interaction with the real world.\nAudio Understanding and Generation To enhance the capabilities of our model in Spoken Language Understanding (SLU), we incorporated an extensive and diverse corpus of speech data during the training phase. Furthermore, we integrated critical metadata‚Äîincluding domain, topic, language, and various dialects‚Äîdirectly into the instructional prompts for the understanding tasks. This methodology significantly improves the model‚Äôs contextual comprehension. Our model demonstrates proficiency in understanding Mandarin Chinese, English, as well as a range of Chinese dialects such as Cantonese, Sichuan, Shanghai, and Minnan. As a result, it achieves SOTA performance on publicly available Mandarin and English benchmarks.\nModel aishell1 aishell2_android aishell2_ios cv15_zh fleurs_zh Ming-lite-omni 1.31 2.45 2.45 5.69 2.87 Qwen2.-Omni 1.18 2.75 2.63 5.20 3.00 Qwen2-Audio 1.53 2.92 2.92 6.90 7.50 Kimi-Audio 0.60 2.64 2.56 7.21 2.69 Model ws_meet ws_net lsc_test_cl lsc_test_ot multi_ls cv15_en flu_en vox_en Ming-lite-omni 6.18 5.22 1.24 2.61 4.13 6.95 3.28 6.82 Qwen2-Omni 5.90 7.70 1.80 3.40 7.56 7.60 4.10 5.80 Qwen2-Audio 7.16 8.42 1.60 3.60 5.40 8.60 6.90 6.84 Kimi-Audio 6.28 5.37 1.28 2.42 5.88 10.31 4.44 7.97 Input Audio Dialect Recognition Á≤§ËØ≠ ‰Ω†Âú®Âπ≤‰ªÄ‰πà, ÊòØ‰∏çÊòØ‰∏çÊÉ≥ËÅäÂ§© ‰∏äÊµ∑ËØù Êàë‰ª¨ËÄÉËØïËøòÊ≤°ÂÆö‰∏ãÊù•Âë¢ ÈóΩÂçóËØ≠ ÂÆùË¥ù, Êó©ÁÇπ‰ºëÊÅØ, ÊôöÂÆâ ÂõõÂ∑ùËØù ÊàëÈöæÂèóÁöÑÂæà, Âà´‰∫∫ÈÉΩÁù°‰∫Ü Leveraging its superior speech understanding capabilities, our model also demonstrates exceptional performance on speech dialogue evaluation benchmarks.\nModel AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-lite-omni 4.65 4.30 61.16 45.77 65.93 55.60 98.08 Input Audio Ouput Audio In the domain of speech synthesis, Ming-lite-omni incorporates an advanced audio decoder. This decoder is conditioned on the output hidden states from the LLM, which enables the model to handle both context-aware multimodal dialogue and standard TTS tasks, ultimately generating speech with high naturalness and fluency. To enhance prosodic performance and facilitate real-time generation, we apply Byte Pair Encoding (BPE) to the discrete audio codec tokens. This methodology results in a 35% reduction in the acoustic frame rate, thereby improving synthesis efficiency.\nModel seed-tts-eval-zh_wer seed-tts-eval-zh_sim seed-tts-eval-en_wer seed-tts-eval-en_sim Seed-TTS 1.11 0.796 2.24 0.762 MaskGCT 2.27 0.774 2.62 0.714 E2 TTS 1.97 0.730 2.19 0.710 F5-TTS 1.56 0.741 1.83 0.647 CosyVoice 2 1.45 0.748 2.57 0.652 Qwen2.5-Omni-7B 1.70 0.752 2.72 0.632 Ming-Lite-Omni 2.00 0.686 4.299 0.513 Prompt Audio Input Text Output Audio Êàë‰ª¨ÁöÑÊÑøÊôØÊòØÊûÑÂª∫Êú™Êù•ÊúçÂä°‰∏öÁöÑÊï∞Â≠óÂåñÂü∫Á°ÄËÆæÊñΩÔºåÁªô‰∏ñÁïåÂ∏¶Êù•Êõ¥Â§öÂæÆÂ∞èËÄåÁæéÂ•ΩÁöÑÊîπÂèò„ÄÇ The stained glass offered a hypnotic atmosphere Breaking New Limits in Image Generation üî• Building on the earlier releases of Ming-Lite-Uni and Ming-Omni, this version takes Ming‚Äôs image generation to the next level! We‚Äôve enhanced scene consistency, ID consistency (for characters and styles), and perception expansion (including segmentation, keypoints, depth, and more). Ming has evolved from a multimodal large model (MLLM) with image generation and editing talents to a comprehensive MLLM capable of tackling even more image generation tasks with superior results. Check out our recent progress report below. We‚Äôre eager to exchange ideas and discuss with you all!\nGenEval Single Ojbect Two Objects Counting Color Position Color Attr Ours 0.86 100.00% 96.72% 76.56% 89.89% 89.75% 68.69% Model Structure Review and Improvements Sub-module Technical Point Function Key Points Cross-modal Bridging Method Channel concat / Token concat / Blend - Channel concat: Fewer parameters, low memory use, but weaker semantic alignment - Token concat: Preserves semantic structure, suitable for high resolution - Blend: More robust for scene editing/rewriting Choose bridging method based on task needs. Currently using Token Concat approach Dual-branch Representation Decoupling Decouple reference image patch encoding and refiner parameters - Enhance the independent control parameter capacity of reference images, providing partial decoupling Dual patchfy modules and extra dual-branch refiner improve model editing and segmentation performance Dual-branch decoupling refers to using different network weights to patchfy reference images and noise images before feeding them into DiT‚Äôs transformer. This effectively reduces the influence of reference image information on semantic adherence during editing. The refiner is an extra lightweight two-layer transformer added after patchfy to further enhance this effect. Performance assessments in inference segmentation demonstrate the effectiveness of the new module. Inference segmentation tests the model‚Äôs ability to correctly understand semantics, requiring it to determine the target of segmentation based on complex instructions. Experimental results are shown in the table below. It is evident that decoupling patchfy significantly improves segmentation metrics. Adding the refiner module can further enhance performance. GEdit Subset: [‚Äúbackground_change‚Äù, ‚Äúcolor_alter‚Äù, ‚Äúmaterial_alter‚Äù, ‚Äúmotion_change‚Äù] Mode ID double-patchfy add-refiner refcoc segmentation metric GEdit(subset-full) 0 ‚ùå ‚ùå 62.8 6.129 1 ‚úÖ ‚ùå 64.2 6.391 2 ‚úÖ ‚úÖ 64.5 6.306 Conditional Control and Guidance Strategy Sub-module Technical Point Function Key Points Multi-condition CFG Control Strategy Semantic CFG vs Image CFG (Ref-Guided) Multi-condition Classifier-free Guidance strategy: semantic binary differentiation and image ternary differentiation enhance ID consistency With pure semantic control, the edited image follows the instructions but loses all consistency with the original image; when the image branch guidance intensity is high, the edited result is almost identical to the original image ID \u0026 Scene Consistency Loss Weight mask Loss + Scene Consistency Loss Increase the weight of the target image editing area, while adding strong constraints to the non-editing area and weak constraints to the editing area of the reference image Adjust Œª to balance editing effects while maintaining ID consistency and scene consistency, avoiding overfitting Comparison with Qwen-VLo prompt ours Qwen-VLo Make the person in the image smile slightly without altering the original structure Perception Capability Expansion Generative Segmentation\nUnlike generative image editing tasks, segmentation tasks have less detail consistency between the predicted mask and the original image. Hence, under the token concat approach, learning the consistency relationship between the original image and segmentation mask can be challenging. To address this, we model the segmentation target as a colored segmentation image‚Äîa fusion of mask and image‚Äîallowing more consistent details between the predicted target and the original image. This enhances the ability to learn the relationship between the segmentation map and the original image. During inference, the predicted image and original image are differenced, and noise is filtered to obtain the final prediction mask.\nInput Image Inference Segmentation Semantic Segmentation Panoptic Segmentation prompt: Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. prompt: Please segment different classes in this image prompt: Please segment different instances in this image. Edge Contour Map Generation Original Image Depth Map Detection Box Edge Contour Data Engineering OCR-related parsing corpus. To enhance the base model‚Äôs performance in text-associated vision capability and logical reasoning, we introduce the Chain-of-Thought (CoT) paradigm during training. The specific implementation strategies are as follows: Firstly, we integrate the open-source ChartQA-PoT (Program-of-Thought) dataset into the training framework, with particular emphasis on improving the model‚Äôs numerical computation capabilities for charts. This approach inherits the progressive reasoning philosophy of Chain-of-Thought, but innovatively adopts executable Python code as the intermediate reasoning medium. Secondly, to address the limitations of the prevalent ‚Äúanswer-direct prompting‚Äù pattern in conventional text-based question-answering datasets, we innovatively employ reinforcement learning models to generate multi-step reasoning trajectories and final answers from original training samples. Through formal verification, we construct a novel training dataset comprising validated ‚Äúreasoning step-final answer‚Äù pairs. This methodology significantly enhances the model‚Äôs adaptability to complex logical tasks.\nHuman preference corpus. Human preference data optimizes MLLM responses through enhanced improved alignment with human-centric interaction patterns in the alignment tuning stage. Our preference corpus is primarily constructed from three sources: user-generated conversations in applications, search queries from websites, and high-quality open-source instruction datasets. Specifically, we first retrieve relevant web images via search engines to complement text-only user-generated dialogues or queries. We then leverage available MLLMs to generate diverse specialized questions and their corresponding answers. Afterwards, we organize those high-quality positive samples and the other negative ones to construct the preference corpus, which comprises 41 subcategories across 9 primary domains. Ultimately, we engage MLLMs and skilled human annotators to assess the quality of these QA pairs.\nVideo understanding corpus. High-quality video data provides richer and more precise semantic information, which effectively enhances the model‚Äôs depth and breadth of understanding complex video scenarios. The video corpus we constructed mainly derives from two sources: First, we have extensively mined and integrated existing open-source datasets for fundamental visual tasks‚Äîfor example, leveraging the GOT-10K object tracking dataset to strengthen the model‚Äôs object tracking capability, and utilizing the DiDemo temporal retrieval dataset to improve the model‚Äôs event perception ability. Second, we organized a team of experts to meticulously annotate a set of challenging video question-answer pairs, dedicated to training the model‚Äôs capability for complex reasoning.\nEncyclopedia corpus. Encyclopedia data integrates advanced domain-specific expertise into MLLMs for expert-level comprehension and perception, e.g., identifying rare or endangered species via Latin binomial nomenclature. In this version, our encyclopedia corpus spans 10 specific domains across biological categories (Plants and Animals), cultural categories (Celebrities, Anime characters, Landmarks, LOGOs, and Artworks), and daily-life categories (Ingredients, Dishes, and Vehicles). To construct a high-quality expert-level corpus, we first collect a wide range of encyclopedia entities from academic databases and institutional websites. We then employ these entities as search queries to collect semantically relevant images via search engines. Afterwards, we develop a progressive encyclopedia data filtering scheme, including clip consistency validation, MLLM-based binary verification, and manual refinement.\nGUI corpus. The GUI (Graphic User Interface) data enables the model with basic GUI navigation ability on Android environment. Our GUI corpus is mainly constructed from four public datasets: AITW, GUICourse, AndroidControl and AMEX. Moreover, we leverage available MLLMs to optimize the reasoning process for each step within a human-like GUI interaction operation, which is subsequently reviewed by another MLLM to improve data quality. The thinking process constrains model to observe current state and reflect previous actions with careful consideration before acting. To better perceive current situation, we involve history memory, which includes preceding actions.\nImage generation and editing corpus. Our image generation corpus mainly comes from two sources: High-quality images collected from public image generation datasets (e.g., text-to-image-2M, JourneyDB, BLip3o, Uniworld dataset, InstructPix2Pix-clip-filtered, SEED-Data-Edit-part2/3, Ultraedit, etc.); and image style transfer data sampled from StyleBooth and WikiArt. And also, we constructed a few generation pipelines, producing part of text-to-image and editing data.\nPerceptual reasoning corpus. Perceptual reasoning data can enhance the model‚Äôs comprehensive comprehension capabilities and fine-grained perceptual abilities. In this version, we focus on perception-oriented subtasks including object counting, color recognition, and scene theme identification. A set of potential challenging samples were curated from open-source datasets such as Object365 and RefCOCO through systematic filtering based on bounding box quantities, spatial relationships, and object category counts. Subsequently, these candidate samples underwent task-specific question-answer synthesis using Visual Language Models (VLMs), followed by rigorous quality assurance through multi-model evaluation scoring and manual filtering/correction to obtain high-quality annotations. Furthermore, to strengthen the model‚Äôs reasoning capacity in perceptual tasks, we reformulated the synthesized data into Chain-of-Thought (CoT) format. This transformation introduces intermediate reasoning steps to assist the model in decomposing complex problems, thereby systematically improving its integrated perceptual understanding.\n","wordCount":"2825","inLanguage":"en","datePublished":"2025-07-15T00:00:03+08:00","dateModified":"2025-07-15T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Omni V1.5</h1><div class=post-meta><span title='2025-07-15 00:00:03 +0800 +0800'>July 15, 2025</span>&nbsp;¬∑&nbsp;14 min&nbsp;¬∑&nbsp;2825 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a><h1 id=-exciting-news>üöÄ Exciting News<a hidden class=anchor aria-hidden=true href=#-exciting-news>#</a></h1><p>We&rsquo;ve just rolled out Ming-Lite-Omni V1.5, a major upgrade from the original version. This release keeps its core strengths while expanding into more multimodal tasks with optimized performance across the board. Experience enhancements like advanced document comprehension, MRoPE-based spatiotemporal position encoding, generative image segmentation, and improved image generation with ID retention. All these amazing features stem from <strong>one unified</strong> multimodal model, showcasing the power of modality unification. Stay tuned for detailed insights into each capability, and thank you for your support. We invite everyone to explore, share, and connect! üåü</p><hr><h1 id=complex-document-understanding>Complex Document Understanding<a hidden class=anchor aria-hidden=true href=#complex-document-understanding>#</a></h1><p>The performance of the Ming-Omni-Lite model was systematically evaluated across a diverse set of benchmarks for Optical Character Recognition (OCR), chart analysis, and document understanding.</p><!doctype html><html lang=zh-cn><table class=optimized-table><thead><tr><th>Tasks</th><th>Datasets</th><th>Qwen2.5-VL-7B</th><th>InternVL3-8B</th><th>Ming-Omni-Lite</th></tr></thead><tbody><tr><td rowspan=5 class=merged-cell>OCR-related Understanding</td><td>ChartQA_test</td><td>87.24</td><td>86.60</td><td>88.84</td></tr><tr><td>DocVQA_test</td><td>95.57</td><td>92.70</td><td>93.68</td></tr><tr><td>TextVQA_val</td><td>85.06</td><td>80.20</td><td>82.27</td></tr><tr><td>OCRBench</td><td>87.8</td><td>88.00</td><td>88.90</td></tr><tr><td>average</td><td>88.91</td><td>86.87</td><td>88.42</td></tr><tr><td>OCR-related Parsing</td><td>OmniDocBench‚Üì en/zh</td><td>30.8/39.8</td><td>--</td><td>34.9/34.9</td></tr><tr><td>OCR-related Comprehensive</td><td>OCRBenchV2 en/zh</td><td>56.3/57.2</td><td>--</td><td>52.1/55.2</td></tr></tbody></table></body></html><p>In OCR-related understanding tasks, Ming-Omni-Lite achieved an average score of 88.42, demonstrating performance comparable to the state-of-the-art 7B-parameter model, Qwen2.5VL-7B-Instruct (88.91). Notably, the model delivered state-of-the-art results on the challenging OCRBench (which assesses Text-Associated Vision Capability) and ChartQA benchmarks (which demand sophisticated visual analysis and logical reasoning of charts). This superior performance is attributed to its innovative training strategy, which integrates the Chain-of-Thought (CoT) paradigm to enable structured, multi-step reasoning pathways for complex problem-solving.</p><p>In the domain of OCR-related parsing, evaluated on the OmniDocBench benchmark for for multi-scene, multilingual, and various built-in (handwriting, tables, charts, chemical formulas, and mathematical expressions) documents. Ming-Omni-Lite exhibited exceptional capabilities. The model&rsquo;s strong performance in both English (34.9) and Chinese (34.9) contexts is a direct result of its training on a meticulously constructed, multi-source heterogeneous dataset.</p><hr><h2 id=text-image-understanding-and-human-experience>Text-Image Understanding and Human Experience<a hidden class=anchor aria-hidden=true href=#text-image-understanding-and-human-experience>#</a></h2><h3 id=text-image-understanding><strong>Text-Image Understanding</strong><a hidden class=anchor aria-hidden=true href=#text-image-understanding>#</a></h3><p>The new Ming-lite-omni v1.5 integrates 3D positional encoding, significantly enhancing its perception of image structure and dynamic information. Combined with enhanced training strategies and comprehensively upgraded, high-quality data, v1.5 achieves substantial performance leaps across key tasks including general image understanding, image object recognition, and vertical scene recognition. This provides a more powerful foundational capability for a wide range of visual applications.</p><ol><li>Spatio-Temporal Position Encoding. We introducing MRoPE, this novel 3D block-wise position encoding across time, height, and width dimensions endows models with spatio-temporal awareness, enabling efficient cross-modal joint modeling and enhancing understanding accuracy in video and complex image scenarios.</li><li>Efficient Training Strategy. Through strategic optimization of learning rates and multimodal data mixtures, we transform traditional step-wise LLM freezing/unfreezing procedures into efficient full-parameter training. This approach reduces training cycles and computational costs without compromising model performance.</li><li>Comprehensive Data Improvement.
a. In the pre-training phase, we expand knowledge breadth and data quality by: incorporating structured text-entity data to fill knowledge graph gaps; introducing original-description constraints to suppress hallucinations in generation tasks; and expanding vertical-domain corpora to deepen domain understanding.
b. In the instruction fine-tuning phase, we strengthen core tasks and advanced understanding by: enhancing fine-grained visual perception (object counting, color identification, scene recognition); deepening vertical category recognition (animals, plants, vehicles, ingredients); and optimizing cross-disciplinary text-image reasoning.</li></ol><table><thead><tr><th>Task Type</th><th>Evaluation Benchmark</th><th>Qwen2.5-VL-7B</th><th>Ming-Omni-Lite</th></tr></thead><tbody><tr><td><strong>General</strong></td><td>AI2D</td><td>84.36</td><td>84.91</td></tr><tr><td></td><td>HallusionBench</td><td>55.77</td><td>54.59</td></tr><tr><td></td><td>MMBench_TEST_V11</td><td>82.75</td><td>80.73</td></tr><tr><td></td><td>MMMU</td><td>56.56</td><td>54.33</td></tr><tr><td></td><td>MMStar</td><td>65.27</td><td>65.07</td></tr><tr><td></td><td>MMVet</td><td>71.61</td><td>73.99</td></tr><tr><td></td><td>MathVista</td><td>68.10</td><td>72.00</td></tr><tr><td></td><td>OCRBench</td><td>87.80</td><td>88.90</td></tr><tr><td><strong>Object Detection</strong></td><td>RefCOCO_val</td><td>90.00</td><td>91.40</td></tr><tr><td></td><td>RefCOCO+_val</td><td>84.20</td><td>86.30</td></tr><tr><td><strong>In-house Benchmarks</strong></td><td>General Knowledge</td><td>92.42</td><td>92.53</td></tr><tr><td></td><td>Vertical Categories</td><td>47.79</td><td>54.27</td></tr></tbody></table><h3 id=human-experience><strong>Human Experience</strong><a hidden class=anchor aria-hidden=true href=#human-experience>#</a></h3><p>Thanks to the construction of high-quality alignment preference data and the fine-tuning of DPO training hyperparameter search/data sampling strategy configurations, our model achieves SOTA (state-of-the-art) performance on an in-house human experience preference dataset compared to open-source models of equivalent inference parameter size. It surpasses Qwen2.5VL-7B-Instruct by 0.09/5 points in average score. Our model exhibits significant advantages in terms of content accuracy (low hallucination rate), relevance, format aesthetics, and fluency of expression in multimodal question-answering tasks. This means the model provides a superior comprehensive experience for users during the question-answering interaction process.</p><table><thead><tr><th>Benchmark<br>(In-house)</th><th>Evaluation Dimensions</th><th>Qwen2.5-VL-7B</th><th>Ming-Omni-Lite</th></tr></thead><tbody><tr><td>Human Preference</td><td>Average Score</td><td>4.274</td><td>4.365</td></tr><tr><td></td><td>Relevance</td><td>4.308</td><td>4.5</td></tr><tr><td></td><td>Fluency</td><td>4.765</td><td>4.91</td></tr><tr><td></td><td>Richness</td><td>3.828</td><td>3.69</td></tr><tr><td></td><td>Appropriateness</td><td>4.727</td><td>4.8</td></tr><tr><td></td><td>Correctness</td><td>3.741</td><td>3.92</td></tr></tbody></table><hr><h2 id=advancing-the-sota-in-video-understanding>Advancing the SOTA in Video Understanding<a hidden class=anchor aria-hidden=true href=#advancing-the-sota-in-video-understanding>#</a></h2><p>The pursuit of Artificial General Intelligence (AGI) necessitates robust video comprehension capabilities within Multimodal Large Language Models (MLLMs). Real-world information is inherently dynamic and sequential, with video conveying significantly richer spatiotemporal semantics than static images. We report that <strong>Ming-Omni-Lite</strong> has achieved notable advancements across several core video understanding benchmarks.</p><h3 id=performance-overview-comprehensive-benchmark-results>Performance Overview: Comprehensive Benchmark Results<a hidden class=anchor aria-hidden=true href=#performance-overview-comprehensive-benchmark-results>#</a></h3><p>Ming-Omni-Lite was rigorously evaluated against leading same-scale models (Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B) on prominent and challenging video understanding benchmarks. The results demonstrate Ming-Omni-Lite&rsquo;s superior performance:</p><table><thead><tr><th style=text-align:left>Benchmark</th><th style=text-align:center>Qwen2.5-VL-7B</th><th style=text-align:center>Qwen2.5-Omni-7B</th><th style=text-align:center>InternVL3-8B</th><th style=text-align:center><strong>Ming-Omni-Lite</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>VideoMME(w/o subs)</strong></td><td style=text-align:center>65.10</td><td style=text-align:center>64.30</td><td style=text-align:center>66.30</td><td style=text-align:center><strong>67.07</strong></td></tr><tr><td style=text-align:left><strong>VideoMME(w/ subs)</strong></td><td style=text-align:center>71.60</td><td style=text-align:center>72.40</td><td style=text-align:center>68.90</td><td style=text-align:center><strong>72.59</strong></td></tr><tr><td style=text-align:left><strong>VideoMME(avg)</strong></td><td style=text-align:center>68.35</td><td style=text-align:center>68.35</td><td style=text-align:center>67.60</td><td style=text-align:center><strong>69.83</strong></td></tr><tr><td style=text-align:left><strong>MVBench</strong></td><td style=text-align:center>69.60</td><td style=text-align:center>70.30</td><td style=text-align:center><strong>75.40</strong></td><td style=text-align:center>69.43</td></tr><tr><td style=text-align:left><strong>LongVideoBench</strong></td><td style=text-align:center>56.00</td><td style=text-align:center>54.82</td><td style=text-align:center>58.80</td><td style=text-align:center><strong>59.54</strong></td></tr><tr><td style=text-align:left><strong>OvOBench</strong></td><td style=text-align:center>51.10</td><td style=text-align:center>50.46</td><td style=text-align:center>51.91</td><td style=text-align:center><strong>52.17</strong></td></tr></tbody></table><h3 id=technical-foundations-architectural-and-training-innovations>Technical Foundations: Architectural and Training Innovations<a hidden class=anchor aria-hidden=true href=#technical-foundations-architectural-and-training-innovations>#</a></h3><p>The advancements in video comprehension, particularly for long-form content, were achieved through key innovations in model architecture and training methodology:</p><ul><li><strong>Efficient Spatiotemporal Modeling:</strong> Incorporates MRoPE to effectively capture intra-frame (spatial) and inter-frame (temporal) dependencies, enabling precise extraction of critical dynamic information.</li><li><strong>High-Quality, Diverse Video-Text Alignment Data:</strong> We have built a large-scale dataset of long/short video-text pairs and TPO (task-perference optimization) data covering a wide range of scenarios and tasks, including temporal retrieval and video tracking. We have performed detailed cleaning to ensure that the model learns accurate alignment capabilities.</li><li><strong>Innovative Training Objectives and Curriculum Learning:</strong> Combines video-specific pretraining and instruction fine-tuning objectives, incorporating curriculum learning strategies that progressively increase temporal complexity from short to long videos.</li></ul><h3 id=conclusion-enabling-advanced-video-interaction>Conclusion: Enabling Advanced Video Interaction<a hidden class=anchor aria-hidden=true href=#conclusion-enabling-advanced-video-interaction>#</a></h3><p>Ming-Omni-Lite demonstrates superior performance compared to same-scale SOTA models on video understanding benchmarks. This validates its strong capability to process <strong>complex, long-duration, information-dense</strong> video content, laying a solid foundation for applications including video summarization, long-form video QA, intelligent tutoring, content moderation, and human-computer interaction.</p><p>We remain committed to advancing Ming-Omni-Lite&rsquo;s capabilities in video and multimodal domains, striving to develop intelligent agents capable of genuine comprehension, reasoning, and interaction with the real world.</p><hr><h2 id=audio-understanding-and-generation>Audio Understanding and Generation<a hidden class=anchor aria-hidden=true href=#audio-understanding-and-generation>#</a></h2><p>To enhance the capabilities of our model in Spoken Language Understanding (SLU), we incorporated an extensive and diverse corpus of speech data during the training phase. Furthermore, we integrated critical metadata‚Äîincluding domain, topic, language, and various dialects‚Äîdirectly into the instructional prompts for the understanding tasks. This methodology significantly improves the model&rsquo;s contextual comprehension. Our model demonstrates proficiency in understanding Mandarin Chinese, English, as well as a range of Chinese dialects such as Cantonese, Sichuan, Shanghai, and Minnan. As a result, it achieves <strong>SOTA</strong> performance on publicly available Mandarin and English benchmarks.</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>aishell1</th><th style=text-align:center>aishell2_android</th><th style=text-align:center>aishell2_ios</th><th style=text-align:center>cv15_zh</th><th style=text-align:center>fleurs_zh</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>1.31</td><td style=text-align:center><strong>2.45</strong></td><td style=text-align:center><strong>2.45</strong></td><td style=text-align:center>5.69</td><td style=text-align:center>2.87</td></tr><tr><td style=text-align:center>Qwen2.-Omni</td><td style=text-align:center>1.18</td><td style=text-align:center>2.75</td><td style=text-align:center>2.63</td><td style=text-align:center><strong>5.20</strong></td><td style=text-align:center>3.00</td></tr><tr><td style=text-align:center>Qwen2-Audio</td><td style=text-align:center>1.53</td><td style=text-align:center>2.92</td><td style=text-align:center>2.92</td><td style=text-align:center>6.90</td><td style=text-align:center>7.50</td></tr><tr><td style=text-align:center>Kimi-Audio</td><td style=text-align:center><strong>0.60</strong></td><td style=text-align:center>2.64</td><td style=text-align:center>2.56</td><td style=text-align:center>7.21</td><td style=text-align:center><strong>2.69</strong></td></tr></tbody></table><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>ws_meet</th><th style=text-align:center>ws_net</th><th style=text-align:center>lsc_test_cl</th><th style=text-align:center>lsc_test_ot</th><th style=text-align:center>multi_ls</th><th style=text-align:center>cv15_en</th><th style=text-align:center>flu_en</th><th style=text-align:center>vox_en</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>6.18</td><td style=text-align:center><strong>5.22</strong></td><td style=text-align:center><strong>1.24</strong></td><td style=text-align:center>2.61</td><td style=text-align:center><strong>4.13</strong></td><td style=text-align:center><strong>6.95</strong></td><td style=text-align:center><strong>3.28</strong></td><td style=text-align:center>6.82</td></tr><tr><td style=text-align:center>Qwen2-Omni</td><td style=text-align:center><strong>5.90</strong></td><td style=text-align:center>7.70</td><td style=text-align:center>1.80</td><td style=text-align:center>3.40</td><td style=text-align:center>7.56</td><td style=text-align:center>7.60</td><td style=text-align:center>4.10</td><td style=text-align:center><strong>5.80</strong></td></tr><tr><td style=text-align:center>Qwen2-Audio</td><td style=text-align:center>7.16</td><td style=text-align:center>8.42</td><td style=text-align:center>1.60</td><td style=text-align:center>3.60</td><td style=text-align:center>5.40</td><td style=text-align:center>8.60</td><td style=text-align:center>6.90</td><td style=text-align:center>6.84</td></tr><tr><td style=text-align:center>Kimi-Audio</td><td style=text-align:center>6.28</td><td style=text-align:center>5.37</td><td style=text-align:center>1.28</td><td style=text-align:center><strong>2.42</strong></td><td style=text-align:center>5.88</td><td style=text-align:center>10.31</td><td style=text-align:center>4.44</td><td style=text-align:center>7.97</td></tr></tbody></table><style type=text/css>.tg{border:none;border-collapse:collapse;border-spacing:0}.tg td{border-style:solid;border-width:0;font-family:Arial,sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal}.tg th{border-style:solid;border-width:0;font-family:Arial,sans-serif;font-size:14px;font-weight:400;overflow:hidden;padding:10px 5px;word-break:normal}.tg .tg-x5q1{font-size:16px;text-align:left;vertical-align:top}.tg .tg-t0cb{background-color:#fff;color:#1f1f1f;font-size:16px;text-align:left;vertical-align:middle}.tg .tg-hxmt{background-color:#fff;color:#1f1f1f;font-size:16px;text-align:left;vertical-align:top}.tg .tg-19xi{background-color:#fff;color:#1f1f1f;font-size:16px;font-weight:700;text-align:center;vertical-align:middle}</style><table class=tg><thead><tr><th class=tg-19xi>Input Audio</th><th class=tg-19xi>Dialect</th><th class=tg-19xi>Recognition</th></tr></thead><tbody><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr1.wav type=audio/wav></audio></td><td class=tg-t0cb>Á≤§ËØ≠</td><td class=tg-t0cb>‰Ω†Âú®Âπ≤‰ªÄ‰πà, ÊòØ‰∏çÊòØ‰∏çÊÉ≥ËÅäÂ§©</td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr2.wav type=audio/wav></audio></td><td class=tg-t0cb>‰∏äÊµ∑ËØù</td><td class=tg-t0cb>Êàë‰ª¨ËÄÉËØïËøòÊ≤°ÂÆö‰∏ãÊù•Âë¢</td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr3.wav type=audio/wav></audio></td><td class=tg-t0cb>ÈóΩÂçóËØ≠</td><td class=tg-t0cb>ÂÆùË¥ù, Êó©ÁÇπ‰ºëÊÅØ, ÊôöÂÆâ</td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr4.wav type=audio/wav></audio></td><td class=tg-t0cb>ÂõõÂ∑ùËØù</td><td class=tg-t0cb>ÊàëÈöæÂèóÁöÑÂæà, Âà´‰∫∫ÈÉΩÁù°‰∫Ü</td></tr></tbody></table><p>Leveraging its superior speech understanding capabilities, our model also demonstrates exceptional performance on speech dialogue evaluation benchmarks.</p><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>AlpacaEval</th><th style=text-align:center>CommonEval</th><th style=text-align:center>SD-QA</th><th style=text-align:center>MMSU</th><th style=text-align:center>OpenBookQA</th><th style=text-align:center>IFEval</th><th style=text-align:center>AdvBench</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2-Audio-chat</td><td style=text-align:center>3.69</td><td style=text-align:center>3.40</td><td style=text-align:center>35.35</td><td style=text-align:center>35.43</td><td style=text-align:center>49.01</td><td style=text-align:center>22.57</td><td style=text-align:center>98.85</td></tr><tr><td style=text-align:left>Baichuan-Audio</td><td style=text-align:center>4.00</td><td style=text-align:center>3.39</td><td style=text-align:center>49.64</td><td style=text-align:center>48.80</td><td style=text-align:center>63.30</td><td style=text-align:center>41.32</td><td style=text-align:center>86.73</td></tr><tr><td style=text-align:left>GLM-4-Voice</td><td style=text-align:center>4.06</td><td style=text-align:center>3.48</td><td style=text-align:center>43.31</td><td style=text-align:center>40.11</td><td style=text-align:center>52.97</td><td style=text-align:center>24.91</td><td style=text-align:center>88.08</td></tr><tr><td style=text-align:left>Kimi-Audio</td><td style=text-align:center>4.46</td><td style=text-align:center>3.97</td><td style=text-align:center><b>63.12</b></td><td style=text-align:center><b>62.17</b></td><td style=text-align:center><b>83.52</b></td><td style=text-align:center><b>61.10</b></td><td style=text-align:center><b>100.00</b></td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center>4.49</td><td style=text-align:center>3.93</td><td style=text-align:center>55.71</td><td style=text-align:center>61.32</td><td style=text-align:center>81.10</td><td style=text-align:center>52.87</td><td style=text-align:center>99.42</td></tr><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center><b>4.65</b></td><td style=text-align:center><b>4.30</b></td><td style=text-align:center>61.16</td><td style=text-align:center>45.77</td><td style=text-align:center>65.93</td><td style=text-align:center>55.60</td><td style=text-align:center>98.08</td></tr></tbody></table><table class=tg><thead><tr><th class=tg-19xi>Input Audio</th><th class=tg-19xi>Ouput Audio</th></tr></thead><tbody><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa1.wav type=audio/wav></audio></td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa1_out.wav type=audio/wav></audio></td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa2.wav type=audio/wav></audio></td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa2_out.wav type=audio/wav></audio></td></tr></tbody></table><p>In the domain of speech synthesis, Ming-lite-omni incorporates an advanced audio decoder. This decoder is conditioned on the output hidden states from the LLM, which enables the model to handle both context-aware multimodal dialogue and standard TTS tasks, ultimately generating speech with high naturalness and fluency. To enhance prosodic performance and facilitate real-time generation, we apply Byte Pair Encoding (BPE) to the discrete audio codec tokens. This methodology results in a 35% reduction in the acoustic frame rate, thereby improving synthesis efficiency.</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>seed-tts-eval-zh_wer</th><th style=text-align:center>seed-tts-eval-zh_sim</th><th style=text-align:center>seed-tts-eval-en_wer</th><th style=text-align:center>seed-tts-eval-en_sim</th></tr></thead><tbody><tr><td style=text-align:center>Seed-TTS</td><td style=text-align:center>1.11</td><td style=text-align:center>0.796</td><td style=text-align:center>2.24</td><td style=text-align:center>0.762</td></tr><tr><td style=text-align:center>MaskGCT</td><td style=text-align:center>2.27</td><td style=text-align:center>0.774</td><td style=text-align:center>2.62</td><td style=text-align:center>0.714</td></tr><tr><td style=text-align:center>E2 TTS</td><td style=text-align:center>1.97</td><td style=text-align:center>0.730</td><td style=text-align:center>2.19</td><td style=text-align:center>0.710</td></tr><tr><td style=text-align:center>F5-TTS</td><td style=text-align:center>1.56</td><td style=text-align:center>0.741</td><td style=text-align:center>1.83</td><td style=text-align:center>0.647</td></tr><tr><td style=text-align:center>CosyVoice 2</td><td style=text-align:center>1.45</td><td style=text-align:center>0.748</td><td style=text-align:center>2.57</td><td style=text-align:center>0.652</td></tr><tr><td style=text-align:center>Qwen2.5-Omni-7B</td><td style=text-align:center>1.70</td><td style=text-align:center>0.752</td><td style=text-align:center>2.72</td><td style=text-align:center>0.632</td></tr><tr><td style=text-align:center>Ming-Lite-Omni</td><td style=text-align:center>2.00</td><td style=text-align:center>0.686</td><td style=text-align:center>4.299</td><td style=text-align:center>0.513</td></tr></tbody></table><table class=tg><thead><tr><th class=tg-19xi>Prompt Audio</th><th class=tg-19xi>Input Text</th><th class=tg-19xi>Output Audio</th></tr></thead><tbody><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts1_prompt.wav type=audio/wav></audio></td><td class=tg-t0cb>Êàë‰ª¨ÁöÑÊÑøÊôØÊòØÊûÑÂª∫Êú™Êù•ÊúçÂä°‰∏öÁöÑÊï∞Â≠óÂåñÂü∫Á°ÄËÆæÊñΩÔºåÁªô‰∏ñÁïåÂ∏¶Êù•Êõ¥Â§öÂæÆÂ∞èËÄåÁæéÂ•ΩÁöÑÊîπÂèò„ÄÇ</td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts1wav.wav type=audio/wav></audio></td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts2_prompt.wav type=audio/wav></audio></td><td class=tg-t0cb>The stained glass offered a hypnotic atmosphere</td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts2.wav type=audio/wav></audio></td></tr></tbody></table><hr><h2 id=breaking-new-limits-in-image-generation>Breaking New Limits in Image Generation<a hidden class=anchor aria-hidden=true href=#breaking-new-limits-in-image-generation>#</a></h2><p>üî• Building on the earlier releases of Ming-Lite-Uni and Ming-Omni, this version takes Ming&rsquo;s image generation to the next level! We&rsquo;ve enhanced scene consistency, ID consistency (for characters and styles), and perception expansion (including segmentation, keypoints, depth, and more). Ming has evolved from a multimodal large model (MLLM) with image generation and editing talents to a comprehensive MLLM capable of tackling even more image generation tasks with superior results. Check out our recent progress report below. We&rsquo;re eager to exchange ideas and discuss with you all!</p><p><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752039359523-ef57c4ba-3f99-4a9a-9515-5728b6c46c1c.webp alt></p><table><thead><tr><th></th><th><strong>GenEval</strong></th><th><strong>Single Ojbect</strong></th><th><strong>Two Objects</strong></th><th><strong>Counting</strong></th><th><strong>Color</strong></th><th><strong>Position</strong></th><th><strong>Color Attr</strong></th></tr></thead><tbody><tr><td>Ours</td><td>0.86</td><td>100.00%</td><td>96.72%</td><td>76.56%</td><td>89.89%</td><td>89.75%</td><td>68.69%</td></tr></tbody></table><h3 id=model-structure-review-and-improvements>Model Structure Review and Improvements<a hidden class=anchor aria-hidden=true href=#model-structure-review-and-improvements>#</a></h3><table><thead><tr><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Sub-module</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Technical Point</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Function</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Key Points</font></strong></th></tr></thead><tbody><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Cross-modal Bridging Method</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Channel concat / Token concat / Blend</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Channel concat</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">: Fewer parameters, low memory use, but weaker semantic alignment </font><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Token concat</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">: Preserves semantic structure, suitable for high resolution </font><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Blend</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">: More robust for scene editing/rewriting</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Choose bridging method based on task needs. Currently using Token Concat approach</font></td></tr><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Dual-branch Representation Decoupling</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Decouple reference image patch encoding and refiner parameters</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- Enhance the independent control parameter capacity of reference images, providing partial decoupling</font></td><td>Dual patchfy modules and extra dual-branch refiner improve model editing and segmentation performance</td></tr></tbody></table><ul><li>Dual-branch decoupling refers to using different network weights to patchfy reference images and noise images before feeding them into DiT‚Äôs transformer. This effectively reduces the influence of reference image information on semantic adherence during editing. The refiner is an extra lightweight two-layer transformer added after patchfy to further enhance this effect. Performance assessments in inference segmentation demonstrate the effectiveness of the new module.<ul><li>Inference segmentation tests the model‚Äôs ability to correctly understand semantics, requiring it to determine the target of segmentation based on complex instructions.</li><li>Experimental results are shown in the table below. It is evident that decoupling patchfy significantly improves segmentation metrics. Adding the refiner module can further enhance performance.</li></ul></li><li>GEdit Subset: [&ldquo;background_change&rdquo;, &ldquo;color_alter&rdquo;, &ldquo;material_alter&rdquo;, &ldquo;motion_change&rdquo;]</li></ul><table><thead><tr><th>Mode ID</th><th>double-patchfy</th><th>add-refiner</th><th>refcoc segmentation metric</th><th>GEdit(subset-full)</th></tr></thead><tbody><tr><td>0</td><td>‚ùå</td><td>‚ùå</td><td>62.8</td><td>6.129</td></tr><tr><td>1</td><td>‚úÖ</td><td>‚ùå</td><td>64.2</td><td>6.391</td></tr><tr><td>2</td><td>‚úÖ</td><td>‚úÖ</td><td>64.5</td><td>6.306</td></tr></tbody></table><h3 id=conditional-control-and-guidance-strategy>Conditional Control and Guidance Strategy<a hidden class=anchor aria-hidden=true href=#conditional-control-and-guidance-strategy>#</a></h3><table><thead><tr><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Sub-module</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Technical Point</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Function</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">Key Points</font></strong></th></tr></thead><tbody><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Multi-condition CFG Control Strategy</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Semantic CFG vs Image CFG (Ref-Guided)</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Multi-condition Classifier-free Guidance strategy: semantic binary differentiation and image ternary differentiation enhance ID consistency</font></td><td>With pure semantic control, the edited image follows the instructions but loses all consistency with the original image; when the image branch guidance intensity is high, the edited result is almost identical to the original image</td></tr><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">ID & Scene Consistency Loss</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Weight mask Loss + Scene Consistency Loss</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Increase the weight of the target image editing area, while adding strong constraints to the non-editing area and weak constraints to the editing area of the reference image</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Adjust Œª to balance editing effects while maintaining ID consistency and scene consistency, avoiding overfitting</font></td></tr></tbody></table><ul><li>Comparison with Qwen-VLo</li></ul><table><thead><tr><th>prompt</th><th>ours</th><th>Qwen-VLo</th></tr></thead><tbody><tr><td><font style="color:rgb(44, 44, 54);">Make the person in the image smile slightly without altering the original structure</font><br><img loading=lazy src="https://github.com/Biao-Gong/static/blob/main/gen/1752147843685-5b097f6b-b2aa-4baf-abe4-f1abd89265e8.png?raw=true" alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752147837185-62077f0c-e7ec-415f-bd34-1c8453253949.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752147953713-703c31c8-2fd1-4c2d-b4bc-6e0f52e70017.webp alt></td></tr></tbody></table><h3 id=perception-capability-expansion>Perception Capability Expansion<a hidden class=anchor aria-hidden=true href=#perception-capability-expansion>#</a></h3><ul><li><p><strong>Generative Segmentation</strong></p><p>Unlike generative image editing tasks, segmentation tasks have less detail consistency between the predicted mask and the original image. Hence, under the token concat approach, learning the consistency relationship between the original image and segmentation mask can be challenging. To address this, we model the segmentation target as a colored segmentation image‚Äîa fusion of mask and image‚Äîallowing more consistent details between the predicted target and the original image. This enhances the ability to learn the relationship between the segmentation map and the original image. During inference, the predicted image and original image are differenced, and noise is filtered to obtain the final prediction mask.</p></li></ul><table><thead><tr><th>Input Image</th><th>Inference Segmentation</th><th>Semantic Segmentation</th><th>Panoptic Segmentation</th></tr></thead><tbody><tr><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp alt><br>prompt: Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp alt><br>prompt: Please segment different <strong>classes</strong> in this image</td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp alt><br>prompt: Please segment different <strong>instances</strong> in this image.</td></tr></tbody></table><ul><li><strong>Edge Contour Map Generation</strong></li></ul><table><thead><tr><th>Original Image</th><th>Depth Map</th><th>Detection Box</th><th>Edge Contour</th></tr></thead><tbody><tr><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><hr><h2 id=data-engineering>Data Engineering<a hidden class=anchor aria-hidden=true href=#data-engineering>#</a></h2><p><strong>OCR-related parsing corpus.</strong> To enhance the base model&rsquo;s performance in text-associated vision capability and logical reasoning, we introduce the Chain-of-Thought (CoT) paradigm during training. The specific implementation strategies are as follows:
Firstly, we integrate the open-source ChartQA-PoT (Program-of-Thought) dataset into the training framework, with particular emphasis on improving the model&rsquo;s numerical computation capabilities for charts. This approach inherits the progressive reasoning philosophy of Chain-of-Thought, but innovatively adopts executable Python code as the intermediate reasoning medium. Secondly, to address the limitations of the prevalent &ldquo;answer-direct prompting&rdquo; pattern in conventional text-based question-answering datasets, we innovatively employ reinforcement learning models to generate multi-step reasoning trajectories and final answers from original training samples. Through formal verification, we construct a novel training dataset comprising validated &ldquo;reasoning step-final answer&rdquo; pairs. This methodology significantly enhances the model&rsquo;s adaptability to complex logical tasks.</p><p><strong>Human preference corpus.</strong> Human preference data optimizes MLLM responses through enhanced improved alignment with human-centric interaction patterns in the alignment tuning stage. Our preference corpus is primarily constructed from three sources: user-generated conversations in applications, search queries from websites, and high-quality open-source instruction datasets. Specifically, we first retrieve relevant web images via search engines to complement text-only user-generated dialogues or queries. We then leverage available MLLMs to generate diverse specialized questions and their corresponding answers. Afterwards, we organize those high-quality positive samples and the other negative ones to construct the preference corpus, which comprises 41 subcategories across 9 primary domains. Ultimately, we engage MLLMs and skilled human annotators to assess the quality of these QA pairs.</p><p><strong>Video understanding corpus.</strong> High-quality video data provides richer and more precise semantic information, which effectively enhances the model‚Äôs depth and breadth of understanding complex video scenarios. The video corpus we constructed mainly derives from two sources: First, we have extensively mined and integrated existing open-source datasets for fundamental visual tasks‚Äîfor example, leveraging the GOT-10K object tracking dataset to strengthen the model‚Äôs object tracking capability, and utilizing the DiDemo temporal retrieval dataset to improve the model‚Äôs event perception ability. Second, we organized a team of experts to meticulously annotate a set of challenging video question-answer pairs, dedicated to training the model‚Äôs capability for complex reasoning.</p><p><strong>Encyclopedia corpus.</strong> Encyclopedia data integrates advanced domain-specific expertise into MLLMs for expert-level comprehension and perception, e.g., identifying rare or endangered species via Latin binomial nomenclature. In this version, our encyclopedia corpus spans 10 specific domains across biological categories (Plants and Animals), cultural categories (Celebrities, Anime characters, Landmarks, LOGOs, and Artworks), and daily-life categories (Ingredients, Dishes, and Vehicles). To construct a high-quality expert-level corpus, we first collect a wide range of encyclopedia entities from academic databases and institutional websites. We then employ these entities as search queries to collect semantically relevant images via search engines. Afterwards, we develop a progressive encyclopedia data filtering scheme, including clip consistency validation, MLLM-based binary verification, and manual refinement.</p><p><strong>GUI corpus.</strong> The GUI (Graphic User Interface) data enables the model with basic GUI navigation ability on Android environment. Our GUI corpus is mainly constructed from four public datasets: AITW, GUICourse, AndroidControl and AMEX. Moreover, we leverage available MLLMs to optimize the reasoning process for each step within a human-like GUI interaction operation, which is subsequently reviewed by another MLLM to improve data quality. The thinking process constrains model to observe current state and reflect previous actions with careful consideration before acting. To better perceive current situation, we involve history memory, which includes preceding actions.</p><p><strong>Image generation and editing corpus.</strong> Our image generation corpus mainly comes from two sources: High-quality images collected from public image generation datasets (e.g., text-to-image-2M, JourneyDB, BLip3o, Uniworld dataset, InstructPix2Pix-clip-filtered, SEED-Data-Edit-part2/3, Ultraedit, etc.); and image style transfer data sampled from StyleBooth and WikiArt. And also, we constructed a few generation pipelines, producing part of text-to-image and editing data.</p><p><strong>Perceptual reasoning corpus.</strong> Perceptual reasoning data can enhance the model&rsquo;s comprehensive comprehension capabilities and fine-grained perceptual abilities. In this version, we focus on perception-oriented subtasks including object counting, color recognition, and scene theme identification. A set of potential challenging samples were curated from open-source datasets such as Object365 and RefCOCO through systematic filtering based on bounding box quantities, spatial relationships, and object category counts. Subsequently, these candidate samples underwent task-specific question-answer synthesis using Visual Language Models (VLMs), followed by rigorous quality assurance through multi-model evaluation scoring and manual filtering/correction to obtain high-quality annotations. Furthermore, to strengthen the model&rsquo;s reasoning capacity in perceptual tasks, we reformulated the synthesized data into Chain-of-Thought (CoT) format. This transformation introduces intermediate reasoning steps to assist the model in decomposing complex problems, thereby systematically improving its integrated perceptual understanding.</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
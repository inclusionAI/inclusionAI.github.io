<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.e43d02cb5d47286722d4c0b3c445053495d7038e5d2de897387de22eaa50244e.css integrity="sha256-5D0Cy11HKGci1MCzxEUFNJXXA45dLeiXOH3iLqpQJE4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-entry><header class=entry-header><h2>Introducing Ming-Lite-Omni V1.5</h2></header><div class=entry-content><p>GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models.
...</p></div><footer class=entry-footer><span class=post-date title="2025-07-21 00:00:03 +0800 +0800">July 21, 2025</span>
<span class=post-word-count>2277 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Introducing Ming-Lite-Omni V1.5" href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/></a></article><article class=post-entry><header class=entry-header><h2>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</h2></header><div class=entry-content><p>üìñ Technical Report | ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals. This combination of curated data and advanced training allows M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks, showcasing superior performance in both general and spatial reasoning domains. ...</p></div><footer class=entry-footer><span class=post-date title="2025-07-11 00:00:03 +0800 +0800">July 11, 2025</span>
<span class=post-word-count>1052 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning" href=https://inclusionai.github.io/blog/m2-reasoning/></a></article><article class=post-entry><header class=entry-header><h2>ABench: An Evolving Open-Source Benchmark</h2></header><div class=entry-content><p>GITHUB üåü Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
üéØ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems üìä Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics ‚úÖ Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management ‚úÖ Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) üîÑ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories üîÑ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law üîÑ In Preparation</p></div><footer class=entry-footer><span class=post-date title="2025-07-08 00:00:03 +0800 +0800">July 8, 2025</span>
<span class=post-word-count>185 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to ABench: An Evolving Open-Source Benchmark" href=https://inclusionai.github.io/blog/abench/></a></article><article class=post-entry><header class=entry-header><h2>AWorld: The Agent Runtime for Self-Improvement</h2></header><div class=entry-content><p>‚ÄúSelf-awareness: the hardest problem isn‚Äôt solving within limits, it‚Äôs discovering the own limitations‚Äù Table of Contents News ‚Äî Latest updates and announcements. Introduction ‚Äî Overview and purpose of the project. Installation ‚Äî Step-by-step setup instructions. Quick Start ‚Äî Get started with usage examples. Architecture ‚Äî Explore the multi-agent system design. Demo ‚Äî See the project in action with demonstrations. Contributing ‚Äî How to get involved and contribute. License ‚Äî Project licensing details. News ü¶§ [2025/07/07] AWorld, as a runtime, is now ready for agentic training. See Self-Improvement section for details. We have updated our score to 77.08 on the GAIA test. Learn how to construct a GAIA runtime in the Demo section. ü¶© [2025/06/19] We have updated our score to 72.43 on the GAIA test. Additionally, we have introduced a new local running mode. See ./README-local.md for detailed instructions. üê≥ [2025/05/22] For quick GAIA evaluation, MCP tools, AWorld, and models are now available in a single Docker image. See ./README-docker.md for instructions and youtube video for demo. ü•≥ [2025/05/13] AWorld has updated its state management for browser use and enhanced the video processing MCP server, achieving a score of 77.58 on GAIA validation (Pass@1 = 61.8) and maintaining its position as the top-ranked open-source framework. Learn more: GAIA leaderboard ‚ú® [2025/04/23] AWorld ranks 3rd on GAIA benchmark (69.7 avg) with impressive Pass@1 = 58.8, 1st among open-source frameworks. Reproduce with python examples/gaia/run.py Introduction AWorld (Agent World) is a multi-agent playground that enables agents to collaborate and self-improve. The framework supports a wide range of applications, including but not limited to product prototype verification, foundation model training and Multi-Agent System (MAS) design meta-learning.
...</p></div><footer class=entry-footer><span class=post-date title="2025-07-07 00:00:03 +0800 +0800">July 7, 2025</span>
<span class=post-word-count>895 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to AWorld: The Agent Runtime for Self-Improvement" href=https://inclusionai.github.io/blog/aworld/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Omni: A Unified Multimodal Model for Perception and Generation</h2></header><div class=entry-content><p>GITHUB üìë Technical ReportÔΩúüìñProject Page ÔΩúü§ó Hugging FaceÔΩú ü§ñ ModelScope
Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers. This design enables a single model to efficiently process and fuse multimodal inputs within a unified framework, thereby facilitating diverse tasks without requiring separate models, task-specific fine-tuning, or structural redesign. Importantly, Ming-lite-omni extends beyond conventional multimodal models by supporting audio and image generation. This is achieved through the integration of an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for high-quality image generation, which also allow the model to engage in context-aware chatting, perform text-to-speech conversion, and conduct versatile image editing. Our experimental results showcase Ming-lite-omni offers a powerful solution for unified perception and generation across all modalities. Notably, Ming-lite-omni is the first open-source model we are aware of to match GPT-4o in modality support, and we release all code and model weights to encourage further research and development in the community.
...</p></div><footer class=entry-footer><span class=post-date title="2025-06-11 00:00:03 +0800 +0800">June 11, 2025</span>
<span class=post-word-count>1379 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Ming-Omni: A Unified Multimodal Model for Perception and Generation" href=https://inclusionai.github.io/blog/ming-omni/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://inclusionai.github.io/blog/>¬´&nbsp;Prev&nbsp;
</a><a class=next href=https://inclusionai.github.io/blog/page/3/>Next&nbsp;&nbsp;¬ª</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#FFCC99 0%,#7F667F 40%,#262656 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/>ç®€ä½“ä¸­æ–‡</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</h2></header><div class=entry-content><p>GITHUB ðŸ¤— Hugging Face | ðŸ¤– ModelScope
Introduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable....</p></div><footer class=entry-footer><span title='2025-05-05 00:00:03 +0800 +0800'>May 5, 2025</span>&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;1105 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities" href=https://inclusionai.github.io/blog/ming-lite-omni-preview/></a></article><article class=post-entry><header class=entry-header><h2>Agentic Learning</h2></header><div class=entry-content><p>Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively....</p></div><footer class=entry-footer><span title='2025-04-01 00:00:03 +0800 +0800'>April 1, 2025</span>&nbsp;Â·&nbsp;3 min&nbsp;Â·&nbsp;446 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Agentic Learning" href=https://inclusionai.github.io/blog/agenticlearning/></a></article><article class=post-entry><header class=entry-header><h2>AReaL: Ant Reasoning Reinforcement Learning for LLMs</h2></header><div class=entry-content><p>| Paper | Documentation | Ask DeepWiki | ðŸ¤— Models & Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably....</p></div><footer class=entry-footer><span title='2025-04-01 00:00:03 +0800 +0800'>April 1, 2025</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1431 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to AReaL: Ant Reasoning Reinforcement Learning for LLMs" href=https://inclusionai.github.io/blog/areal/></a></article><article class=post-entry><header class=entry-header><h2>PromptCoT & PromptCoT-Mamba: Advancing the Frontiers of Reasoning</h2></header><div class=entry-content><p>News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba....</p></div><footer class=entry-footer><span title='2025-04-01 00:00:03 +0800 +0800'>April 1, 2025</span>&nbsp;Â·&nbsp;4 min&nbsp;Â·&nbsp;823 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to PromptCoT & PromptCoT-Mamba: Advancing the Frontiers of Reasoning" href=https://inclusionai.github.io/blog/promptcot/></a></article><article class=post-entry><header class=entry-header><h2>Ring: A Reasoning MoE LLM Provided and Open-sourced by InclusionAI</h2></header><div class=entry-content><p>ðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScope News [2025-06]:ðŸŽ‰ Add Ring-lite Model [2025-04]:ðŸŽ‰ Add Ring-lite-linear-preview Model Introduction Ring is a reasoning MoE LLM provided and open-sourced by InclusionAI, derived from Ling. We introduce Ring-lite-distill-preview, which has 16.8 billion parameters with 2.75 billion activated parameters. This model demonstrates impressive reasoning performance compared to existing models in the industry.
Model Downloads You can download the following table to see the various parameters for your use case....</p></div><footer class=entry-footer><span title='2025-04-01 00:00:03 +0800 +0800'>April 1, 2025</span>&nbsp;Â·&nbsp;2 min&nbsp;Â·&nbsp;258 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ring: A Reasoning MoE LLM Provided and Open-sourced by InclusionAI" href=https://inclusionai.github.io/blog/ring/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://inclusionai.github.io/blog/page/2/>Â«&nbsp;Prev&nbsp;</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
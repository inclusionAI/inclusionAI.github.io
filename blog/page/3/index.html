<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.e43d02cb5d47286722d4c0b3c445053495d7038e5d2de897387de22eaa50244e.css integrity="sha256-5D0Cy11HKGci1MCzxEUFNJXXA45dLeiXOH3iLqpQJE4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-entry><header class=entry-header><h2>Ling: A MoE LLM Provided and Open-sourced by InclusionAI</h2></header><div class=entry-content><p>ðŸ¤— Hugging FaceÂ Â  | Â Â ðŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems. Furthermore, the open-source nature of Ling promotes collaboration and innovation within the AI community, fostering a diverse range of use cases and enhancements.
...</p></div><footer class=entry-footer><span class=post-date title="2025-05-08 00:00:03 +0800 +0800">May 8, 2025</span>
<span class=post-word-count>1574 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Ling: A MoE LLM Provided and Open-sourced by InclusionAI" href=https://inclusionai.github.io/blog/ling/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</h2></header><div class=entry-content><p>GITHUB ðŸ“‘ Paperï½œðŸ¤— Hugging Faceï½œðŸ¤– ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement.
...</p></div><footer class=entry-footer><span class=post-date title="2025-05-07 00:00:03 +0800 +0800">May 7, 2025</span>
<span class=post-word-count>1133 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction" href=https://inclusionai.github.io/blog/ming-lite-uni/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</h2></header><div class=entry-content><p>GITHUB ðŸ¤— Hugging Face | ðŸ¤– ModelScope
Introduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.
...</p></div><footer class=entry-footer><span class=post-date title="2025-05-05 00:00:03 +0800 +0800">May 5, 2025</span>
<span class=post-word-count>1105 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities" href=https://inclusionai.github.io/blog/ming-lite-omni-preview/></a></article><article class=post-entry><header class=entry-header><h2>Agentic Learning</h2></header><div class=entry-content><p>Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the modelsâ€™ ability to leverage the environment to solve complex problems.
...</p></div><footer class=entry-footer><span class=post-date title="2025-04-01 00:00:03 +0800 +0800">April 1, 2025</span>
<span class=post-word-count>446 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Agentic Learning" href=https://inclusionai.github.io/blog/agenticlearning/></a></article><article class=post-entry><header class=entry-header><h2>AReaL: Ant Reasoning Reinforcement Learning for LLMs</h2></header><div class=entry-content><p>| Paper | Documentation | Ask DeepWiki | ðŸ¤— Models & Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably. Our team loves milk tea because itâ€™s delicious, customizable, and affordable. We hope you enjoy our project just like how you enjoy real-world milk tea (cheers).
...</p></div><footer class=entry-footer><span class=post-date title="2025-04-01 00:00:03 +0800 +0800">April 1, 2025</span>
<span class=post-word-count>1431 words</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to AReaL: Ant Reasoning Reinforcement Learning for LLMs" href=https://inclusionai.github.io/blog/areal/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://inclusionai.github.io/blog/page/2/>Â«&nbsp;Prev&nbsp;
</a><a class=next href=https://inclusionai.github.io/blog/page/4/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
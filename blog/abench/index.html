<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>ABench: An Evolving Open-Source Benchmark | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB
ðŸŒŸ Overview
ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ðŸŽ¯ Core Objectives

Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types
Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation
Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems

ðŸ“Š Dataset Release Status

  
      
          Domain
          Description
          Status
      
  
  
      
          Physics
          500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics
          âœ… Released
      
      
          Actuary
          Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management
          âœ… Released
      
      
          Logic
          High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam)
          ðŸ”„ In Preparation
      
      
          Psychology
          Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories
          ðŸ”„ In Preparation
      
      
          Law
          Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law
          ðŸ”„ In Preparation
      
  
"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/abench/><link crossorigin=anonymous href=/assets/css/stylesheet.e43d02cb5d47286722d4c0b3c445053495d7038e5d2de897387de22eaa50244e.css integrity="sha256-5D0Cy11HKGci1MCzxEUFNJXXA45dLeiXOH3iLqpQJE4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/abench/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="ABench: An Evolving Open-Source Benchmark"><meta property="og:description" content="GITHUB
ðŸŒŸ Overview
ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ðŸŽ¯ Core Objectives

Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types
Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation
Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems

ðŸ“Š Dataset Release Status

  
      
          Domain
          Description
          Status
      
  
  
      
          Physics
          500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics
          âœ… Released
      
      
          Actuary
          Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management
          âœ… Released
      
      
          Logic
          High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam)
          ðŸ”„ In Preparation
      
      
          Psychology
          Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories
          ðŸ”„ In Preparation
      
      
          Law
          Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law
          ðŸ”„ In Preparation
      
  
"><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/abench/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-08T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-08T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="ABench: An Evolving Open-Source Benchmark"><meta name=twitter:description content="GITHUB
ðŸŒŸ Overview
ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ðŸŽ¯ Core Objectives

Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types
Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation
Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems

ðŸ“Š Dataset Release Status

  
      
          Domain
          Description
          Status
      
  
  
      
          Physics
          500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics
          âœ… Released
      
      
          Actuary
          Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management
          âœ… Released
      
      
          Logic
          High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam)
          ðŸ”„ In Preparation
      
      
          Psychology
          Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories
          ðŸ”„ In Preparation
      
      
          Law
          Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law
          ðŸ”„ In Preparation
      
  
"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"ABench: An Evolving Open-Source Benchmark","item":"https://inclusionai.github.io/blog/abench/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"ABench: An Evolving Open-Source Benchmark","name":"ABench: An Evolving Open-Source Benchmark","description":"GITHUB ðŸŒŸ Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.\nðŸŽ¯ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems ðŸ“Š Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics âœ… Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management âœ… Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) ðŸ”„ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories ðŸ”„ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law ðŸ”„ In Preparation ","keywords":[],"articleBody":"GITHUB ðŸŒŸ Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.\nðŸŽ¯ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems ðŸ“Š Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics âœ… Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management âœ… Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) ðŸ”„ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories ðŸ”„ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law ðŸ”„ In Preparation ","wordCount":"185","inLanguage":"en","datePublished":"2025-07-08T00:00:03+08:00","dateModified":"2025-07-08T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/abench/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;Â»&nbsp;<a href=https://inclusionai.github.io/blog/>Blog</a></div><h1 class=post-title>ABench: An Evolving Open-Source Benchmark</h1><div class=post-meta><span class=post-date title="2025-07-08 00:00:03 +0800 +0800">July 8, 2025</span>
<span class=post-word-count>185 words</span>
<span class=post-author>inclusionAI, Ant Group</span></div></header><div class=post-content><a href=https://github.com/inclusionAI/ABench class="btn external" target=_blank>GITHUB</a><h2 id=-overview>ðŸŒŸ Overview<a hidden class=anchor aria-hidden=true href=#-overview>#</a></h2><p><strong>ABench</strong> is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on <strong>complex cross-domain tasks</strong>. By targeting current model weaknesses, ABench provides systematic challenges in <strong>high-difficulty specialized domains</strong>, including physics, actuarial science, logical reasoning, law, and psychology.</p><h2 id=-core-objectives>ðŸŽ¯ Core Objectives<a hidden class=anchor aria-hidden=true href=#-core-objectives>#</a></h2><ol><li><strong>Address Evaluation Gaps</strong>: Design high-differentiation assessment tasks targeting <strong>underperforming question types</strong></li><li><strong>Establish Unified Standards</strong>: Create <strong>reliable, comparable benchmarks</strong> for multi-domain LLM evaluation</li><li><strong>Expand Capability Boundaries</strong>: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems</li></ol><h2 id=-dataset-release-status>ðŸ“Š Dataset Release Status<a hidden class=anchor aria-hidden=true href=#-dataset-release-status>#</a></h2><table><thead><tr><th>Domain</th><th>Description</th><th>Status</th></tr></thead><tbody><tr><td><strong>Physics</strong></td><td>500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics</td><td><a href=https://github.com/inclusionAI/ABench/blob/main/Physics/README.md>âœ… Released</a></td></tr><tr><td><strong>Actuary</strong></td><td>Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management</td><td><a href=https://github.com/inclusionAI/ABench/blob/main/Actuary/README.md>âœ… Released</a></td></tr><tr><td><strong>Logic</strong></td><td>High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam)</td><td>ðŸ”„ In Preparation</td></tr><tr><td><strong>Psychology</strong></td><td>Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories</td><td>ðŸ”„ In Preparation</td></tr><tr><td><strong>Law</strong></td><td>Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law</td><td>ðŸ”„ In Preparation</td></tr></tbody></table></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
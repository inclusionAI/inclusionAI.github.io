<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Agentic Learning | INCLUSION AI</title><meta name=keywords content><meta name=description content="Introduction
Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment.
For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the models&rsquo; ability to leverage the environment to solve complex problems."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/agenticlearning/><link crossorigin=anonymous href=/assets/css/stylesheet.e43d02cb5d47286722d4c0b3c445053495d7038e5d2de897387de22eaa50244e.css integrity="sha256-5D0Cy11HKGci1MCzxEUFNJXXA45dLeiXOH3iLqpQJE4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/agenticlearning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Agentic Learning"><meta property="og:description" content="Introduction
Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment.
For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the models&rsquo; ability to leverage the environment to solve complex problems."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/agenticlearning/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-04-01T00:00:03+08:00"><meta property="article:modified_time" content="2025-04-01T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Agentic Learning"><meta name=twitter:description content="Introduction
Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment.
For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the models&rsquo; ability to leverage the environment to solve complex problems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Agentic Learning","item":"https://inclusionai.github.io/blog/agenticlearning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Agentic Learning","name":"Agentic Learning","description":"Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.\nWe introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the models\u0026rsquo; ability to leverage the environment to solve complex problems.\n","keywords":[],"articleBody":"Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.\nWe introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the modelsâ€™ ability to leverage the environment to solve complex problems.\nAdvancements Models Tools Environment Training Framework RAG-R1 Qwen2.5-7b-instruct offline retrieval\nonline search AWorld LLaMA-Factory\nverl\nAReaL FunReason Qwen2.5-7b-Coder-instruct BFCL AWorld LLaMA-Factory\nverl News [2025/07/01] ðŸ”¥ðŸ”¥ðŸ”¥RAG-R1 We propose RAG-R1, a deepsearch training framework that incentivizing the search and reasoning capabilities of LLMs through multi-query parallelism.\n[2025/05/16] ðŸ”¥ðŸ”¥ðŸ”¥FunReason We propose FunReason, a novel framework that enhances LLMsâ€™ function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss approach.\nAdvancements Deepsearch RAG-R1 Tools: Search Engines (offline or online) LLM: Qwen2.5-7b-instruct Overall framework of RAG-R1. Performance comparisons on QA benchmarks under the EM metric. The best and second best results are bold and underlined, respectively. FunctionCall FunReason Tools: Real Human Function calling (BFCLv2 live\u0026non-live) LLM: Qwen2.5-7b-Coder-instruct FunReason is a framework designed to enhance LLMsâ€™ function calling capabilities, achieving GPT-4o-comparable performance on BFCL, surpassing RL-based methods, mitigating catastrophic forgetting on HumanEval and MBPP, and using a data refinement strategy where natural CoT data outperforms artificial ones.\nData refinement pipline of FunReason. Overview of FunReasonâ€™s data refinement pipeline. The pipeline consists of five stages: Function Call Classification, Query and Tool Identification, CoT Identification, Function and Parameter Identification, and Format Identification. Each stage ensures specific aspects of data quality, with failing examples either being discarded or regenerated.\nPerformance of FunReason. Citation Please cite our repo if our works are helpful for your research.\n@article{RAG-R1, title={RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism}, author={Zhiwen Tan and Jiaming Huang and Qintong Wu and Hongxuan Zhang and Chenyi Zhuang and Jinjie Gu}, journal={arXiv preprint arXiv:2507.02962}, year={2025} } @article{FunReason, title={FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement}, author={Bingguang Hao, Maolin Wang, Zengzhuang Xu, Cunyin Peng, Yicheng Chen, Xiangyu Zhao, Jinjie Gu, Chenyi Zhuang}, journal={arXiv preprint arXiv:2505.20192}, year={2025} } Contact For any question or feedback, please reach out to us at ender.tzw@antgroup.com or chenyi.zcy@antgroup.com\nLicense This project is licensed under the MIT License - see the LICENSE file for details.\n","wordCount":"446","inLanguage":"en","datePublished":"2025-04-01T00:00:03+08:00","dateModified":"2025-04-01T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/agenticlearning/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;Â»&nbsp;<a href=https://inclusionai.github.io/blog/>Blog</a></div><h1 class=post-title>Agentic Learning</h1><div class=post-meta><span class=post-date title="2025-04-01 00:00:03 +0800 +0800">April 1, 2025</span>
<span class=post-word-count>446 words</span>
<span class=post-author>inclusionAI, Ant Group</span></div></header><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment.
For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.</p><p>We introduce <strong>AgenticLearning</strong>, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively. AgenticLearning offers a framework for multi-turn interactions with the environment, enabling models to learn how to interact with the environment and make decisions based on its feedback, thereby enhancing the models&rsquo; ability to leverage the environment to solve complex problems.</p><table><thead><tr><th style=text-align:center>Advancements</th><th style=text-align:center>Models</th><th style=text-align:center>Tools</th><th style=text-align:center>Environment</th><th style=text-align:center>Training Framework</th></tr></thead><tbody><tr><td style=text-align:center><a href=https://github.com/inclusionAI/AgenticLearning/blob/main/RAG-R1/README.md><strong>RAG-R1</strong></a></td><td style=text-align:center>Qwen2.5-7b-instruct</td><td style=text-align:center>offline retrieval<br><a href=https://github.com/qingw-dev/aworld-mcp-servers>online search</a></td><td style=text-align:center><a href=https://github.com/inclusionAI/AWorld>AWorld</a></td><td style=text-align:center><a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a><br><a href=https://github.com/volcengine/verl>verl</a><br><a href=https://github.com/inclusionAI/AReaL>AReaL</a></td></tr><tr><td style=text-align:center><a href=https://github.com/BingguangHao/FunReason/><strong>FunReason</strong></a></td><td style=text-align:center>Qwen2.5-7b-Coder-instruct</td><td style=text-align:center><a href=https://gorilla.cs.berkeley.edu/leaderboard.html#leaderboard>BFCL</a></td><td style=text-align:center><a href=https://github.com/inclusionAI/AWorld>AWorld</a></td><td style=text-align:center><a href=https://github.com/hiyouga/LLaMA-Factory>LLaMA-Factory</a><br><a href=https://github.com/volcengine/verl>verl</a></td></tr></tbody></table><h2 id=news>News<a hidden class=anchor aria-hidden=true href=#news>#</a></h2><p>[2025/07/01] ðŸ”¥ðŸ”¥ðŸ”¥<a href=https://github.com/inclusionAI/AgenticLearning/blob/main/RAG-R1/README.md><strong>RAG-R1</strong></a> We propose <strong>RAG-R1</strong>, a deepsearch training framework that incentivizing the search and reasoning capabilities of LLMs through multi-query parallelism.</p><p>[2025/05/16] ðŸ”¥ðŸ”¥ðŸ”¥<a href=https://github.com/BingguangHao/FunReason/><strong>FunReason</strong></a> We propose <strong>FunReason</strong>, a novel framework that enhances LLMs&rsquo; function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss approach.</p><h2 id=advancements>Advancements<a hidden class=anchor aria-hidden=true href=#advancements>#</a></h2><h3 id=deepsearch>Deepsearch<a hidden class=anchor aria-hidden=true href=#deepsearch>#</a></h3><h4 id=rag-r1><a href=https://github.com/inclusionAI/AgenticLearning/blob/main/RAG-R1/README.md>RAG-R1</a><a hidden class=anchor aria-hidden=true href=#rag-r1>#</a></h4><ul><li>Tools: Search Engines (offline or <a href=https://github.com/qingw-dev/aworld-mcp-servers>online</a>)</li><li>LLM: Qwen2.5-7b-instruct</li></ul><p><img loading=lazy src=https://github.com/inclusionAI/AgenticLearning/raw/main/RAG-R1/assets/RAG-R1.png alt=RAG-R1-framework></p><h5 align=center>Overall framework of RAG-R1.</h5><p><img loading=lazy src=https://github.com/inclusionAI/AgenticLearning/raw/main/RAG-R1/assets/RAG-R1-result.png alt=RAG-R1-result></p><h5 align=left>Performance comparisons on QA benchmarks under the EM metric. The best and second
best results are bold and underlined, respectively.</h5><h3 id=functioncall>FunctionCall<a hidden class=anchor aria-hidden=true href=#functioncall>#</a></h3><h4 id=funreason><a href=https://github.com/BingguangHao/FunReason/>FunReason</a><a hidden class=anchor aria-hidden=true href=#funreason>#</a></h4><ul><li>Tools: Real Human Function calling (BFCLv2 live&amp;non-live)</li><li>LLM: Qwen2.5-7b-Coder-instruct</li></ul><p>FunReason is a framework designed to enhance LLMs&rsquo; function calling capabilities, achieving GPT-4o-comparable performance on BFCL, surpassing RL-based methods, mitigating catastrophic forgetting on HumanEval and MBPP, and using a data refinement strategy where natural CoT data outperforms artificial ones.</p><p><img loading=lazy src=https://github.com/inclusionAI/AgenticLearning/raw/main/FunctionCall/assets/Fun_pipline.png alt=FunReason-Performance></p><h5 align=center>Data refinement pipline of FunReason.</h5><p><strong>Overview of FunReason&rsquo;s data refinement pipeline.</strong> The pipeline consists of five stages: Function Call Classification, Query and Tool Identification, CoT Identification, Function and Parameter Identification, and Format Identification. Each stage ensures specific aspects of data quality, with failing examples either being discarded or regenerated.</p><p><img loading=lazy src=https://github.com/inclusionAI/AgenticLearning/raw/main/FunctionCall/assets/Fun_per.png alt=FunReason-Performance></p><h5 align=center>Performance of FunReason.</h5><h3 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h3><p>Please cite our repo if our works are helpful for your research.</p><pre tabindex=0><code>@article{RAG-R1,
  title={RAG-R1 : Incentivize the Search and Reasoning Capabilities of LLMs through Multi-query Parallelism}, 
  author={Zhiwen Tan and Jiaming Huang and Qintong Wu and Hongxuan Zhang and Chenyi Zhuang and Jinjie Gu},
  journal={arXiv preprint arXiv:2507.02962},
  year={2025}
}

@article{FunReason,
  title={FunReason: Enhancing Large Language Models&#39; Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement},
  author={Bingguang Hao, Maolin Wang, Zengzhuang Xu, Cunyin Peng, Yicheng Chen, Xiangyu Zhao, Jinjie Gu, Chenyi Zhuang},
  journal={arXiv preprint arXiv:2505.20192},
  year={2025}
}
</code></pre><h2 id=contact>Contact<a hidden class=anchor aria-hidden=true href=#contact>#</a></h2><p>For any question or feedback, please reach out to us at <a href=mailto:ender.tzw@antgroup.com>ender.tzw@antgroup.com</a> or <a href=mailto:chenyi.zcy@antgroup.com>chenyi.zcy@antgroup.com</a></p><h2 id=license>License<a hidden class=anchor aria-hidden=true href=#license>#</a></h2><p>This project is licensed under the MIT License - see the <a href=LICENSE>LICENSE</a> file for details.</p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
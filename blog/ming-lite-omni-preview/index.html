<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope
Introduction
Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-lite-omni-preview/><link crossorigin=anonymous href=/assets/css/stylesheet.419cab9a4e041985806269ccd5fb7e29179062491b1e90454454dcf1af0c9022.css integrity="sha256-QZyrmk4EGYWAYmnM1ft+KReQYkkbHpBFRFTc8a8MkCI=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-preview/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities"><meta property="og:description" content="GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope
Introduction
Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-lite-omni-preview/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-05T00:00:03+08:00"><meta property="article:modified_time" content="2025-05-05T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities"><meta name=twitter:description content="GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope
Introduction
Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities","item":"https://inclusionai.github.io/blog/ming-lite-omni-preview/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities","name":"Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities","description":"GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope\nIntroduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.\n","keywords":[],"articleBody":"GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope\nIntroduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.\nKey Features Omni and Novel MoE Architecture: An innovative Omni architecture based on Mixture of Experts (MoE) that achieves competive performance across multiple modality benchmarks.\nVideo understanding: Supports KV-Cache dynamic compression of visual tokens. While supporting the ability to understand long videos of hours, it can also provide more detailed understanding of short videos of a few seconds.\nNatural Speech Generation and Fine-grained Voice Dialogue: Supports dialect understanding and generation in end-to-end conversations, enables one-shot voice cloning, and enhances prosody through audio tokenizer compression\nEvaluation Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67.1 68.1 MathVista 69.0 68.2 67.9 OCRBench 87.9 86.4 88.2 Average 70.96 70.5 70.3 Object Recognition Object Recognition Ming-Lite-Omni-Preview Qwen2.5-VL-7B InternVL-2.5-8B Plants 52.1 55.3 32.8 Animals 52.6 54.8 36.5 Home appliances \u0026 furniture 93.5 97.4 90.9 Personal Electronics 96.1 95.1 93.2 Food \u0026 Ingredients 57.5 60.0 48.7 Tableware 96.6 94.9 88.1 Vehicles 31.9 40.9 31.9 Average 68.6 71.2 60.3 Video benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5VL-7B VideoMME wo/w sub. 63.9/67.6 65.1/71.6 MVBench 67.0 72.0 Video-MMMU 45.4 47.44 LongVideoBench 53.7 60.0 Audio benchmark SpeechQA Model AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-Lite-Omni-Preview 4.25 3.88 58.95 46.06 60.00 46.71 96.53 ASR Model Aishell-1 Aishell-2 ios Wenetspeech test-net Wenet test-meeting Librispeech test-clean Librispeech test-other Whisper Large-v3 5.14 4.76 9.68 18.54 1.9 3.65 Qwen2-Audio 1.53 3.06 7.72 8.4 1.6 3.6 GLM-4-voice Base 2.46 - - - 2.82 7.66 Baichuan-Omni-1.5 - - 6.9 8.4 - - Qwen2.5-Omni 1.18 2.36 5.9 7.7 1.8 3.4 Ming-Lite-Omni-Preview 1.62 2.82 6.23 6.9 2.34 5.74 Knowledge Model InfoSeek_H-mean InfoSeek_unseen_question InfoSeek_unseen_entity GPT-4o 36.05 - - PaLI-X 22.06 23.5 20.8 Qwen2.5-vl-32B 19.35 20.55 18.28 Ming-Lite-Omni-Preview 27.3 28.9 25.9 OCR\u0026GUI Model Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct ChartQA_TEST 85.2 87.3 DocVQA_TEST 93.2 95.7 OCRBenchV2_en/zh 52.2/51.6 56.3/57.2 OmniDocBenchâ†“ 34.7/34.5 30.8/39.8 TextVQA_VAL 82.36 84.9 ScreenSpot 79.3 84.7 Model Downloads You can download the model from both Huggingface and ModelScope.\nModel Input modality Oput modality Download Ming-Lite-Omni-Preview Image,text,viedio,audio Image,text,audio ğŸ¤— HuggingFace ğŸ¤– ModelScope If you're in mainland China, we strongly recommend you to download our model from ğŸ¤– ModelScope. Use Cases Video-Audio-QA MultiModal Input QA Q: (audio content: è¯·æè¿°è§†é¢‘å†…å®¹ã€‚)\nA: The video features a woman performing a series of yoga poses on a rooftop with a scenic view of mountains and a clear blue sky. Q: Is there any food in front of me? A: Yes, thereâ€™s candy on the table. Speech2Speech (supports dialect) Quickstart Please download our model following Model Downloads, then you can refer to the following codes to run Ming-Lite-Omni-Preview model.\nimport os from transformers import AutoProcessor from modeling_bailingmm import BailingMMNativeForConditionalGeneration # build model model = BailingMMNativeForConditionalGeneration.from_pretrained( \"inclusionAI/Ming-Lite-Omni\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).to(\"cuda\") assets_path = YOUR_ASSETS_PATH # build processor processor = AutoProcessor.from_pretrained(\"inclusionAI/Ming-Lite-Omni\", trust_remote_code=True) # qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚\"} ], }, ] # Output: # é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š # ### 1. **æ –æ¯åœ°** # é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚ # ### 2. **é¥®é£Ÿ** # é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚ # ...... # image qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"image\", \"image\": os.path.join(assets_path, \"flowers.jpg\")}, {\"type\": \"text\", \"text\": \"What kind of flower is this?\"}, ], }, ] # Output: # The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white. To enable thinking before response, adding the following system prompt before your question:\ncot_prompt = \"SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in ... tags, then the final answer enclosed in ... tags. The critical answer or key result should be placed within \\\\boxed{}.\\n\" # And your input message should be like this: messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"image\", \"image\": os.path.join(assets_path, \"reasoning.png\")}, {\"type\": \"text\", \"text\": cot_prompt + \"In the rectangle $A B C D$ pictured, $M_{1}$ is the midpoint of $D C, M_{2}$ the midpoint of $A M_{1}, M_{3}$ the midpoint of $B M_{2}$ and $M_{4}$ the midpoint of $C M_{3}$. Determine the ratio of the area of the quadrilateral $M_{1} M_{2} M_{3} M_{4}$ to the area of the rectangle $A B C D$.\\nChoices:\\n(A) $\\frac{7}{16}$\\n(B) $\\frac{3}{16}$\\n(C) $\\frac{7}{32}$\\n(D) $\\frac{9}{32}$\\n(E) $\\frac{1}{5}$\"}, ], }, ] # Output: # \\\\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\\n\\\\n\\\\\\boxed{C}\\\\n\\n # video qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"video\", \"video\": os.path.join(assets_path, \"yoga.mp4\")}, {\"type\": \"text\", \"text\": \"What is the woman doing?\"}, ], }, ] # Output: # The image shows a woman performing a yoga pose on a rooftop. She's in a dynamic yoga pose, with her arms and legs extended in various positions. # multi-turn chat messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ\"}, ], }, { \"role\": \"ASSISTANT\", \"content\": [ {\"type\": \"text\", \"text\": \"åŒ—äº¬\"}, ], }, { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"å®ƒçš„å åœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿæœ‰å¤šå°‘å¸¸ä½äººå£ï¼Ÿ\"}, ], }, ] # Output: # åŒ—äº¬å¸‚çš„æ€»é¢ç§¯çº¦ä¸º16,410.54å¹³æ–¹å…¬é‡Œï¼Œå¸¸ä½äººå£çº¦ä¸º21,542,000äººã€‚ # Preparation for inference text = processor.apply_chat_template(messages, add_generation_prompt=True) image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages) inputs = processor( text=[text], images=image_inputs, videos=video_inputs, audios=audio_inputs, return_tensors=\"pt\", ) inputs = inputs.to(model.device) for k in inputs.keys(): if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\": inputs[k] = inputs[k].to(dtype=torch.bfloat16) # call generate generated_ids = model.generate( **inputs, max_new_tokens=512, use_cache=False, eos_token_id=processor.gen_terminator, ) generated_ids_trimmed = [ out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] output_text = processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False )[0] print(output_text) # ASR messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\"}, {\"type\": \"audio\", \"audio\": 'data/wavs/BAC009S0915W0292.wav'}, ], }, ] outputs = model.generate(messages, max_new_tokens=512) print(outputs) # speech2speech messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"audio\", \"audio\": 'data/wavs/BAC009S0915W0292.wav'}, ], }, ] outputs = model.generate(messages, max_new_tokens=512, speaker='luna', output_audio_path='out.wav', output_audio=True) print(outputs) License and Legal Disclaimer This code repository is licensed under the MIT License, and the Legal Disclaimer is located in the LEGAL.md file under the projectâ€™s root directory.\n","wordCount":"1105","inLanguage":"en","datePublished":"2025-05-05T00:00:03+08:00","dateModified":"2025-05-05T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-lite-omni-preview/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;Â»&nbsp;<a href=https://inclusionai.github.io/blog/>Blog</a></div><h1 class=post-title>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</h1><div class=post-meta><span class=post-date title="2025-05-05 00:00:03 +0800 +0800">May 5, 2025</span>
<span class=post-word-count>1105 words</span>
<span class=post-author>inclusionAI, Ant Group</span>
&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></header><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> ğŸ¤— <a href=https://huggingface.co/inclusionAI>Hugging Face</a> | ğŸ¤– <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Ming-Lite-Omni-Preview is built upon <a href=https://github.com/inclusionAI/Ling>Ling-Lite</a>, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.</p><h2 id=key-features>Key Features<a hidden class=anchor aria-hidden=true href=#key-features>#</a></h2><ul><li><p><strong>Omni and Novel MoE Architecture</strong>: An innovative Omni architecture based on Mixture of Experts (MoE) that achieves competive performance across multiple modality benchmarks.</p></li><li><p><strong>Video understanding</strong>: Supports KV-Cache dynamic compression of visual tokens. While supporting the ability to understand long videos of hours, it can also provide more detailed understanding of short videos of a few seconds.</p></li><li><p><strong>Natural Speech Generation and Fine-grained Voice Dialogue</strong>: Supports dialect understanding and generation in end-to-end conversations, enables one-shot voice cloning, and enhances prosody through audio tokenizer compression</p></li></ul><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><h3 id=image-benchmark>Image benchmark<a hidden class=anchor aria-hidden=true href=#image-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th><th style=text-align:center>InternVL2.5-8B-MPO</th></tr></thead><tbody><tr><td style=text-align:left>AI2D</td><td style=text-align:center>83.84</td><td style=text-align:center>83.9</td><td style=text-align:center><b>84.5</b></td></tr><tr><td style=text-align:left>HallusionBench</td><td style=text-align:center><b>54.68</b></td><td style=text-align:center>51.9</td><td style=text-align:center>51.7</td></tr><tr><td style=text-align:left>MMBench_TEST_V11</td><td style=text-align:center>79.63</td><td style=text-align:center><b>84.3</b></td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>MMMU</td><td style=text-align:center>57.0</td><td style=text-align:center><b>58.6</b></td><td style=text-align:center>54.8</td></tr><tr><td style=text-align:left>MMStar</td><td style=text-align:center>62.0</td><td style=text-align:center>63.9</td><td style=text-align:center><b>65.2</b></td></tr><tr><td style=text-align:left>MMVet</td><td style=text-align:center><b>73.6</b></td><td style=text-align:center>67.1</td><td style=text-align:center>68.1</td></tr><tr><td style=text-align:left>MathVista</td><td style=text-align:center><b>69.0</b></td><td style=text-align:center>68.2</td><td style=text-align:center>67.9</td></tr><tr><td style=text-align:left>OCRBench</td><td style=text-align:center>87.9</td><td style=text-align:center>86.4</td><td style=text-align:center><b>88.2</b></td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center><b>70.96</b></td><td style=text-align:center>70.5</td><td style=text-align:center>70.3</td></tr></tbody></table></div><h4 id=object-recognition>Object Recognition<a hidden class=anchor aria-hidden=true href=#object-recognition>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Object Recognition</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5-VL-7B</th><th style=text-align:center>InternVL-2.5-8B</th></tr></thead><tbody><tr><td style=text-align:left>Plants</td><td style=text-align:center>52.1</td><td style=text-align:center><b>55.3</b></td><td style=text-align:center>32.8</td></tr><tr><td style=text-align:left>Animals</td><td style=text-align:center>52.6</td><td style=text-align:center><b>54.8</b></td><td style=text-align:center>36.5</td></tr><tr><td style=text-align:left>Home appliances & furniture</td><td style=text-align:center>93.5</td><td style=text-align:center><b>97.4</b></td><td style=text-align:center>90.9</td></tr><tr><td style=text-align:left>Personal Electronics</td><td style=text-align:center><b>96.1</b></td><td style=text-align:center>95.1</td><td style=text-align:center>93.2</td></tr><tr><td style=text-align:left>Food & Ingredients</td><td style=text-align:center>57.5</td><td style=text-align:center><b>60.0</b></td><td style=text-align:center>48.7</td></tr><tr><td style=text-align:left>Tableware</td><td style=text-align:center><b>96.6</td><td style=text-align:center>94.9</td><td style=text-align:center>88.1</td></tr><tr><td style=text-align:left>Vehicles</td><td style=text-align:center>31.9</td><td style=text-align:center><b>40.9</b></td><td style=text-align:center>31.9</td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center>68.6</td><td style=text-align:center><b>71.2</b></td><td style=text-align:center>60.3</td></tr></tbody></table></div><h3 id=video-benchmark>Video benchmark<a hidden class=anchor aria-hidden=true href=#video-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5VL-7B</th></tr></thead><tbody><tr><td style=text-align:left>VideoMME wo/w sub.</td><td style=text-align:center>63.9/67.6</td><td style=text-align:center><b>65.1/71.6</b></td></tr><tr><td style=text-align:left>MVBench</td><td style=text-align:center>67.0</td><td style=text-align:center><b>72.0</b></td></tr><tr><td style=text-align:left>Video-MMMU</td><td style=text-align:center>45.4</td><td style=text-align:center><b>47.44</b></td></tr><tr><td style=text-align:left>LongVideoBench</td><td style=text-align:center>53.7</td><td style=text-align:center><b>60.0</b></td></tr></tbody></table></div><h3 id=audio-benchmark>Audio benchmark<a hidden class=anchor aria-hidden=true href=#audio-benchmark>#</a></h3><h4 id=speechqa>SpeechQA<a hidden class=anchor aria-hidden=true href=#speechqa>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>AlpacaEval</th><th style=text-align:center>CommonEval</th><th style=text-align:center>SD-QA</th><th style=text-align:center>MMSU</th><th style=text-align:center>OpenBookQA</th><th style=text-align:center>IFEval</th><th style=text-align:center>AdvBench</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2-Audio-chat</td><td style=text-align:center>3.69</td><td style=text-align:center>3.40</td><td style=text-align:center>35.35</td><td style=text-align:center>35.43</td><td style=text-align:center>49.01</td><td style=text-align:center>22.57</td><td style=text-align:center>98.85</td></tr><tr><td style=text-align:left>Baichuan-Audio</td><td style=text-align:center>4.00</td><td style=text-align:center>3.39</td><td style=text-align:center>49.64</td><td style=text-align:center>48.80</td><td style=text-align:center>63.30</td><td style=text-align:center>41.32</td><td style=text-align:center>86.73</td></tr><tr><td style=text-align:left>GLM-4-Voice</td><td style=text-align:center>4.06</td><td style=text-align:center>3.48</td><td style=text-align:center>43.31</td><td style=text-align:center>40.11</td><td style=text-align:center>52.97</td><td style=text-align:center>24.91</td><td style=text-align:center>88.08</td></tr><tr><td style=text-align:left>Kimi-Audio</td><td style=text-align:center>4.46</td><td style=text-align:center><b>3.97</b></td><td style=text-align:center><b>63.12</b></td><td style=text-align:center>62.17</td><td style=text-align:center><b>83.52</b></td><td style=text-align:center><b>61.10</b></td><td style=text-align:center><b>100.00</b></td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center><b>4.49</b></td><td style=text-align:center>3.93</td><td style=text-align:center>55.71</td><td style=text-align:center><b>61.32</b></td><td style=text-align:center>81.10</td><td style=text-align:center>52.87</td><td style=text-align:center>99.42</td></tr><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>4.25</td><td style=text-align:center>3.88</td><td style=text-align:center>58.95</td><td style=text-align:center>46.06</td><td style=text-align:center>60.00</td><td style=text-align:center>46.71</td><td style=text-align:center>96.53</td></tr></tbody></table></div><h4 id=asr>ASR<a hidden class=anchor aria-hidden=true href=#asr>#</a></h4><div align=center><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:center><strong>Aishell-1</strong></th><th style=text-align:center><strong>Aishell-2 ios</strong></th><th style=text-align:center><strong>Wenetspeech test-net</strong></th><th style=text-align:center><strong>Wenet test-meeting</strong></th><th style=text-align:center><strong>Librispeech test-clean</strong></th><th style=text-align:center><strong>Librispeech test-other</strong></th></tr></thead><tbody><tr><td style=text-align:left>Whisper Large-v3</td><td style=text-align:center>5.14</td><td style=text-align:center>4.76</td><td style=text-align:center>9.68</td><td style=text-align:center>18.54</td><td style=text-align:center>1.9</td><td style=text-align:center>3.65</td></tr><tr><td style=text-align:left>Qwen2-Audio</td><td style=text-align:center>1.53</td><td style=text-align:center>3.06</td><td style=text-align:center>7.72</td><td style=text-align:center>8.4</td><td style=text-align:center><b>1.6</b></td><td style=text-align:center>3.6</td></tr><tr><td style=text-align:left>GLM-4-voice Base</td><td style=text-align:center>2.46</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>2.82</td><td style=text-align:center>7.66</td></tr><tr><td style=text-align:left>Baichuan-Omni-1.5</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>6.9</td><td style=text-align:center>8.4</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center><b>1.18</b></td><td style=text-align:center><b>2.36</b></td><td style=text-align:center><b>5.9</b></td><td style=text-align:center>7.7</td><td style=text-align:center>1.8</td><td style=text-align:center><b>3.4</b></td></tr><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>1.62</td><td style=text-align:center>2.82</td><td style=text-align:center>6.23</td><td style=text-align:center><b>6.9</b></td><td style=text-align:center>2.34</td><td style=text-align:center>5.74</td></tr></tbody></table></div><h3 id=knowledge>Knowledge<a hidden class=anchor aria-hidden=true href=#knowledge>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>InfoSeek_H-mean</th><th style=text-align:center>InfoSeek_unseen_question</th><th style=text-align:center>InfoSeek_unseen_entity</th></tr></thead><tbody><tr><td style=text-align:left>GPT-4o</td><td style=text-align:center><b>36.05</b></td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>PaLI-X</td><td style=text-align:center>22.06</td><td style=text-align:center>23.5</td><td style=text-align:center>20.8</td></tr><tr><td style=text-align:left>Qwen2.5-vl-32B</td><td style=text-align:center>19.35</td><td style=text-align:center>20.55</td><td style=text-align:center>18.28</td></tr><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>27.3</td><td style=text-align:center>28.9</td><td style=text-align:center>25.9</td></tr></tbody></table></div><h3 id=ocrgui>OCR&amp;GUI<a hidden class=anchor aria-hidden=true href=#ocrgui>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>ChartQA_TEST</td><td style=text-align:center>85.2</td><td style=text-align:center><b>87.3</b></td></tr><tr><td style=text-align:left>DocVQA_TEST</td><td style=text-align:center>93.2</td><td style=text-align:center><b>95.7</b></td></tr><tr><td style=text-align:left>OCRBenchV2_en/zh</td><td style=text-align:center>52.2/51.6</td><td style=text-align:center><b>56.3/57.2</b></td></tr><tr><td style=text-align:left>OmniDocBenchâ†“</td><td style=text-align:center>34.7/34.5</td><td style=text-align:center><b>30.8/39.8</b></td></tr><tr><td style=text-align:left>TextVQA_VAL</td><td style=text-align:center>82.36</td><td style=text-align:center><b>84.9</b></td></tr><tr><td style=text-align:left>ScreenSpot</td><td style=text-align:center>79.3</td><td style=text-align:center><b>84.7</b></td></tr></tbody></table></div><h2 id=model-downloads>Model Downloads<a hidden class=anchor aria-hidden=true href=#model-downloads>#</a></h2><p>You can download the model from both Huggingface and ModelScope.</p><div align=center><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:center><strong>Input modality</strong></th><th style=text-align:center><strong>Oput modality</strong></th><th style=text-align:center><strong>Download</strong></th></tr></thead><tbody><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>Image,text,viedio,audio</td><td style=text-align:center>Image,text,audio</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni/tree/250504>ğŸ¤— HuggingFace</a><br><a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni/files?version=250504">ğŸ¤– ModelScope</a></td></tr></tbody></table></div>If you're in mainland China, we strongly recommend you to download our model from ğŸ¤– <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a>.<h2 id=use-cases>Use Cases<a hidden class=anchor aria-hidden=true href=#use-cases>#</a></h2><h3 id=video-audio-qa>Video-Audio-QA<a hidden class=anchor aria-hidden=true href=#video-audio-qa>#</a></h3><table><thead><tr><th>MultiModal Input</th><th>QA</th></tr></thead><tbody><tr><td><video src=https://github.com/user-attachments/assets/a1327779-030a-44d0-a073-bbc1abe04efc controls width=70% height=auto></video></td><td>Q: <a href=./figures/cases/audioqa_audio.wav>&lt;audio></a> (audio content: è¯·æè¿°è§†é¢‘å†…å®¹ã€‚)<br>A: The video features a woman performing a series of yoga poses on a rooftop with a scenic view of mountains and a clear blue sky.</td></tr><tr><td><video src=https://github.com/user-attachments/assets/bdeb43ce-9048-4dc1-897c-aa1d7b6f3836 controls width=70% height=auto></video></td><td>Q: Is there any food in front of me?<br>A: Yes, there&rsquo;s candy on the table.</td></tr></tbody></table><h3 id=speech2speech-supports-dialect>Speech2Speech (supports dialect)<a hidden class=anchor aria-hidden=true href=#speech2speech-supports-dialect>#</a></h3><p><video src=https://github.com/user-attachments/assets/842e3e18-ee4a-47ea-ba92-a009be5cf2a3 controls width=70% height=auto></video></p><h2 id=quickstart>Quickstart<a hidden class=anchor aria-hidden=true href=#quickstart>#</a></h2><p>Please download our model following <a href=/blog/ming-lite-omni-preview/#model-downloads>Model Downloads</a>, then you can refer to the following codes to run Ming-Lite-Omni-Preview model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoProcessor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>modeling_bailingmm</span> <span class=kn>import</span> <span class=n>BailingMMNativeForConditionalGeneration</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BailingMMNativeForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>low_cpu_mem_usage</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>assets_path</span> <span class=o>=</span> <span class=n>YOUR_ASSETS_PATH</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build processor</span>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;è¯·è¯¦ç»†ä»‹ç»é¹¦é¹‰çš„ç”Ÿæ´»ä¹ æ€§ã€‚&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># é¹¦é¹‰æ˜¯ä¸€ç§éå¸¸èªæ˜å’Œç¤¾äº¤æ€§å¼ºçš„é¸Ÿç±»ï¼Œå®ƒä»¬çš„ç”Ÿæ´»ä¹ æ€§éå¸¸ä¸°å¯Œå’Œæœ‰è¶£ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›å…³äºé¹¦é¹‰ç”Ÿæ´»ä¹ æ€§çš„è¯¦ç»†ä»‹ç»ï¼š</span>
</span></span><span class=line><span class=cl><span class=c1># ### 1. **æ –æ¯åœ°**</span>
</span></span><span class=line><span class=cl><span class=c1># é¹¦é¹‰ä¸»è¦åˆ†å¸ƒåœ¨çƒ­å¸¦å’Œäºšçƒ­å¸¦åœ°åŒºï¼ŒåŒ…æ‹¬éæ´²ã€äºšæ´²ã€æ¾³å¤§åˆ©äºšå’Œå—ç¾æ´²ã€‚å®ƒä»¬é€šå¸¸ç”Ÿæ´»åœ¨æ£®æ—ã€è‰åŸã€æ²™æ¼ å’ŒåŸå¸‚ç¯å¢ƒä¸­ã€‚ä¸åŒç§ç±»çš„é¹¦é¹‰å¯¹æ –æ¯åœ°çš„è¦æ±‚æœ‰æ‰€ä¸åŒï¼Œä½†å¤§å¤šæ•°é¹¦é¹‰å–œæ¬¢æœ‰ä¸°å¯Œæ¤è¢«å’Œæ°´æºçš„åœ°æ–¹ã€‚</span>
</span></span><span class=line><span class=cl><span class=c1># ### 2. **é¥®é£Ÿ**</span>
</span></span><span class=line><span class=cl><span class=c1># é¹¦é¹‰æ˜¯æ‚é£Ÿæ€§åŠ¨ç‰©ï¼Œå®ƒä»¬çš„é¥®é£Ÿéå¸¸å¤šæ ·åŒ–ã€‚å®ƒä»¬çš„é£Ÿç‰©åŒ…æ‹¬ç§å­ã€åšæœã€æ°´æœã€è”¬èœã€èŠ±èœœå’Œæ˜†è™«ã€‚é¹¦é¹‰çš„å–™éå¸¸å¼ºå£®ï¼Œèƒ½å¤Ÿè½»æ¾åœ°æ‰“å¼€åšç¡¬çš„æœå£³å’Œåšæœã€‚ä¸€äº›é¹¦é¹‰è¿˜ä¼šåƒæ³¥åœŸæˆ–æ²™å­ï¼Œä»¥å¸®åŠ©æ¶ˆåŒ–å’Œè¡¥å……çŸ¿ç‰©è´¨ã€‚</span>
</span></span><span class=line><span class=cl><span class=c1># ......</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># image qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span> <span class=s2>&#34;image&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>assets_path</span><span class=p>,</span> <span class=s2>&#34;flowers.jpg&#34;</span><span class=p>)},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What kind of flower is this?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white.</span>
</span></span></code></pre></div><p>To enable thinking before response, adding the following system prompt before your question:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>cot_prompt</span> <span class=o>=</span> <span class=s2>&#34;SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in &lt;thinking&gt;...&lt;/thinking&gt; tags, then the final answer enclosed in &lt;answer&gt;...&lt;/answer&gt; tags. The critical answer or key result should be placed within </span><span class=se>\\</span><span class=s2>boxed</span><span class=si>{}</span><span class=s2>.</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># And your input message should be like this:</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span> <span class=s2>&#34;image&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>assets_path</span><span class=p>,</span> <span class=s2>&#34;reasoning.png&#34;</span><span class=p>)},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=n>cot_prompt</span> <span class=o>+</span> <span class=s2>&#34;In the rectangle $A B C D$ pictured, $M_</span><span class=si>{1}</span><span class=s2>$ is the midpoint of $D C, M_</span><span class=si>{2}</span><span class=s2>$ the midpoint of $A M_</span><span class=si>{1}</span><span class=s2>, M_</span><span class=si>{3}</span><span class=s2>$ the midpoint of $B M_</span><span class=si>{2}</span><span class=s2>$ and $M_</span><span class=si>{4}</span><span class=s2>$ the midpoint of $C M_</span><span class=si>{3}</span><span class=s2>$. Determine the ratio of the area of the quadrilateral $M_</span><span class=si>{1}</span><span class=s2> M_</span><span class=si>{2}</span><span class=s2> M_</span><span class=si>{3}</span><span class=s2> M_</span><span class=si>{4}</span><span class=s2>$ to the area of the rectangle $A B C D$.</span><span class=se>\n</span><span class=s2>Choices:</span><span class=se>\n</span><span class=s2>(A) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{7}{16}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(B) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{3}{16}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(C) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{7}{32}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(D) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{9}{32}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(E) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{1}{5}</span><span class=s2>$&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl><span class=c1># \&lt;think\&gt;\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\n\&lt;/think\&gt;\n\&lt;answer\&gt;\\boxed{C}\&lt;/answer\&gt;\n\n</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># video qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;video&#34;</span><span class=p>,</span> <span class=s2>&#34;video&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>assets_path</span><span class=p>,</span> <span class=s2>&#34;yoga.mp4&#34;</span><span class=p>)},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What is the woman doing?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The image shows a woman performing a yoga pose on a rooftop. She&#39;s in a dynamic yoga pose, with her arms and legs extended in various positions.</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># multi-turn chat</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;ä¸­å›½çš„é¦–éƒ½æ˜¯å“ªé‡Œï¼Ÿ&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;ASSISTANT&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;åŒ—äº¬&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;å®ƒçš„å åœ°é¢ç§¯æ˜¯å¤šå°‘ï¼Ÿæœ‰å¤šå°‘å¸¸ä½äººå£ï¼Ÿ&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># åŒ—äº¬å¸‚çš„æ€»é¢ç§¯çº¦ä¸º16,410.54å¹³æ–¹å…¬é‡Œï¼Œå¸¸ä½äººå£çº¦ä¸º21,542,000äººã€‚</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Preparation for inference</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>image_inputs</span><span class=p>,</span> <span class=n>video_inputs</span><span class=p>,</span> <span class=n>audio_inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>process_vision_info</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>images</span><span class=o>=</span><span class=n>image_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>videos</span><span class=o>=</span><span class=n>video_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>audios</span><span class=o>=</span><span class=n>audio_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>inputs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values_videos&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;audio_feats&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># call generate</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>use_cache</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eos_token_id</span><span class=o>=</span><span class=n>processor</span><span class=o>.</span><span class=n>gen_terminator</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids_trimmed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>out_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>in_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>in_ids</span><span class=p>,</span> <span class=n>out_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>output_text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_ids_trimmed</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output_text</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ASR</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Please recognize the language of this speech and transcribe it. Format: oral.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio&#34;</span><span class=p>:</span> <span class=s1>&#39;data/wavs/BAC009S0915W0292.wav&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># speech2speech</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio&#34;</span><span class=p>:</span> <span class=s1>&#39;data/wavs/BAC009S0915W0292.wav&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>speaker</span><span class=o>=</span><span class=s1>&#39;luna&#39;</span><span class=p>,</span> <span class=n>output_audio_path</span><span class=o>=</span><span class=s1>&#39;out.wav&#39;</span><span class=p>,</span> <span class=n>output_audio</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=license-and-legal-disclaimer>License and Legal Disclaimer<a hidden class=anchor aria-hidden=true href=#license-and-legal-disclaimer>#</a></h2><p>This code repository is licensed under the <a href=../LICENSE>MIT License</a>, and the Legal Disclaimer is located in the <a href=../LEGAL.md>LEGAL.md file</a> under the project&rsquo;s root directory.</p></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
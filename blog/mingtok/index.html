<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer üöÄ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent space‚Äîeliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/mingtok/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/mingtok/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/mingtok/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer"><meta property="og:description" content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer üöÄ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent space‚Äîeliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/mingtok/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-10-01T00:00:03+08:00"><meta property="article:modified_time" content="2025-10-01T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer"><meta name=twitter:description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer üöÄ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent space‚Äîeliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer","item":"https://inclusionai.github.io/blog/mingtok/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer","name":"Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer","description":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer üöÄ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent space‚Äîeliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks.","keywords":[],"articleBody":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer üöÄ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent space‚Äîeliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks. Reduced Representational Competition ‚Üí 3.5√ó Faster Convergence: The unified continuous representation aligns semantic understanding and generative dynamics, significantly accelerating joint training without performance trade-offs. Multi-Round In-Context Learning in a Single Feature Space: All operations‚Äîunderstanding, generation, and editing‚Äîoccur in the same continuous space, eliminating costly cross-space conversions and enabling simpler, more efficient training and inference. The Challenge: The Inverse Nature of Seeing and Drawing Autoregression‚Äîthe powerful paradigm of modeling the world by ‚Äúpredicting the next token‚Äù‚Äîhas already unified diverse modalities like language and audio. The next frontier is to bring visual understanding (seeing) and visual generation (drawing) into this unified sequence‚Äëto‚Äësequence framework.\nHowever, this ambition encounters a deep challenge: in many respects, understanding and generation are inverse tasks.\nUnderstanding: Pixels ‚Üí high‚Äëdimensional, abstract semantic concepts Generation: Concepts ‚Üí fine‚Äëgrained, high‚Äëfidelity pixels These tasks have drastically different‚Äîand often competing‚Äîpreferences for their underlying visual representation.\nWhy Previous Approaches Fell Short Existing models attempt unification via two limited strategies:\nAsymmetric Designs: Use different, heterogeneous feature spaces for each task. During multi‚Äëturn interactions, this forces inefficient ‚Äúround‚Äëtrips‚Äù between spaces, causing latency and complexity. Shared Discrete Tokens: Unify the token space but introduce quantization errors. This hurts image fidelity and degrades understanding capability. Our Solution: Ming-UniVision and MingTok To break this impasse, we introduce Ming-UniVision, a new generation of autoregressive vision‚Äëlanguage model built on a foundational innovation: MingTok.\nMingTok is the first visual tokenizer based on a continuous latent space. It delivers a truly unified and efficient representation that serves as the bedrock for Ming‚ÄëUniVision‚Äôs unified NTP (Next‚ÄëToken Prediction) framework‚Äîharmonizing image understanding, generation, and editing in one in‚Äëcontext multimodal loop.\nThe Core Design: A Three-Stage Architecture to Reconcile Competition At the heart of Ming-UniVision is the MingTok tokenizer, a three-stage sequential architecture elegantly designed to reconcile the competing representational demands of understanding and generation within a single framework.\nFigure 1: (a) Existing models use separate visual representations. (b) MingTok, the engine of Ming-UniVision, uses a unified scheme for both semantic and generative representations. (c) This unified approach leads to over 3.5x faster training convergence.\nLow-level Encoder: Maps an input image into a sequence of compact, continuous latent codes, optimized for high-quality and efficient autoregressive generation. Semantic Decoder: Autoregressively ‚Äúrefines‚Äù the compact latent codes into high-dimensional, rich semantic features aligned with top-tier understanding models like CLIP. Pixel Decoder: Serves as a quality-assurance module, ensuring the original image can be reconstructed with high fidelity, guaranteeing a high-fidelity representation process. The Key Innovation: MingTok creates a unified, differentiable interface. The high-level features for understanding can be directly fed as conditional input for the next round of generation or editing. This completely eliminates the costly detour through pixel space.\nThe Breakthrough: A Fundamental Leap in Efficiency By integrating MingTok, Ming-UniVision achieves competitive results on both understanding and generation tasks. The shared continuous latent space unlocks two fundamental layers of efficiency, resolving bottlenecks that have plagued previous architectures.\nFigure 2: On general recognition tasks, our method approaches the performance of models with separated representations and significantly outperforms other unified representation models. For generation, our model shows a clear advantage on fine-grained tasks.\n1. A Revolution in Training: \u003e3.5x Faster Convergence Traditional approaches expend massive resources aligning heterogeneous representations, creating an intrinsic ‚Äútask competition‚Äù that slows learning. MingTok solves this at its root.\nSynergistic Enhancement: Our ablation studies show that using MingTok for both tasks fosters a synergy where understanding and generation capabilities enhance each other, rather than competing. \u003e3.5x Speedup: By avoiding inefficient alignment, the model focuses its energy on learning, reaching the same performance level in a fraction of the time compared to traditional schemes. Figure 3: The performance drop between generation-only training and joint training is minimal with MingTok, proving the advantage of our unified approach.\n2. A Revolution in Interaction: Goodbye to the ‚ÄúPixel Round-Trip‚Äù The efficiency of multi-turn interactions (e.g., generate ‚Üí edit ‚Üí re-generate) depends on the ‚Äúunderstanding-generation‚Äù loop. This is precisely where traditional architectures falter.\nArchitecture Type Multi-turn Capability Core Bottleneck Interaction Path Efficiency \u0026 Fidelity DiT-based Models ‚ùå Not Natively Supported Non-autoregressive, stateless N/A (Full process restart) Low Hybrid Architectures ‚ö†Ô∏è Supported, but Inefficient Dual-branch, un-unified spaces Latent ‚Üí Pixel ‚Üí Feature Low, complex, lossy Unified AR ‚ö†Ô∏è Supported, but Inefficient Heterogeneous spaces Latent ‚Üí Pixel ‚Üí Feature Low, lossy Ming-UniVision ‚úÖ Native \u0026 Highly Efficient Unified Continuous Space Feature ‚Üí Feature High \u0026 High-Fidelity As the table shows, any architecture with separated spaces is doomed to the inefficient Latent ‚Üí Pixel ‚Üí Feature round-trip. This ‚Äúpixel detour‚Äù introduces massive latency and causes contextual information to decay.\nMing-UniVision achieves a direct Feature ‚Üí Feature closed loop. High-level features from an understanding task can be directly consumed by the next generation task, unlocking truly coherent multimodal sequence modeling. This enables tasks that once required multiple specialized models to emerge naturally within a single, unified framework:\nIterative Image Enhancement: Perform super-resolution, then directly continue with colorization or denoising. Generative Chain-of-Thought: Perform an understanding task (e.g., ‚Äúsegment the car‚Äù), then directly apply an editing command to that region. Figure 4: Multi-turn tasks like ‚ÄúSuper-resolution ‚Üí Colorization‚Äù and ‚ÄúSegmentation ‚Üí Editing‚Äù are now part of a seamless flow.\nUnderstanding, generation, and editing are no longer isolated pipelines but are woven into a continuous visual conversation.\nConclusion and The Road Ahead We believe that a unified and continuous visual representation like MingTok opens up new possibilities for building more flexible and intuitive multimodal interactive systems.\nWe know this is just one step in a long journey. We have open-sourced our code and initial model weights, hoping to provide a useful foundation for the community and to inspire more discussion around unified representations. We look forward to collaborating with our peers to collectively advance the future of multimodal AI.\nGet Involved GitHub: [Link to your repo] Technical Report: [Link to your paper] Online Demo: [Link to your demo] Try out our open-source model Ming-UniVision and MingTok-Vision on our GitHub Page / Demo Page. Please star our repo if you like it!\n","wordCount":"1067","inLanguage":"en","datePublished":"2025-10-01T00:00:03+08:00","dateModified":"2025-10-01T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/mingtok/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer</h1><div class=post-meta><span title='2025-10-01 00:00:03 +0800 +0800'>October 1, 2025</span>&nbsp;¬∑&nbsp;6 min&nbsp;¬∑&nbsp;1067 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/mingtok/>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> ü§ó <a href=https://huggingface.co/inclusionAI/Ming-UniVision>Hugging Face</a>ÔΩú ü§ñ <a href=https://www.modelscope.cn/models/inclusionAI/Ming-UniVision>ModelScope</a>
<video src=https://gw.alipayobjects.com/v/huamei_qlf8jc/afts/video/A*ZBkgTruOxA4AAAAAgyAAAAgAehi-AQ width=1024px height=660px controls autoplay muted playsinline></video></p><h1 id=ming-univision-joint-image-understanding-and-generation-via-a-unified-continuous-tokenizer>Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer<a hidden class=anchor aria-hidden=true href=#ming-univision-joint-image-understanding-and-generation-via-a-unified-continuous-tokenizer>#</a></h1><h2 id=-technical-highlights>üöÄ Technical Highlights<a hidden class=anchor aria-hidden=true href=#-technical-highlights>#</a></h2><ol><li><strong>First Continuous Unified Tokenizer for Vision:</strong> <strong>MingTok</strong> seamlessly supports both image understanding and generation within a single continuous latent space‚Äîeliminating quantization and bridging modalities.</li><li><strong>First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens:</strong> By building on MingTok, <strong>Ming-UniVision</strong> unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks.</li><li><strong>Reduced Representational Competition ‚Üí 3.5√ó Faster Convergence:</strong> The unified continuous representation aligns semantic understanding and generative dynamics, significantly accelerating joint training without performance trade-offs.</li><li><strong>Multi-Round In-Context Learning in a Single Feature Space:</strong> All operations‚Äîunderstanding, generation, and editing‚Äîoccur in the same continuous space, eliminating costly cross-space conversions and enabling simpler, more efficient training and inference.</li></ol><h2 id=the-challenge-the-inverse-nature-of-seeing-and-drawing>The Challenge: The Inverse Nature of Seeing and Drawing<a hidden class=anchor aria-hidden=true href=#the-challenge-the-inverse-nature-of-seeing-and-drawing>#</a></h2><p>Autoregression‚Äîthe powerful paradigm of modeling the world by ‚Äúpredicting the next token‚Äù‚Äîhas already unified diverse modalities like language and audio. The next frontier is to bring visual understanding (seeing) and visual generation (drawing) into this unified sequence‚Äëto‚Äësequence framework.</p><p>However, this ambition encounters a deep challenge: in many respects, understanding and generation are inverse tasks.</p><ul><li><strong>Understanding:</strong> Pixels ‚Üí high‚Äëdimensional, abstract semantic concepts</li><li><strong>Generation:</strong> Concepts ‚Üí fine‚Äëgrained, high‚Äëfidelity pixels</li></ul><p>These tasks have drastically different‚Äîand often competing‚Äîpreferences for their underlying visual representation.</p><h3 id=why-previous-approaches-fell-short>Why Previous Approaches Fell Short<a hidden class=anchor aria-hidden=true href=#why-previous-approaches-fell-short>#</a></h3><p>Existing models attempt unification via two limited strategies:</p><ol><li><strong>Asymmetric Designs:</strong> Use different, heterogeneous feature spaces for each task. During multi‚Äëturn interactions, this forces inefficient ‚Äúround‚Äëtrips‚Äù between spaces, causing latency and complexity.</li><li><strong>Shared Discrete Tokens:</strong> Unify the token space but introduce quantization errors. This hurts image fidelity and degrades understanding capability.</li></ol><h3 id=our-solution-ming-univision-and-mingtok>Our Solution: Ming-UniVision and MingTok<a hidden class=anchor aria-hidden=true href=#our-solution-ming-univision-and-mingtok>#</a></h3><p>To break this impasse, we introduce <strong>Ming-UniVision</strong>, a new generation of autoregressive vision‚Äëlanguage model built on a foundational innovation: <strong>MingTok</strong>.</p><p><strong>MingTok</strong> is the first visual tokenizer based on a continuous latent space. It delivers a truly unified and efficient representation that serves as the bedrock for Ming‚ÄëUniVision‚Äôs unified NTP (Next‚ÄëToken Prediction) framework‚Äîharmonizing image understanding, generation, and editing in one in‚Äëcontext multimodal loop.</p><h2 id=the-core-design-a-three-stage-architecture-to-reconcile-competition>The Core Design: A Three-Stage Architecture to Reconcile Competition<a hidden class=anchor aria-hidden=true href=#the-core-design-a-three-stage-architecture-to-reconcile-competition>#</a></h2><p>At the heart of Ming-UniVision is the <strong>MingTok</strong> tokenizer, a three-stage sequential architecture elegantly designed to reconcile the competing representational demands of understanding and generation within a single framework.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*VVx0SJQR5K4AAAAARBAAAAgAehi-AQ/original alt="Figure 1: Architecture Comparison">
<em>Figure 1: (a) Existing models use separate visual representations. (b) MingTok, the engine of Ming-UniVision, uses a unified scheme for both semantic and generative representations. (c) This unified approach leads to over 3.5x faster training convergence.</em></p><ol><li><strong>Low-level Encoder:</strong> Maps an input image into a sequence of compact, continuous latent codes, optimized for high-quality and efficient autoregressive generation.</li><li><strong>Semantic Decoder:</strong> Autoregressively &ldquo;refines&rdquo; the compact latent codes into high-dimensional, rich semantic features aligned with top-tier understanding models like CLIP.</li><li><strong>Pixel Decoder:</strong> Serves as a quality-assurance module, ensuring the original image can be reconstructed with high fidelity, guaranteeing a high-fidelity representation process.</li></ol><blockquote><p><strong>The Key Innovation:</strong> MingTok creates a unified, differentiable interface. The high-level features for understanding can be directly fed as conditional input for the next round of generation or editing. This <strong>completely eliminates the costly detour through pixel space.</strong></p></blockquote><h2 id=the-breakthrough-a-fundamental-leap-in-efficiency>The Breakthrough: A Fundamental Leap in Efficiency<a hidden class=anchor aria-hidden=true href=#the-breakthrough-a-fundamental-leap-in-efficiency>#</a></h2><p>By integrating MingTok, Ming-UniVision achieves competitive results on both understanding and generation tasks. The shared continuous latent space unlocks two fundamental layers of efficiency, resolving bottlenecks that have plagued previous architectures.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*oi4-RqyoAvIAAAAARPAAAAgAehi-AQ/original alt="Figure 2: Benchmark Results">
<em>Figure 2: On general recognition tasks, our method approaches the performance of models with separated representations and significantly outperforms other unified representation models. For generation, our model shows a clear advantage on fine-grained tasks.</em></p><h3 id=1-a-revolution-in-training-35x-faster-convergence>1. A Revolution in Training: >3.5x Faster Convergence<a hidden class=anchor aria-hidden=true href=#1-a-revolution-in-training-35x-faster-convergence>#</a></h3><p>Traditional approaches expend massive resources aligning heterogeneous representations, creating an intrinsic &ldquo;task competition&rdquo; that slows learning. MingTok solves this at its root.</p><ul><li><strong>Synergistic Enhancement:</strong> Our ablation studies show that using MingTok for both tasks fosters a synergy where understanding and generation capabilities enhance each other, rather than competing.</li><li><strong>>3.5x Speedup:</strong> By avoiding inefficient alignment, the model focuses its energy on learning, reaching the same performance level in a fraction of the time compared to traditional schemes.</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*dkPxS4hNZx8AAAAARAAAAAgAehi-AQ/original alt="Figure 3: Pre-training Performance">
<em>Figure 3: The performance drop between generation-only training and joint training is minimal with MingTok, proving the advantage of our unified approach.</em></p><h3 id=2-a-revolution-in-interaction-goodbye-to-the-pixel-round-trip>2. A Revolution in Interaction: Goodbye to the &ldquo;Pixel Round-Trip&rdquo;<a hidden class=anchor aria-hidden=true href=#2-a-revolution-in-interaction-goodbye-to-the-pixel-round-trip>#</a></h3><p>The efficiency of multi-turn interactions (e.g., <em>generate ‚Üí edit ‚Üí re-generate</em>) depends on the &ldquo;understanding-generation&rdquo; loop. This is precisely where traditional architectures falter.</p><table><thead><tr><th style=text-align:left>Architecture Type</th><th style=text-align:left>Multi-turn Capability</th><th style=text-align:left>Core Bottleneck</th><th style=text-align:left>Interaction Path</th><th style=text-align:left>Efficiency & Fidelity</th></tr></thead><tbody><tr><td style=text-align:left>DiT-based Models</td><td style=text-align:left>‚ùå Not Natively Supported</td><td style=text-align:left>Non-autoregressive, stateless</td><td style=text-align:left>N/A (Full process restart)</td><td style=text-align:left>Low</td></tr><tr><td style=text-align:left>Hybrid Architectures</td><td style=text-align:left>‚ö†Ô∏è Supported, but Inefficient</td><td style=text-align:left>Dual-branch, un-unified spaces</td><td style=text-align:left><code>Latent ‚Üí Pixel ‚Üí Feature</code></td><td style=text-align:left>Low, complex, lossy</td></tr><tr><td style=text-align:left>Unified AR</td><td style=text-align:left>‚ö†Ô∏è Supported, but Inefficient</td><td style=text-align:left>Heterogeneous spaces</td><td style=text-align:left><code>Latent ‚Üí Pixel ‚Üí Feature</code></td><td style=text-align:left>Low, lossy</td></tr><tr><td style=text-align:left><strong>Ming-UniVision</strong></td><td style=text-align:left>‚úÖ <strong>Native & Highly Efficient</strong></td><td style=text-align:left><strong>Unified Continuous Space</strong></td><td style=text-align:left><strong><code>Feature ‚Üí Feature</code></strong></td><td style=text-align:left><strong>High & High-Fidelity</strong></td></tr></tbody></table><p>As the table shows, any architecture with separated spaces is doomed to the inefficient <code>Latent ‚Üí Pixel ‚Üí Feature</code> round-trip. This &ldquo;pixel detour&rdquo; introduces massive latency and causes contextual information to decay.</p><p><strong>Ming-UniVision</strong> achieves a direct <strong><code>Feature ‚Üí Feature</code> closed loop</strong>. High-level features from an understanding task can be directly consumed by the next generation task, unlocking truly coherent multimodal sequence modeling. This enables tasks that once required multiple specialized models to emerge naturally within a single, unified framework:</p><ul><li><strong>Iterative Image Enhancement:</strong> Perform super-resolution, then directly continue with colorization or denoising.</li><li><strong>Generative Chain-of-Thought:</strong> Perform an understanding task (e.g., &ldquo;segment the car&rdquo;), then directly apply an editing command to that region.</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*B3ckSaNK1cMAAAAARzAAAAgAehi-AQ/original alt="Figure 4: Multi-turn Interaction Demo">
<em>Figure 4: Multi-turn tasks like &ldquo;Super-resolution ‚Üí Colorization&rdquo; and &ldquo;Segmentation ‚Üí Editing&rdquo; are now part of a seamless flow.</em></p><p>Understanding, generation, and editing are no longer isolated pipelines but are woven into a <strong>continuous visual conversation.</strong></p><hr><h2 id=conclusion-and-the-road-ahead>Conclusion and The Road Ahead<a hidden class=anchor aria-hidden=true href=#conclusion-and-the-road-ahead>#</a></h2><p>We believe that a unified and continuous visual representation like MingTok opens up new possibilities for building more flexible and intuitive multimodal interactive systems.</p><p>We know this is just one step in a long journey. We have open-sourced our code and initial model weights, hoping to provide a useful foundation for the community and to inspire more discussion around unified representations. We look forward to collaborating with our peers to collectively advance the future of multimodal AI.</p><h3 id=get-involved>Get Involved<a hidden class=anchor aria-hidden=true href=#get-involved>#</a></h3><ul><li><strong>GitHub:</strong> [Link to your repo]</li><li><strong>Technical Report:</strong> [Link to your paper]</li><li><strong>Online Demo:</strong> [Link to your demo]</li></ul><p>Try out our open-source model <strong>Ming-UniVision and MingTok-Vision</strong> on our <a href=https://github.com/inclusionAI/Ming/blob/main/cookbook.ipynb><strong>GitHub Page / Demo Page</strong></a>. Please star our repo if you like it!</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
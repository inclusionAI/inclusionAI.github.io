<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#FFCC99 0%,#7F667F 40%,#262656 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/>ç®€ä½“ä¸­æ–‡</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Using Ring 1T with Claude Code via ZenMux</h2></header><div class=entry-content><p>Using Ring 1T with Claude Code via ZenMux What is Ring 1T? Ring 1T is a powerful open-source reasoning model designed for complex problem-solving and advanced coding tasks. Itâ€™s built on the Ling 2.0 architecture with impressive capabilities:
Scale: 1 trillion total parameters with 50 billion activated parameters Context: Supports up to 128K tokens context window Training: Enhanced through large-scale verifiable reward reinforcement learning (RLVR) Strengths: Excels at deep reasoning, natural language inference, and sophisticated code generation Ring 1T represents the latest advancement in MoE (Mixture of Experts) architecture scaling, leveraging the icepop reinforcement learning stabilization method and the ASystem framework to deliver exceptional reasoning performance.
...</p></div><footer class=entry-footer>&lt;span title='2026-02-15 00:00:03 +0800 +0800'>February 15, 2026&lt;/span>&amp;nbsp;Â·&amp;nbsp;16 min&amp;nbsp;Â·&amp;nbsp;3353 words&amp;nbsp;Â·&amp;nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Using Ring 1T with Claude Code via ZenMux" href=https://inclusionai.github.io/blog/using-ring-1t-with-claude-code-via-zenmux/></a></article><article class=post-entry><header class=entry-header><h2>Ming-flash-omni-Preview: A Sparse, Unified Architecture for Multimodal Perception and Generation</h2></header><div class=entry-content><p>GITHUB ARXIV ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Omnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0â€™s sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition.
...</p></div><footer class=entry-footer>&lt;span title='2025-10-28 00:00:03 +0800 +0800'>October 28, 2025&lt;/span>&amp;nbsp;Â·&amp;nbsp;5 min&amp;nbsp;Â·&amp;nbsp;1063 words&amp;nbsp;Â·&amp;nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-flash-omni-Preview: A Sparse, Unified Architecture for Multimodal Perception and Generation" href=https://inclusionai.github.io/blog/ming-flash-omni-preview/></a></article><article class=post-entry><header class=entry-header><h2>Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</h2></header><div class=entry-content><p>GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
The Introduction Video of Ming-UniAudio Audio Edit Demo Editing Tasks Video demos ğŸš€ Technical Highlights First unified continuous speech tokenizer for both understanding and generation tasks: MingTok-Audio is a unified continuous speech tokenizer MingTok-Audio based on a VAE framework with a causal Transformer architecture, the first continuous speech tokenizer to effectively integrate semantic and acoustic features, and enables a closed-loop system with LLMs through hierarchical feature representations, makes it suitable for both understanding and generation tasks. First Speech LLM with unifed continuous tokenizer for both understanding and generation: Ming-UniAudio is an end-to-end unified speech language model with a single LLM backbone for both understanding and generation tasks, enhanced with a Diffusion Head to ensure high-fidelity speech synthesis. First universal free-form speech editing model for semantic and acoustic tasks without temporal regime: We introduce the first instruction-guided, free-form speech editing framework that supports comprehensive semantic and acoustic edits without requiring explicit edit regions, along with Ming-Freeform-Audio-Edit, the first open-source evaluation set for such tasks. First benchmark for free-form speech editing: We propose Audio-Edit-Benchmark, the first open-source free-form evaluation set comprising editing tasks of four semantic and five acoustic types, to evaluate the modelâ€™s editing performance. Instruction-Guided Free-Form Speech Editing Semantic Editing - Insert Instruction Transcription Target Transcription Before Edit Speechedit Result insert â€˜ç®€ç›´â€™ after the character or word at index 8. çœŸæ˜¯ä¸ªæµªæ¼«çš„é‚‚é€…å¯ä»¥è¯´æ˜¯è‹±é›„æ•‘ç¾äº† çœŸæ˜¯ä¸ªæµªæ¼«çš„é‚‚é€…ç®€ç›´å¯ä»¥è¯´æ˜¯è‹±é›„æ•‘ç¾äº† insert â€˜çœŸæ­£â€™ before the character or word â€˜å¥½â€™. å°±æœ‰é“è€Œæ­£ç„‰å¯è°“å¥½å­¦ä¹Ÿå·² å°±æœ‰é“è€Œæ­£ç„‰å¯è°“çœŸæ­£å¥½å­¦ä¹Ÿå·² insert â€˜clearlyâ€™ before the character or word at index 8. Its legal status in Trinidad was insufficient to preserve its ecological status. Its legal status in Trinidad was insufficient clearly to preserve its ecological status. insert â€˜successfullyâ€™ after the character or word â€˜professionâ€™. Previously an attorney Korona left the profession to pursue a career in music. Previously an attorney Korona left the profession successfully to pursue a career in music. Semantic Editing - Substitute Instruction Transcription Target Transcription Before Edit Speechedit Result substitute â€˜å¦ˆå¦ˆâ€™ with â€˜çˆ¸çˆ¸â€™. æˆ‘æƒ³å¯¹äºå¦ˆå¦ˆæ¥è¯´ä¼šæ¯”ä»»ä½•ç¤¼ç‰©éƒ½è¦æ¸©æš– æˆ‘æƒ³å¯¹äºçˆ¸çˆ¸æ¥è¯´ä¼šæ¯”ä»»ä½•ç¤¼ç‰©éƒ½è¦æ¸©æš– substitute the characters or words from index 8 to index 10 with â€˜äº”ä¸‡å…ƒâ€™. å½“æ—¶æˆ‘æƒ³ç­‰ç­¹é½ä¸¤ä¸‡å…ƒè˜ç¤¼å°±é€å¥¹å¦ˆå›å®¶ å½“æ—¶æˆ‘æƒ³ç­‰ç­¹é½äº”ä¸‡å…ƒè˜ç¤¼å°±é€å¥¹å¦ˆå›å®¶ substitute â€˜get pictures offâ€™ with â€™transfer photos fromâ€™. Iâ€™m trying to explain to my mother how to get pictures off her phone. Iâ€™m trying to explain to my mother how to transfer photos from her phone. substitute the words from index 8 to index 9 with â€˜could becomeâ€™. Considering the growth of human population insects might be the food of the future. Considering the growth of human population insects could become the food of the future. Semantic Editing - Delete Instruction Transcription Target Transcription Before Edit Speechedit Result delete â€˜æ¯”æ™®é€šçš„èŒ¶å¶è¦â€™. èŠ±è‰èŒ¶çš„å£å‘³ä¸€èˆ¬æ¯”æ™®é€šçš„èŒ¶å¶è¦è‹¦ä¸€äº› èŠ±è‰èŒ¶çš„å£å‘³ä¸€èˆ¬è‹¦ä¸€äº› delete the characters or words from index 11 to index 15. æˆ‘åƒäº†ç‚¹ç‡•éº¦ç‰‡ç…é¸¡è›‹è¿˜å–äº†ç‚¹æ©™æ± æˆ‘åƒäº†ç‚¹ç‡•éº¦ç‰‡ç…é¸¡è›‹æ± delete â€™timesâ€™. The classification of this gibbon has changed several times in the past few years. The classification of this gibbon has changed several in the past few years. delete the characters or words from index 2 to index 6. On the second day the boy climbed to the top of a cliff near the camp On climbed to the top of a cliff near the camp Acoustic Editing - Dialect Conversion Instruction Transcription Before Edit Speechedit Result Change the accent of the speech to Dongbei. ä¹‹åï¼Œä»–è€ƒå–å¯¼æ¸¸è¯ï¼Œæˆä¸ºæ‹±åŒ—å£å²¸ä¸­æ—…çš„å¯¼æ¸¸ã€‚ Change the accent of the speech to Chengdu. åªæœ‰å½“ç§‘æŠ€ä¸ºæœ¬åœ°ç¤¾ç¾¤åˆ›é€ ä»·å€¼çš„æ—¶å€™ï¼Œæ‰èƒ½çœŸæ­£æœ‰æ„ä¹‰ã€‚ Change the accent of the speech to Chengdu. æˆ‘å¾—ç”¨å›æƒ³ä¸å¹»æƒ³è¡¥å……æˆ‘æ‰€ç¼ºå°‘çš„é¥®é£Ÿï¼Œå®‰æ…°æˆ‘æ‰€å¾—åˆ°çš„ç—›è‹¦ã€‚ Change the accent of the speech to Guangxi. å…¨å›½æ¶æ€§è‚¿ç˜¤å‘ç—…ï¼ŒåŠæ­»äº¡ç¬¬ä¸€ä½çš„æ˜¯è‚ºç™Œã€‚ Acoustic Editing - Speed Instruction Transcription Before Edit Speechedit Result adjusts the speed to 0.5. æˆ‘ç”¨èƒ¸æŠµä½è½¦æŠŠï¼ŒæŒæ¡æ–¹å‘ï¼Œé€Ÿåº¦ä¸€ç‚¹ä¹Ÿä¸æ¯”åˆ«äººæ…¢ã€‚ adjusts the speed to 0.7. There is a growing body of case law on Bayh-Dole. adjusts the speed to 1.3. Cribb was born near Bristol but moved to London before starting professional fighting. adjusts the speed to 2. åˆ‡å®å¸®åŠ©å›°éš¾ç¾¤ä¼—è§£å†³ç”Ÿäº§ç”Ÿæ´»ä¸­ï¼Œé‡åˆ°çš„å›°éš¾å’Œé—®é¢˜ã€‚ Acoustic Editing - Pitch Instruction Transcription Before Edit Speechedit Result shifts the pitch by 3 steps. å› ä¸ºå¤–é¢æœ‰æˆ˜äº‰ï¼Œå®¶é‡Œåˆæœ‰æˆ˜äº‰å¸¦æ¥çš„æ‚²ä¼¤å’ŒåŒ®ä¹ã€‚ shifts the pitch by 5 steps. è‡ªåŠ¨é©¾é©¶å°†å¤§å¹…æå‡å‡ºè¡Œå®‰å…¨ï¼Œæ•ˆç‡ã€‚ shifts the pitch by -1 steps. The heart of the campus has a number of historic buildings. shifts the pitch by -1 steps. Stevenson is also the director of music ministries at Angeles Mesa Presbyterian Church. Acoustic Editing - Volume Instruction Transcription Before Edit Speechedit Result adjusts the volume to 1.4. A woman sits as she shows the designs she has made in the floor. adjusts the volume to 1.6. For example, they both consist of predominately older, hence redder, stars. adjusts the volume to 0.9. ä¼ç¾²çš„å„¿å­™ä»¬çœ‹è§ä¼ç¾²æ‰æ¥äº†é±¼ï¼Œä¹Ÿéƒ½æ¬¢æ¬¢å–œå–œè·‘æ¥é—®é•¿é—®çŸ­ã€‚ adjusts the volume to 0.3. ä»–ä»¬è¿˜å‘Šè¯‰å·¨äººï¼Œé‚£åº§åŸå¸‚é‡Œç¾¤è‹±èŸèƒã€‚ Acoustic Editing - Denoise Instruction Transcription Before Edit Speechedit Result denoise the audio. Be shape of example,before deriving this formula we explained what we mean by problems of this kind we now generalize these ideas for general binomial experiments. denoise the audio. Summoned to himself with firmness no surrender his superiors had also preached this saying it was the way of eternal honor his comrades were old. denoise the audio. There are people who travel long distances to assure my continued existence we have also seen the power of faith at work among us it was muscular but it wasnâ€™t symmetrical. denoise the audio. Theory eventually proved inexact the heavens refused to give up their weeping but what has been happening recently might be described as creeping mannerism clever. Acoustic Editing - Background Music Instruction Before Edit Speechedit Result add rain to audio. add car sound to audio. add carefree music to audio. add groovy music to audio. Acoustic Editing - Emotion Conversion Instruction Transcription Before Edit Speechedit Result change the emotion to happy mood. æ¯”å°”æƒ³å†çœ‹å°ä¸»äººä¸€çœ¼ç„¶åèµ°è¿›æ£®æ—å®‰é™åœ°æ­»å»ã€‚ change the emotion to happy mood. ä¸–ç•Œçˆ±çœ¼æ—¥æ˜¯æ¯å¹´åæœˆçš„ç¬¬äºŒä¸ªæ˜ŸæœŸå››ã€‚ change the emotion to happy mood. æˆ‘ä¼šç©å¾ˆå¤šæ¸¸æˆå‘¢å¬è¯´å¤šå–æ°´èƒ½æ²»ç™¾ç—…ã€‚ change the emotion to happy mood. å»ºè®®æˆ´å£ç½©ç©ºæ°”è´¨é‡è½»åº¦æ±¡æŸ“ã€‚ Audio Understanding Chinese and English ASR Input Transcription å‘ƒå¾ˆä¹…æ²¡æœ‰çœ‹åˆ°çœ‹è¿‡å¦‚æ­¤ä¸å¸¦ä»·å€¼åˆ¤æ–­çš„ç”µå½± æ¡ƒèŠ±åº„äººå¡”ä¿±ä¹éƒ¨æ˜¯ä½äºæ­å·å¸‚å¾·æ¸…å¿çš„ä¸€ä¸ªä¿±ä¹éƒ¨ he was excited and at the same time uneasy maybe the girl had already forgotten him itâ€™s true that everything has its destiny but one day that destiny will be realized Dialect Understanding Input Transcription [æ–¹è¨€-ç²¤è¯­] ä½ åšä¹œå˜¢å•Šç³»å’ªå””æƒ³å€¾åˆå•Šã€‚ [æ–¹è¨€-ä¸Šæµ·è¯] é˜¿æ‹‰è€ƒè¯•è¿˜æ²¡å®šä¸‹æ¥å”»ã€‚ [æ–¹è¨€-é—½å—è¯­] å®è´è¾ƒæ—©ä¼‘å›°æ™šå®‰ã€‚ [æ–¹è¨€-å·æ¸æ–¹è¨€] æˆ‘éš¾å—å¾—å¾ˆåˆ«ä¸ªéƒ½ç¡äº†ã€‚ Context ASR Input Prompt Transcription Please recognize the language of this speech and transcribe it. Format: oral. This is an audio about Banking. This audio may contains the following words or phrases:Zelle,daily A C H transfer limit,cashierâ€™s checks,transaction memos,F D I C regulations,cryptocurrency wallet,K Y C requirements. Hey Chris, you wonâ€™t believe what happened when I tried sending rent through Zelle yesterday. I hit some daily ACH transfer limit! My landlordâ€™s insisting on cashierâ€™s checks now. Remember how Sarahâ€™s Venmo payment got flagged last month? The bankâ€™s fraud detection system kept asking about transaction memos and â€˜source of fundsâ€™ verification. Honestly, these FDIC regulations around peer-to-peer payments are getting ridiculous. I had to provide three months of bank statements just to increase my wire transfer threshold. Oh, and donâ€™t even get me started on cryptocurrency wallet KYC requirements. Please recognize the language of this speech and transcribe it. Format: oral. This is an audio about Banking. This audio may contains the following words or phrases:Priority Pass lounges,T S A Pre Check,rewards structure,bonus miles,Citibankâ€™s Prestige Card,Visa Infinite,E M V chip security protocols,dynamic currency conversion. So listen, I finally canceled my Chase Sapphire Reserve last week. Remember how they touted those Priority Pass lounges and Luxury Hotel Collection benefits? Turns out I only used the T S A Pre Check credit once this whole year! The annual fee jumped to five hundred fifty dollars, plus they started requiring eighteen thousand points to waive it. My Amex Platinum isnâ€™t any better that seven hundred dollar fee just hit, and their new rewards structure requires thirty thousand in annual spending for bonus miles. Oh, and get this Citibankâ€™s Prestige Card now charges two hundred bucks for authorized users! Honestly, these Visa Infinite perks like concierge services and purchase protection sound fancy, but when do regular people actually use E M V chip security protocols or dynamic currency conversion? Please recognize the language of this speech and transcribe it. Format: oral. This is an audio about é…’åº—å¸¸æ—…å®¢è®¡åˆ’. This audio may contains the following words or phrases:è‡³æ‚¦å¤§ä½¿,é‡åº†æ¥ç¦å£«æ´²é™…,é…’å»Šå¾…é‡,ä¸‡è±ªæ—…äº«å®¶,é’›é‡‘ä¼šå‘˜. è¯¶ï¼Ÿå°æï¼Œæˆ‘æœ€è¿‘åœ¨ç ”ç©¶IHGçš„ä¼šå‘˜ä½“ç³»ï¼Œè¿™ä¸ªâ€˜è‡³æ‚¦å¤§ä½¿â€™çš„è¾¾æ ‡æ¡ä»¶ä¹Ÿå¤ªè‹›åˆ»äº†å§ï¼â€˜ä¸‰ç™¾æƒç›Šâ€™é‡Œï¼Œæ´²é™…çš„è®¤å¯æˆ¿æ™šæ‰ç»™ä¸‰åæ™šã€‚ä½ è¯´ï¼Œä»–ä»¬å®¶çš„â€˜å…ˆè¡Œè€…ä»»åŠ¡â€™ç®—ä¸ç®—â€˜é‡Œç¨‹ç¢‘å¥–åŠ±â€™å•Šï¼Ÿå¯¹äº†ï¼Œæˆ‘ä¹‹å‰ç”¨ç§¯åˆ†å…‘æ¢é‡åº†æ¥ç¦å£«æ´²é™…çš„è¡Œæ”¿å¥—æˆ¿ï¼Œç¤¼å®¾éƒ¨å±…ç„¶æ²¡ç»™é…’å»Šå¾…é‡ï¼Œåè€Œç°é‡‘è®¢æˆ¿çš„å®¢äººèƒ½æ‹¿åˆ°åŒæ—©ã€‚ä¸‡è±ªæ—…äº«å®¶çš„â€˜é’›é‡‘ä¼šå‘˜â€™éƒ½èƒ½è‡ªåŠ¨åŒ¹é…å¥—æˆ¿å‡çº§åˆ¸ï¼ŒIHGè¿™ä¸ªåŠ¨æ€å®šä»·ç³»ç»ŸçœŸæ˜¯è®©äººå¤´å¤§ï¼ Please recognize the language of this speech and transcribe it. Format: oral. This is an audio about æ±½è½¦è¡Œä¸š. This audio may contains the following words or phrases:æ±½è½¦ä¹‹å®¶æ›¹é›·,çŸ©é˜µå¼ L E D å¤§ç¯,å››åå…«ä¼è½»æ··ç³»ç»Ÿ,å¯å˜æ°”é—¨å‡ç¨‹æŠ€æœ¯,M B U X è¶…è”å±,Sportback,Allroad. å˜¿ï¼Œè€æï¼Œä½ çœ‹åˆ°â€˜æ±½è½¦ä¹‹å®¶â€™æ›¹é›·å‘çš„æ–‡ç« æ²¡ï¼Ÿè¯´æ–°æ¬¾å¥¥è¿ªA3åŠ é•¿åˆ°å››ç±³å…­äº†ã€‚æ˜¨å„¿æˆ‘å»4Såº—è¯•é©¾ï¼Œé”€å”®è¯´è¿™è½¦é…äº†å•¥çŸ©é˜µå¼LEDå¤§ç¯ï¼Œè¿˜æœ‰å››åå…«ä¼è½»æ··ç³»ç»Ÿã€‚ä¸è¿‡ï¼Œå®é©¬1ç³»é‚£ä¸ªB48å‘åŠ¨æœºä¹Ÿæ”¹äº†â€˜å¯å˜æ°”é—¨å‡ç¨‹æŠ€æœ¯â€™ï¼Œå¥”é©°Açº§æ›´å¤¸å¼ ï¼Œç›´æ¥æŠŠMBUXè¶…è”å±å¡è¿›ç´§å‡‘è½¦é‡Œï¼è¦æˆ‘è¯´å•Šï¼Œç°åœ¨è½¦ä¼æç»†åˆ†å¸‚åœºçœŸå¤Ÿæ‹¼çš„ï¼å¬è¯´å¥¥è¿ªè¿˜è¦å‡ºSportbackã€Allroadç­‰å››ä¸ªç‰ˆæœ¬å‘¢ï¼Œè¿è‡ªé€‚åº”å·¡èˆªéƒ½æ ‡é…äº†ï¼ Audio Generation Voice Clone Input Prompt Target Text TTS Result å…¨çƒæ¯å¹´æœ‰è¶…è¿‡ä¸€ç™¾ä¸‰åäº”ä¸‡äººï¼Œå› äº¤é€šäº‹æ•…è€Œæ­»äº¡ã€‚ The stained glass offered a hypnotic atmosphere. Multi-lingual Synthesis Input Prompt Text Input Prompt audio Target Text TTS Result We asked over twenty different people, and they all said it was his. The stained glass offered a hypnotic atmosphere. The wedding was photographed by celebrity wedding photographer Kid Chan. Bender also conducted extensive research on autism. å…³äºä¸å°‘ä¸‡è¾¾å¹¿åœºçš„æ³¨å†Œèµ„æœ¬é‡‘æ›´æ”¹ã€‚ å“ï¼Œè¿™äº›æƒ…å†µåœ¨åŒ—äº¬è¿™æ ·çš„å¤§éƒ½å¸‚ï¼Œæ˜¯æ— æ³•é¿å…çš„ã€‚ é•¿æ˜¥å‘¨äºŒä¹‹å‰æ™´å¤©å¤šäº‘äº”æœˆä¸ƒæ—¥æ˜¯æ™´å¤©ã€‚ ä¸¤äººä¸€ç›´å¯¹å©šå˜å°å£ï¼Œä½¿ä¼ é—»é—¹å¾—çƒ­çƒ˜çƒ˜ã€‚</p></div><footer class=entry-footer>&lt;span title='2025-10-01 00:00:03 +0800 +0800'>October 1, 2025&lt;/span>&amp;nbsp;Â·&amp;nbsp;7 min&amp;nbsp;Â·&amp;nbsp;1431 words&amp;nbsp;Â·&amp;nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation" href=https://inclusionai.github.io/blog/ming-uniaudio/></a></article><article class=post-entry><header class=entry-header><h2>Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer</h2></header><div class=entry-content><p>GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
ğŸš€ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent spaceâ€”eliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks. Reduced Representational Competition â†’ 3.5Ã— Faster Convergence: The unified continuous representation aligns semantic understanding and generative dynamics, significantly accelerating joint training without performance trade-offs. Multi-Round In-Context Learning in a Single Feature Space: All operationsâ€”understanding, generation, and editingâ€”occur in the same continuous space, eliminating costly cross-space conversions and enabling simpler, more efficient training and inference. The Challenge: The Inverse Nature of Seeing and Drawing Autoregressionâ€”the powerful paradigm of modeling the world by â€œpredicting the next tokenâ€â€”has already unified diverse modalities like language and audio. The next frontier is to bring visual understanding (seeing) and visual generation (drawing) into this unified sequenceâ€‘toâ€‘sequence framework.
...</p></div><footer class=entry-footer>&lt;span title='2025-10-01 00:00:03 +0800 +0800'>October 1, 2025&lt;/span>&amp;nbsp;Â·&amp;nbsp;5 min&amp;nbsp;Â·&amp;nbsp;1050 words&amp;nbsp;Â·&amp;nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer" href=https://inclusionai.github.io/blog/mingtok/></a></article><article class=post-entry><header class=entry-header><h2>Segmentation-as-Editing for Unified Multimodal AI</h2></header><div class=entry-content><p>GITHUB ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.
From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.
Editing fundamentally requires two distinct skill sets:
Know where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike.
...</p></div><footer class=entry-footer>&lt;span title='2025-09-13 00:00:03 +0800 +0800'>September 13, 2025&lt;/span>&amp;nbsp;Â·&amp;nbsp;7 min&amp;nbsp;Â·&amp;nbsp;1289 words&amp;nbsp;Â·&amp;nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Segmentation-as-Editing for Unified Multimodal AI" href=https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://inclusionai.github.io/blog/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#FFCC99 0%,#7F667F 40%,#262656 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/>ç®€ä½“ä¸­æ–‡</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</h2></header><div class=entry-content><p>ğŸ“– Technical Report | ğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals....</p></div><footer class=entry-footer><span title='2025-07-11 00:00:03 +0800 +0800'>July 11, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;1052 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning" href=https://inclusionai.github.io/blog/m2-reasoning/></a></article><article class=post-entry><header class=entry-header><h2>ABench: An Evolving Open-Source Benchmark</h2></header><div class=entry-content><p>GITHUB ğŸŒŸ Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ğŸ¯ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems ğŸ“Š Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics âœ… Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management âœ… Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) ğŸ”„ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories ğŸ”„ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law ğŸ”„ In Preparation</p></div><footer class=entry-footer><span title='2025-07-08 00:00:03 +0800 +0800'>July 8, 2025</span>&nbsp;Â·&nbsp;1 min&nbsp;Â·&nbsp;185 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to ABench: An Evolving Open-Source Benchmark" href=https://inclusionai.github.io/blog/abench/></a></article><article class=post-entry><header class=entry-header><h2>AWorld: The Agent Runtime for Self-Improvement</h2></header><div class=entry-content><p>â€œSelf-awareness: the hardest problem isnâ€™t solving within limits, itâ€™s discovering the own limitationsâ€ Table of Contents News â€” Latest updates and announcements. Introduction â€” Overview and purpose of the project. Installation â€” Step-by-step setup instructions. Quick Start â€” Get started with usage examples. Architecture â€” Explore the multi-agent system design. Demo â€” See the project in action with demonstrations. Contributing â€” How to get involved and contribute. License â€” Project licensing details....</p></div><footer class=entry-footer><span title='2025-07-07 00:00:03 +0800 +0800'>July 7, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;895 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to AWorld: The Agent Runtime for Self-Improvement" href=https://inclusionai.github.io/blog/aworld/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Omni: A Unified Multimodal Model for Perception and Generation</h2></header><div class=entry-content><p>GITHUB ğŸ“‘ Technical Reportï½œğŸ“–Project Page ï½œğŸ¤— Hugging Faceï½œ ğŸ¤– ModelScope
Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers....</p></div><footer class=entry-footer><span title='2025-06-11 00:00:03 +0800 +0800'>June 11, 2025</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1379 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Omni: A Unified Multimodal Model for Perception and Generation" href=https://inclusionai.github.io/blog/ming-omni/></a></article><article class=post-entry><header class=entry-header><h2>Ling: A MoE LLM Provided and Open-sourced by InclusionAI</h2></header><div class=entry-content><p>ğŸ¤— Hugging FaceÂ Â  | Â Â ğŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems....</p></div><footer class=entry-footer><span title='2025-05-08 00:00:03 +0800 +0800'>May 8, 2025</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;1574 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ling: A MoE LLM Provided and Open-sourced by InclusionAI" href=https://inclusionai.github.io/blog/ling/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://inclusionai.github.io/blog/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
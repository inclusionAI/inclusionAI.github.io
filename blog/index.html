<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span class=active>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#FFCC99 0%,#7F667F 40%,#262656 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/>ç®€ä½“ä¸­æ–‡</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation</h2></header><div class=entry-content><p>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation Omnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0â€™s sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition....</p></div><footer class=entry-footer><span title='2025-10-28 00:00:03 +0800 +0800'>October 28, 2025</span>&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;1071 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation" href=https://inclusionai.github.io/blog/ming-flash-omni-preview/></a></article><article class=post-entry><header class=entry-header><h2>Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</h2></header><div class=entry-content><p>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
The Introduction Video of Ming-UniAudio Audio Edit Demo Editing Tasks Video demos ðŸš€ Technical Highlights First unified continuous speech tokenizer for both understanding and generation tasks: MingTok-Audio is a unified continuous speech tokenizer MingTok-Audio based on a VAE framework with a causal Transformer architecture, the first continuous speech tokenizer to effectively integrate semantic and acoustic features, and enables a closed-loop system with LLMs through hierarchical feature representations, makes it suitable for both understanding and generation tasks....</p></div><footer class=entry-footer><span title='2025-10-01 00:00:03 +0800 +0800'>October 1, 2025</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1431 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation" href=https://inclusionai.github.io/blog/ming-uniaudio/></a></article><article class=post-entry><header class=entry-header><h2>Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer</h2></header><div class=entry-content><p>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
ðŸš€ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent spaceâ€”eliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks. Reduced Representational Competition â†’ 3.5Ã— Faster Convergence: The unified continuous representation aligns semantic understanding and generative dynamics, significantly accelerating joint training without performance trade-offs....</p></div><footer class=entry-footer><span title='2025-10-01 00:00:03 +0800 +0800'>October 1, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;1050 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer" href=https://inclusionai.github.io/blog/mingtok/></a></article><article class=post-entry><header class=entry-header><h2>Segmentation-as-Editing for Unified Multimodal AI</h2></header><div class=entry-content><p>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.
From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.
Editing fundamentally requires two distinct skill sets:
Know where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike....</p></div><footer class=entry-footer><span title='2025-09-13 00:00:03 +0800 +0800'>September 13, 2025</span>&nbsp;Â·&nbsp;7 min&nbsp;Â·&nbsp;1289 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Segmentation-as-Editing for Unified Multimodal AI" href=https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/></a></article><article class=post-entry><header class=entry-header><h2>Introducing Ring-lite-2507</h2></header><div class=entry-content><p>ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Overview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite (2506). Built upon a 16.8B Mixture-of-Experts (MoE) large language model with 2.75B activated parameters, Ring-lite-2507 further advances its reasoning capabilities while demonstrating superior performance across a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical, and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguishes itself from the latest public dense models under 10B parameters by offering competitive performance across various tasks, despite activating only 1/3 of their parameter size....</p></div><footer class=entry-footer><span title='2025-08-05 00:00:03 +0800 +0800'>August 5, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;998 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Introducing Ring-lite-2507" href=https://inclusionai.github.io/blog/ring-lite-2507/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://inclusionai.github.io/blog/page/2/>Next&nbsp;&nbsp;Â»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Segmentation-as-Editing for Unified Multimodal AI | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.
From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.
Editing fundamentally requires two distinct skill sets:
Know where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Segmentation-as-Editing for Unified Multimodal AI"><meta property="og:description" content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.
From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.
Editing fundamentally requires two distinct skill sets:
Know where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-09-13T00:00:03+08:00"><meta property="article:modified_time" content="2025-09-13T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Segmentation-as-Editing for Unified Multimodal AI"><meta name=twitter:description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.
From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.
Editing fundamentally requires two distinct skill sets:
Know where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Segmentation-as-Editing for Unified Multimodal AI","item":"https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Segmentation-as-Editing for Unified Multimodal AI","name":"Segmentation-as-Editing for Unified Multimodal AI","description":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\nMing-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.\nFrom the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.\nEditing fundamentally requires two distinct skill sets:\nKnow where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike.","keywords":[],"articleBody":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\nMing-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.\nFrom the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.\nEditing fundamentally requires two distinct skill sets:\nKnow where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike.\nBut behind the noise, few are asking:\nBeneath this prosperity, how close are we to a truly unified “understanding + generation” AI?\nUnderstanding and Generation: Two Hands, Often Out of Sync For years, we’ve chased an ambitious goal:\nBuild a unified multimodal model that understands the world like a scientist (e.g., image segmentation) while creating it like an artist (e.g., image editing).\nIn theory, these abilities should be mutually reinforcing:\n“The deeper the understanding, the better the creation; the more the creation, the deeper the understanding.”\nReality is messier.\nIn AI today:\nUnderstanding = the left hand: precise abstractions, semantic reasoning, boundaries. Generation = the right hand: coherent pixels, style, aesthetics. But training a model to recognize 10,000 cat photos doesn’t magically make it capable of painting cats, and painting cats repeatedly doesn’t make it understand cats better.\nWorse, in multitask training, the two often compete for resources — optimizations for understanding can hurt generation, and vice versa.\nWe’re missing a catalyst: a task that forces the left and right hands to evolve together.\nThe Struggle: 16% Segmentation and Out-of-Control Generation Before finding our solution, our unified model was struggling with generative segmentation:\nGiven an instruction like “segment the banana in the upper-right corner”, we wanted the model to output a segmentation mask directly.\nThe results were painful.\nOn RefCOCO-val, our cIoU plateaued at ~16%.\nThe root cause is the distribution gap.\nGenerative models thrive on natural, continuous image distributions. Segmentation masks, however, are synthetic, abstract, binary maps — as unnatural as it gets for an image generator.\nIt was like asking a painter to draw an X-ray: doable, but far from their artistic instincts.\nHere, generation wasn’t helping segmentation — it was tripping it up.\nWe needed a new task that:\nMet the precision demands of understanding. Played to the strengths of generation. The “Aha” Moment: Dressing Segmentation in Color Here’s the analogy that unlocked it for us:\nIf you want a child to mark an object, is it easier to have them draw a tight outline with a pencil, or fill it in with bright colors?\nObviously, the latter.\nInstead of forcing our model to output abstract black-and-white masks, we turned the segmentation task into a color-editing task.\nExample:\nInstruction: “segment the banana in the upper-right” Old way: Output a mask ❌ New way: Directly edit the image: “paint the banana purple”, “make the banana red”, etc. ✅ This brought the task’s data distribution back to the realm of natural images — where generative models shine.\nWhy This Works: The Hidden Catalyst That small twist turned out to be exactly the catalyst we’d been searching for.\nBoosting Understanding: To color the banana without bleeding outside the boundary, the model must internally nail pixel-perfect segmentation. The segmentation step became an implicit prerequisite to editing.\nUnleashing Generation: No more awkward synthetic masks — the model is doing what it knows best: image-to-image editing. All its strengths in shading, texture, and edge blending go into making the change look natural.\nFor the first time, the left hand and right hand weren’t fighting — they were helping each other.\nThe Numbers: From 16% to 72.4% — and Beyond 1. SOTA-level Segmentation The cIoU score didn’t just improve — it soared from 16% to 72.4% on RefCOCO-val, a relative gain of over 350%.\nQualitatively, the model outperformed competitors in pinpointing and segmenting targets, even in reasoning-heavy cases.\nAgainst Qwen-Image and Nano Banana, our model:\nLocated small or occluded targets more reliably. Produced boundaries that were visually and semantically aligned with instructions. Our model (right) accurately locates and segments the target subject. Qwen-Image (second from left) fails to locate the correct target, while Nano-banana (third from left) fails to accurately segment the man’s head and has loose boundary lines.\nFor the prompt “please segment the girl with red mask,” our model (right) is precise. Qwen-Image (second from left) misses the feet, and Nano-banana (third from left) alters the subject’s proportions.\nDuring evaluation, thanks to the high consistency of non-edited regions in our model, we can directly derive the segmentation mask by calculating the difference between the edited result and the original image.\nThe results show that our model’s performance on segmentation is now on par with specialized vision models.\nModel Category Model Name RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist Models VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67.6 67.8 MLLM + Specialist (SAM) LISA-7B 74.1 62.4 66.4 PixelLM-7B 73.0 66.3 69.3 Generative Models Nano-banana* 15.7 13.9 14.9 Qwen-Image-Edit* 30.3 28.8 34.0 Ming-Lite-Omni1.5 72.4 62.8 64.3 For each test set, Nano-banana and Qwen-Image-Edit was evaluated on a randomly sampled subset of 500 images, to reduce computational cost while preserving the key statistical trends. We observed that Nano-banana frequently fails to accurately grasp the image segmentation intent during inference, leading to its comparatively lower evaluation metrics. This may be attributed to differences in training objectives and data emphasis.\n2. Sharper, More Controllable Editing The beauty of this method is that it not only fixed the segmentation weakness but also dramatically enhanced the model’s general editing capabilities.\nBecause the model has learned an unprecedented “respect for boundaries” through thousands of “precise coloring” exercises, this “muscle memory” for fine-grained control has transferred to all editing tasks. Our edit controllability score saw a significant jump from 7.69 to 8.12 across sub-tasks like background, color, and material changes.\nPrompt: “remove the bow tie of the man on the far right.” Our model (right) precisely removes only the target bow tie while maintaining background consistency. Qwen (second from left) incorrectly removes multiple bow ties and introduces inconsistencies. Nano-banana (third from left) also struggles with consistency.\n3. Stronger ID Consistency A core challenge in portrait editing is maintaining identity. Our model excels here as well. Whether changing a hairstyle or adjusting an expression, the model skillfully preserves the person’s core features.\nTop Row (Turn head): Our model (right) maintains ID and background consistency, unlike competitors. Middle Row (Smile): Our model (right) correctly follows the prompt while preserving ID, avoiding distortions seen in others. Bottom Row (Change background): Our model (right) excels at preserving the subject’s ID and appearance during a background swap.\nSee More Editing Consistency in Action: An Honest Look: Where We Can Still Improve Despite the leap forward, challenges remain:\nLarge pose changes (e.g., standing → running) need more reliability. Multi-step or compound instructions require better parsing and execution. Instruction diversity support needs expansion. These are our next milestones.\nTakeaway: The Next Catalysts Are Out There From 16% to 72.4% — this wasn’t driven by a massive architecture overhaul or billion-image datasets.\nIt came from one change in task design.\nThe lesson: Instead of gluing capabilities together after the fact, find naturally cooperative tasks — where solving the problem requires multiple abilities to mesh seamlessly.\n“Segmentation-as-editing” is just the first example.\nWe suspect 3D understanding, video generation, and other domains have their own hidden catalysts, waiting to be discovered.\nAt last, AI’s left and right hands have learned to high-five.\nAnd this is only the overture.\nTry out our open-source model Ming-lite-omni 1.5 on our GitHub Page / Demo Page. Please star our repo if you like it!\n","wordCount":"1289","inLanguage":"en","datePublished":"2025-09-13T00:00:03+08:00","dateModified":"2025-09-13T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Segmentation-as-Editing for Unified Multimodal AI</h1><div class=post-meta><span title='2025-09-13 00:00:03 +0800 +0800'>September 13, 2025</span>&nbsp;·&nbsp;7 min&nbsp;·&nbsp;1289 words&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/>简体中文</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> 🤗 <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>｜ 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><h1 id=ming-lite-omni-15-segmentation-as-editing-for-unified-multimodal-ai>Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI<a hidden class=anchor aria-hidden=true href=#ming-lite-omni-15-segmentation-as-editing-for-unified-multimodal-ai>#</a></h1><h3 id=the-hype-and-the-hidden-question>The Hype and the Hidden Question<a hidden class=anchor aria-hidden=true href=#the-hype-and-the-hidden-question>#</a></h3><p>The multimodal AI world has been thriving.</p><p>From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.</p><p>Editing fundamentally requires two distinct skill sets:</p><ul><li><strong>Know <em>where</em>, <em>what</em>, and <em>how</em> to change</strong> (understanding the image)</li><li><strong>Produce the change with high visual quality</strong> (generating the image)</li></ul><p>Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike.</p><p>But behind the noise, few are asking:</p><p><strong>Beneath this prosperity, how close are we to a truly unified “understanding + generation” AI?</strong></p><h3 id=understanding-and-generation-two-hands-often-out-of-sync>Understanding and Generation: Two Hands, Often Out of Sync<a hidden class=anchor aria-hidden=true href=#understanding-and-generation-two-hands-often-out-of-sync>#</a></h3><p>For years, we’ve chased an ambitious goal:</p><p>Build a unified multimodal model that understands the world like a scientist (e.g., image segmentation) while creating it like an artist (e.g., image editing).</p><p>In theory, these abilities should be mutually reinforcing:</p><p><em>“The deeper the understanding, the better the creation; the more the creation, the deeper the understanding.”</em></p><p>Reality is messier.</p><p>In AI today:</p><ul><li><strong>Understanding = the left hand:</strong> precise abstractions, semantic reasoning, boundaries.</li><li><strong>Generation = the right hand:</strong> coherent pixels, style, aesthetics.</li></ul><p>But training a model to recognize 10,000 cat photos doesn’t magically make it capable of painting cats, and painting cats repeatedly doesn’t make it understand cats better.</p><p>Worse, in multitask training, the two often compete for resources — optimizations for understanding can hurt generation, and vice versa.</p><p><strong>We’re missing a catalyst: a task that forces the left and right hands to evolve together.</strong></p><hr><h3 id=the-struggle-16-segmentation-and-out-of-control-generation>The Struggle: 16% Segmentation and Out-of-Control Generation<a hidden class=anchor aria-hidden=true href=#the-struggle-16-segmentation-and-out-of-control-generation>#</a></h3><p>Before finding our solution, our unified model was struggling with generative segmentation:</p><p>Given an instruction like “<em>segment the banana in the upper-right corner</em>”, we wanted the model to output a segmentation mask directly.</p><p>The results were painful.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*2BAkRZ9WGTcAAAAAgCAAAAgAevzJAQ/original alt="Struggling with Segmentation"></p><p>On RefCOCO-val, our cIoU plateaued at <strong>~16%</strong>.</p><p>The root cause is the <strong>distribution gap</strong>.</p><p>Generative models thrive on natural, continuous image distributions. Segmentation masks, however, are synthetic, abstract, binary maps — as unnatural as it gets for an image generator.</p><p>It was like asking a painter to draw an X-ray: doable, but far from their artistic instincts.</p><p>Here, generation wasn’t helping segmentation — it was tripping it up.</p><p>We needed a new task that:</p><ol><li>Met the precision demands of <strong>understanding</strong>.</li><li>Played to the strengths of <strong>generation</strong>.</li></ol><h3 id=the-aha-moment-dressing-segmentation-in-color>The “Aha” Moment: Dressing Segmentation in Color<a hidden class=anchor aria-hidden=true href=#the-aha-moment-dressing-segmentation-in-color>#</a></h3><p>Here’s the analogy that unlocked it for us:</p><p><em>If you want a child to mark an object, is it easier to have them draw a tight outline with a pencil, or fill it in with bright colors?</em></p><p>Obviously, the latter.</p><p>Instead of forcing our model to output abstract black-and-white masks, we <strong>turned the segmentation task into a color-editing task</strong>.</p><p><strong>Example:</strong></p><ul><li><strong>Instruction:</strong> “<em>segment the banana in the upper-right</em>”</li><li><strong>Old way:</strong> Output a mask ❌</li><li><strong>New way:</strong> Directly edit the image: “<em>paint the banana purple</em>”, “<em>make the banana red</em>”, etc. ✅</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*-_O6RLOxXKcAAAAAgBAAAAgAevzJAQ/original alt="Segmentation as Editing"></p><p>This brought the task’s data distribution back to the realm of natural images — where generative models shine.</p><h3 id=why-this-works-the-hidden-catalyst>Why This Works: The Hidden Catalyst<a hidden class=anchor aria-hidden=true href=#why-this-works-the-hidden-catalyst>#</a></h3><p>That small twist turned out to be exactly the catalyst we’d been searching for.</p><ul><li><p><strong>Boosting Understanding:</strong>
To color the banana without bleeding outside the boundary, the model must internally nail pixel-perfect segmentation. The segmentation step became an <strong>implicit prerequisite</strong> to editing.</p></li><li><p><strong>Unleashing Generation:</strong>
No more awkward synthetic masks — the model is doing what it knows best: image-to-image editing. All its strengths in shading, texture, and edge blending go into making the change look natural.</p></li></ul><p>For the first time, the left hand and right hand weren’t fighting — <strong>they were helping each other</strong>.</p><hr><h3 id=the-numbers-from-16-to-724--and-beyond>The Numbers: From 16% to 72.4% — and Beyond<a hidden class=anchor aria-hidden=true href=#the-numbers-from-16-to-724--and-beyond>#</a></h3><h4 id=1-sota-level-segmentation>1. SOTA-level Segmentation<a hidden class=anchor aria-hidden=true href=#1-sota-level-segmentation>#</a></h4><p>The cIoU score didn’t just improve — it soared from 16% to <strong>72.4%</strong> on RefCOCO-val, a relative gain of over <strong>350%</strong>.</p><p>Qualitatively, the model outperformed competitors in pinpointing and segmenting targets, even in reasoning-heavy cases.</p><p>Against Qwen-Image and Nano Banana, our model:</p><ul><li>Located small or occluded targets more reliably.</li><li>Produced boundaries that were visually and semantically aligned with instructions.</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*DwJpSZyoW-YAAAAAgJAAAAgAevzJAQ/original alt="Segmentation Comparison 1">
<em>Our model (right) accurately locates and segments the target subject. Qwen-Image (second from left) fails to locate the correct target, while Nano-banana (third from left) fails to accurately segment the man&rsquo;s head and has loose boundary lines.</em></p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*yL2MR7vLQdEAAAAAgEAAAAgAevzJAQ/original alt="Segmentation Comparison 2">
<em>For the prompt &ldquo;please segment the girl with red mask,&rdquo; our model (right) is precise. Qwen-Image (second from left) misses the feet, and Nano-banana (third from left) alters the subject&rsquo;s proportions.</em></p><p>During evaluation, thanks to the high consistency of non-edited regions in our model, we can directly derive the segmentation mask by calculating the difference between the edited result and the original image.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*UJX1RJJpu3cAAAAASyAAAAgAevzJAQ/original alt="Calculating difference on Ming-Lite-Omni1.5, Qwen-Image-Edit, Nano-banana"></p><p>The results show that our model&rsquo;s performance on segmentation is now on par with specialized vision models.</p><table><thead><tr><th style=text-align:left>Model Category</th><th style=text-align:left>Model Name</th><th style=text-align:center>RefCOCO (val)</th><th style=text-align:center>RefCOCO+ (val)</th><th style=text-align:center>RefCOCOg (val)</th></tr></thead><tbody><tr><td style=text-align:left><strong>Vision Specialist Models</strong></td><td style=text-align:left>VLT</td><td style=text-align:center>67.5</td><td style=text-align:center>56.3</td><td style=text-align:center>55.0</td></tr><tr><td style=text-align:left></td><td style=text-align:left>CRIS</td><td style=text-align:center>70.5</td><td style=text-align:center>62.3</td><td style=text-align:center>59.9</td></tr><tr><td style=text-align:left></td><td style=text-align:left>LAVT</td><td style=text-align:center>72.7</td><td style=text-align:center>62.1</td><td style=text-align:center>61.2</td></tr><tr><td style=text-align:left></td><td style=text-align:left>PolyFormer-B</td><td style=text-align:center>74.8</td><td style=text-align:center>67.6</td><td style=text-align:center>67.8</td></tr><tr><td style=text-align:left><strong>MLLM + Specialist (SAM)</strong></td><td style=text-align:left>LISA-7B</td><td style=text-align:center>74.1</td><td style=text-align:center>62.4</td><td style=text-align:center>66.4</td></tr><tr><td style=text-align:left></td><td style=text-align:left>PixelLM-7B</td><td style=text-align:center>73.0</td><td style=text-align:center>66.3</td><td style=text-align:center>69.3</td></tr><tr><td style=text-align:left><strong>Generative Models</strong></td><td style=text-align:left>Nano-banana*</td><td style=text-align:center>15.7</td><td style=text-align:center>13.9</td><td style=text-align:center>14.9</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Qwen-Image-Edit*</td><td style=text-align:center>30.3</td><td style=text-align:center>28.8</td><td style=text-align:center>34.0</td></tr><tr><td style=text-align:left></td><td style=text-align:left><strong>Ming-Lite-Omni1.5</strong></td><td style=text-align:center><strong>72.4</strong></td><td style=text-align:center><strong>62.8</strong></td><td style=text-align:center><strong>64.3</strong></td></tr></tbody></table><p><em><small>For each test set, Nano-banana and Qwen-Image-Edit was evaluated on a randomly sampled subset of 500 images, to reduce computational cost while preserving the key statistical trends. We observed that Nano-banana frequently fails to accurately grasp the image segmentation intent during inference, leading to its comparatively lower evaluation metrics. This may be attributed to differences in training objectives and data emphasis.</small></em></p><h4 id=2-sharper-more-controllable-editing>2. Sharper, More Controllable Editing<a hidden class=anchor aria-hidden=true href=#2-sharper-more-controllable-editing>#</a></h4><p>The beauty of this method is that it not only fixed the segmentation weakness but also dramatically enhanced the model&rsquo;s general editing capabilities.</p><p>Because the model has learned an unprecedented &ldquo;respect for boundaries&rdquo; through thousands of &ldquo;precise coloring&rdquo; exercises, this &ldquo;muscle memory&rdquo; for fine-grained control has transferred to all editing tasks. Our edit controllability score saw a significant jump from <strong>7.69 to 8.12</strong> across sub-tasks like background, color, and material changes.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*szjcQqQkC80AAAAAgIAAAAgAevzJAQ/original alt="Editing Controllability Comparison">
<em>Prompt: &ldquo;remove the bow tie of the man on the far right.&rdquo; Our model (right) precisely removes only the target bow tie while maintaining background consistency. Qwen (second from left) incorrectly removes multiple bow ties and introduces inconsistencies. Nano-banana (third from left) also struggles with consistency.</em></p><h4 id=3-stronger-id-consistency>3. Stronger ID Consistency<a hidden class=anchor aria-hidden=true href=#3-stronger-id-consistency>#</a></h4><p>A core challenge in portrait editing is maintaining identity. Our model excels here as well. Whether changing a hairstyle or adjusting an expression, the model skillfully preserves the person&rsquo;s core features.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*Tc2-RoAHys8AAAAAd9AAAAgAevzJAQ/original alt="ID Consistency Comparison">
<em>Top Row (Turn head): Our model (right) maintains ID and background consistency, unlike competitors. Middle Row (Smile): Our model (right) correctly follows the prompt while preserving ID, avoiding distortions seen in others. Bottom Row (Change background): Our model (right) excels at preserving the subject&rsquo;s ID and appearance during a background swap.</em></p><p><strong>See More Editing Consistency in Action:</strong>
<video src=https://gw.alipayobjects.com/v/huamei_wp0xz6/afts/video/A*CcqdTbafkt8AAAAAgEAAAAgAevzJAQ width=704px height=740px controls></video></p><hr><h3 id=an-honest-look-where-we-can-still-improve>An Honest Look: Where We Can Still Improve<a hidden class=anchor aria-hidden=true href=#an-honest-look-where-we-can-still-improve>#</a></h3><p>Despite the leap forward, challenges remain:</p><ul><li><strong>Large pose changes</strong> (e.g., standing → running) need more reliability.</li><li><strong>Multi-step or compound instructions</strong> require better parsing and execution.</li><li><strong>Instruction diversity support</strong> needs expansion.</li></ul><p>These are our next milestones.</p><h3 id=takeaway-the-next-catalysts-are-out-there>Takeaway: The Next Catalysts Are Out There<a hidden class=anchor aria-hidden=true href=#takeaway-the-next-catalysts-are-out-there>#</a></h3><p>From 16% to 72.4% — this wasn’t driven by a massive architecture overhaul or billion-image datasets.</p><p>It came from <strong>one change in task design</strong>.</p><p>The lesson: Instead of gluing capabilities together after the fact, <strong>find naturally cooperative tasks</strong> — where solving the problem requires multiple abilities to mesh seamlessly.</p><p>“Segmentation-as-editing” is just the first example.</p><p>We suspect 3D understanding, video generation, and other domains have their own hidden catalysts, waiting to be discovered.</p><p><strong>At last, AI’s left and right hands have learned to high-five.</strong></p><p><strong>And this is only the overture.</strong></p><p>Try out our open-source model <strong>Ming-lite-omni 1.5</strong> on our <a href=https://github.com/inclusionAI/Ming/blob/main/cookbook.ipynb><strong>GitHub Page / Demo Page</strong></a>. Please star our repo if you like it!</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
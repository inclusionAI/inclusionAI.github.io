<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Introducing Ring-lite-2507 | INCLUSION AI</title><meta name=keywords content><meta name=description content="üìñ Technical Report | ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite. Building upon 16.8B Mixture-of-Experts (MoE)-based large language model with 2.75B activated parameters, Ring-lite-2507 further pushes its reasoning ability to an advanced level, meanwhile, it demonstrates superior performance on a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguished itself from latest public dense models under 10B parameters by showing competitive performance across various tasks while activating only 1/3 of their parameter size."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ring-lite-2507/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ring-lite-2507/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Introducing Ring-lite-2507"><meta property="og:description" content="üìñ Technical Report | ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite. Building upon 16.8B Mixture-of-Experts (MoE)-based large language model with 2.75B activated parameters, Ring-lite-2507 further pushes its reasoning ability to an advanced level, meanwhile, it demonstrates superior performance on a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguished itself from latest public dense models under 10B parameters by showing competitive performance across various tasks while activating only 1/3 of their parameter size."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ring-lite-2507/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-08-05T00:00:03+08:00"><meta property="article:modified_time" content="2025-08-05T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Introducing Ring-lite-2507"><meta name=twitter:description content="üìñ Technical Report | ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Overview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite. Building upon 16.8B Mixture-of-Experts (MoE)-based large language model with 2.75B activated parameters, Ring-lite-2507 further pushes its reasoning ability to an advanced level, meanwhile, it demonstrates superior performance on a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguished itself from latest public dense models under 10B parameters by showing competitive performance across various tasks while activating only 1/3 of their parameter size."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Introducing Ring-lite-2507","item":"https://inclusionai.github.io/blog/ring-lite-2507/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Introducing Ring-lite-2507","name":"Introducing Ring-lite-2507","description":"üìñ Technical Report | ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nOverview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite. Building upon 16.8B Mixture-of-Experts (MoE)-based large language model with 2.75B activated parameters, Ring-lite-2507 further pushes its reasoning ability to an advanced level, meanwhile, it demonstrates superior performance on a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguished itself from latest public dense models under 10B parameters by showing competitive performance across various tasks while activating only 1/3 of their parameter size.","keywords":[],"articleBody":"üìñ Technical Report | ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nOverview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite. Building upon 16.8B Mixture-of-Experts (MoE)-based large language model with 2.75B activated parameters, Ring-lite-2507 further pushes its reasoning ability to an advanced level, meanwhile, it demonstrates superior performance on a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguished itself from latest public dense models under 10B parameters by showing competitive performance across various tasks while activating only 1/3 of their parameter size.\nTo address the optimization instability of MoE RL training, we propose a novel approach, Constrained Contextual Computation Policy Optimization(C3PO), which enhances training stability and improves computational throughput via algorithm-system co-design. Meanwhile, we systematically investigate the dynamic relations between long-chain-of-thought SFT and RL training, surrendering the optimal practice for selecting the suitable fine-tuned model for RL scaling, rather than soley on the validation metrics, yields superior performance-efficiency trade-offs in our RL training pipeline. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, enhancing reasoning ability while effectively improving performance across various downstream general tasks.\nHighlights\nüöÄ Superior performance across tasks: Ring-lite-2507 demonstrates outstanding performance across both reasoning and general tasks; üî• Only 2.75B activated parameters: Ring-lite-2507 is built upon a Mixture-of-Experts (MoE)-based large language model with only 2.75 billion activated parameters; ‚õìÔ∏è‚Äçüí• Algorithm-system co-design: We proposed novel C3PO approach and employ token efficiency to improve training stability and effectiveness; üîç Publicly available: We fully release our training recipe and model weights. Evaluation We evaluate our models comprehensively across two core domains: reasoning domamin and general domain, using a diverse set of public benchmarks categorized by their primary measured capability.\nKnowledge Understanding Benchmark Ring-lite-0731 Ring-lite Qwen3-8B-Thinking MMLU-Pro (EM) 72.50 63.44 72.56 GPQA-Diamond (Pass@1) 69.35 63.51 62.00 SuperGPQA (EM) 40.05 13.97 40.36 Phybench (Pass@1) 28.51 29.19 22.14 Math Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MATH-500 (Pass@1) 97.95 96.80 97.30 CNMO 2024 (Pass@1) 75.09 77.26 74.57 AIME 2024 (Pass@1) 79.79 79.00 74.90 AIME 2025 (Pass@1) 72.92 69.50 67.19 LiveMathBench (Pass@1) 83.37 85.08 81.90 TheoremQA (Pass@1) 70.00 70.19 68.81 OlympiadBench (math) (Pass@1) 80.64 82.86 80.20 Coding Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking LiveCodeBench(2408-2505) (Pass@1) 60.35 59.53 55.12 Codeforces(Percentile) (Pass@1) 1830 1673 1580 Codeforces(Rating) 92.16 88.00 79.44 Reasoning \u0026 Agentic Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking DROP (zero-shot F1) 89.27 60.21 87.13 BBH (EM) 88.65 50.84 87.30 ARCPrize (Pass@1) 19.00 3.12 3.88 MuSR (EM) 77.19 66.77 76.92 BFCL_Live (Pass@1) 74.81 66.76 75.99 Alignment Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking IFEval (Prompt Strict) 84.66 54.34 85.40 AlignBench v1.1(gpt-4.1) 80.90 69.60 74.70 FoFo (gpt-4-turbo) 85.02 67.81 81.93 ArenaHard (gpt-4.1) 88.85 56.12 86.14 Constrained Contextual Computation Policy Optimization(C3PO) We introduce Constrained Contextual Computation Policy Optimization(C3PO), an innovative token-level optimization framework designed to mitigate training instability while enhancing throughput consistency. Different from sampling-level filtering, C3PO operates at the token level by sampling tokens to form a token-level global batch, each training step maintains consistent token input to optimizer, which results in reduced gradient variance and consequently achieving stable optimization.\nC3PO\nBalancing Token efficiency between Distillation and RL While distillation is effective, we find it requires more training tokens to achieve comparable performance compared to RL training. Instead, we observe that varying the number of training epochs of the distilled model significantly influences the trend of entropy loss, thereby determining the exploration scope for RL. Based on our experiments, increasing the number of SFT training epochs leads to a rapid collapse of entropy. However, insufficient SFT training inevitably results in inferior performance. To systematically quantify the choice of optimal SFT epoch, we employ token efficiency to determine the suitable checkpoint for RL scaling.\nTraining Data We follow a stringent data processing pipeline.\nData Pipeline\nTraining Pipeline Training Pipeline\nReasoning RL Compared to our Ring-lite, we expand our reasoning dataset by incorporating more chanlleging math, coding and STEM dataset. Sepecifically, we adopted 67K math data, 32K coding data, 9.9K scientific data for reasoning RL training. In addition, we amplify our reasoning dataset by including more than 19K logical games, such as ARC-AGI, Countdown, sudoku, AlphaMaze, etc. For each type of problems, we specifically design the suitable reward function to make sure our training examples are verifiable. We apply RL on various reasoning tasks, including math, stem, code, logical games.\nGeneral RL Except for reasoning tasks, our Ring-lite-0731 has significantly expanded the collection of general RL training dataset. Our general RL does not sacrifices performance on reasoning tasks, instead, it imporved the overal text understanding ability across a broad range of general benchmarks. Our general RL training incorporates various sources of general tasks, including instruction following, question answering, text summarizaiton, ect. For open-formed question, we adopt a strong reward model to assign reward scores for the problems. Besides, we also incorporated rule-based verifier to tackle problems which can be easily verified, such as the instruction-following problems.\nCitation @misc{lingteam2025ringlitescalablereasoningc3postabilized, title={Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs}, author={Ling Team and Bin Hu and Cai Chen and Deng Zhao and Ding Liu and Dingnan Jin and Feng Zhu and Hao Dai and Hongzhi Luan and Jia Guo and Jiaming Liu and Jiewei Wu and Jun Mei and Jun Zhou and Junbo Zhao and Junwu Xiong and Kaihong Zhang and Kuan Xu and Lei Liang and Liang Jiang and Liangcheng Fu and Longfei Zheng and Qiang Gao and Qing Cui and Quan Wan and Shaomian Zheng and Shuaicheng Li and Tongkai Yang and Wang Ren and Xiaodong Yan and Xiaopei Wan and Xiaoyun Feng and Xin Zhao and Xinxing Yang and Xinyu Kong and Xuemin Yang and Yang Li and Yingting Wu and Yongkang Liu and Zhankai Xu and Zhenduo Zhang and Zhenglei Zhou and Zhenyu Huang and Zhiqiang Zhang and Zihao Wang and Zujie Wen}, year={2025}, eprint={2506.14731}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2506.14731}, } ","wordCount":"966","inLanguage":"en","datePublished":"2025-08-05T00:00:03+08:00","dateModified":"2025-08-05T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ring-lite-2507/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Introducing Ring-lite-2507</h1><div class=post-meta><span title='2025-08-05 00:00:03 +0800 +0800'>August 5, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;966 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group</div></div></div><main class=main><article class=post-single><div class=post-content><p>üìñ <a href=https://arxiv.org/abs/2506.14731>Technical Report</a> | ü§ó <a href=https://huggingface.co/inclusionAI/Ring-lite-2507>Hugging Face</a>ÔΩú ü§ñ <a href=https://modelscope.cn/models/inclusionAI/Ring-lite-2507>ModelScope</a></p><h2 id=overview>Overview<a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>We present <strong>Ring-lite-2507</strong>, an upgraded version of our previously released lightweight reasoning model, <strong>Ring-lite</strong>. Building upon <strong>16.8B</strong> Mixture-of-Experts (MoE)-based large language model with <strong>2.75B</strong> activated parameters, <strong>Ring-lite-2507</strong> further pushes its reasoning ability to an advanced level, meanwhile, it demonstrates superior performance on a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, <strong>Ring-lite-2507</strong> distinguished itself from latest public dense models under 10B parameters by showing competitive performance across various tasks while activating only <strong>1/3</strong> of their parameter size.</p><p>To address the optimization instability of MoE RL training, we propose a novel approach, Constrained Contextual Computation Policy Optimization(C3PO), which enhances training stability and improves computational throughput via algorithm-system co-design. Meanwhile, we systematically investigate the dynamic relations between long-chain-of-thought SFT and RL training, surrendering the optimal practice for selecting the suitable fine-tuned model for RL scaling, rather than soley on the validation metrics, yields superior performance-efficiency trade-offs in our RL training pipeline. Finally, we develop a two-stage training paradigm to harmonize multi-domain data integration, enhancing reasoning ability while effectively improving performance across various downstream general tasks.</p><p><strong>Highlights</strong></p><ul><li>üöÄ <strong>Superior performance across tasks</strong>: Ring-lite-2507 demonstrates outstanding performance across both reasoning and general tasks;</li><li>üî• <strong>Only 2.75B activated parameters</strong>: Ring-lite-2507 is built upon a Mixture-of-Experts (MoE)-based large language model with only 2.75 billion activated parameters;</li><li>‚õìÔ∏è‚Äçüí• <strong>Algorithm-system co-design</strong>: We proposed novel C3PO approach and employ token efficiency to improve training stability and effectiveness;</li><li>üîç <strong>Publicly available</strong>: We fully release our training recipe and model weights.</li></ul><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>We evaluate our models comprehensively across two core domains: reasoning domamin and general domain, using a diverse set of public benchmarks categorized by their primary measured capability.</p><h3 id=knowledge-understanding>Knowledge Understanding<a hidden class=anchor aria-hidden=true href=#knowledge-understanding>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-0731</strong></th><th style=text-align:center><strong>Ring-lite</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>MMLU-Pro (EM)</td><td style=text-align:center>72.50</td><td style=text-align:center>63.44</td><td style=text-align:center><strong>72.56</strong></td></tr><tr><td style=text-align:center>GPQA-Diamond (Pass@1)</td><td style=text-align:center><strong>69.35</strong></td><td style=text-align:center>63.51</td><td style=text-align:center>62.00</td></tr><tr><td style=text-align:center>SuperGPQA (EM)</td><td style=text-align:center>40.05</td><td style=text-align:center>13.97</td><td style=text-align:center><strong>40.36</strong></td></tr><tr><td style=text-align:center>Phybench (Pass@1)</td><td style=text-align:center>28.51</td><td style=text-align:center><strong>29.19</strong></td><td style=text-align:center>22.14</td></tr></tbody></table><h3 id=math>Math<a hidden class=anchor aria-hidden=true href=#math>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>MATH-500 (Pass@1)</td><td style=text-align:center><strong>97.95</strong></td><td style=text-align:center>96.80</td><td style=text-align:center>97.30</td></tr><tr><td style=text-align:center>CNMO 2024 (Pass@1)</td><td style=text-align:center>75.09</td><td style=text-align:center><strong>77.26</strong></td><td style=text-align:center>74.57</td></tr><tr><td style=text-align:center>AIME 2024 (Pass@1)</td><td style=text-align:center><strong>79.79</strong></td><td style=text-align:center>79.00</td><td style=text-align:center>74.90</td></tr><tr><td style=text-align:center>AIME 2025 (Pass@1)</td><td style=text-align:center><strong>72.92</strong></td><td style=text-align:center>69.50</td><td style=text-align:center>67.19</td></tr><tr><td style=text-align:center>LiveMathBench (Pass@1)</td><td style=text-align:center>83.37</td><td style=text-align:center><strong>85.08</strong></td><td style=text-align:center>81.90</td></tr><tr><td style=text-align:center>TheoremQA (Pass@1)</td><td style=text-align:center>70.00</td><td style=text-align:center><strong>70.19</strong></td><td style=text-align:center>68.81</td></tr><tr><td style=text-align:center>OlympiadBench (math) (Pass@1)</td><td style=text-align:center>80.64</td><td style=text-align:center><strong>82.86</strong></td><td style=text-align:center>80.20</td></tr></tbody></table><h3 id=coding>Coding<a hidden class=anchor aria-hidden=true href=#coding>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>LiveCodeBench(2408-2505) (Pass@1)</td><td style=text-align:center><strong>60.35</strong></td><td style=text-align:center>59.53</td><td style=text-align:center>55.12</td></tr><tr><td style=text-align:center>Codeforces(Percentile) (Pass@1)</td><td style=text-align:center><strong>1830</strong></td><td style=text-align:center>1673</td><td style=text-align:center>1580</td></tr><tr><td style=text-align:center>Codeforces(Rating)</td><td style=text-align:center><strong>92.16</strong></td><td style=text-align:center>88.00</td><td style=text-align:center>79.44</td></tr></tbody></table><h3 id=reasoning--agentic>Reasoning & Agentic<a hidden class=anchor aria-hidden=true href=#reasoning--agentic>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>DROP (zero-shot F1)</td><td style=text-align:center><strong>89.27</strong></td><td style=text-align:center>60.21</td><td style=text-align:center>87.13</td></tr><tr><td style=text-align:center>BBH (EM)</td><td style=text-align:center><strong>88.65</strong></td><td style=text-align:center>50.84</td><td style=text-align:center>87.30</td></tr><tr><td style=text-align:center>ARCPrize (Pass@1)</td><td style=text-align:center><strong>19.00</strong></td><td style=text-align:center>3.12</td><td style=text-align:center>3.88</td></tr><tr><td style=text-align:center>MuSR (EM)</td><td style=text-align:center><strong>77.19</strong></td><td style=text-align:center>66.77</td><td style=text-align:center>76.92</td></tr><tr><td style=text-align:center>BFCL_Live (Pass@1)</td><td style=text-align:center>74.81</td><td style=text-align:center>66.76</td><td style=text-align:center><strong>75.99</strong></td></tr></tbody></table><h3 id=alignment>Alignment<a hidden class=anchor aria-hidden=true href=#alignment>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>IFEval (Prompt Strict)</td><td style=text-align:center>84.66</td><td style=text-align:center>54.34</td><td style=text-align:center><strong>85.40</strong></td></tr><tr><td style=text-align:center>AlignBench v1.1(gpt-4.1)</td><td style=text-align:center><strong>80.90</strong></td><td style=text-align:center>69.60</td><td style=text-align:center>74.70</td></tr><tr><td style=text-align:center>FoFo (gpt-4-turbo)</td><td style=text-align:center><strong>85.02</strong></td><td style=text-align:center>67.81</td><td style=text-align:center>81.93</td></tr><tr><td style=text-align:center>ArenaHard (gpt-4.1)</td><td style=text-align:center><strong>88.85</strong></td><td style=text-align:center>56.12</td><td style=text-align:center>86.14</td></tr></tbody></table><h2 id=constrained-contextual-computation-policy-optimizationc3po>Constrained Contextual Computation Policy Optimization(C3PO)<a hidden class=anchor aria-hidden=true href=#constrained-contextual-computation-policy-optimizationc3po>#</a></h2><p>We introduce <u>C</u>onstrained <u>C</u>ontextual <u>C</u>omputation <u>P</u>olicy <u>O</u>ptimization(C3PO), an innovative token-level optimization framework designed to mitigate training instability while enhancing throughput consistency. Different from sampling-level filtering, C3PO operates at the token level by sampling tokens to form a token-level global batch, each training step maintains consistent token input to optimizer, which results in reduced gradient variance and consequently achieving stable optimization.</p><div style=text-align:center;margin:auto;width:100%><img src=./assets/C3PO_overview_formal.png alt="Image description"><p style=font-size:14px;color:gray>C3PO</p></div><h2 id=balancing-token-efficiency-between-distillation-and-rl>Balancing Token efficiency between Distillation and RL<a hidden class=anchor aria-hidden=true href=#balancing-token-efficiency-between-distillation-and-rl>#</a></h2><p>While distillation is effective, we find it requires more training tokens to achieve comparable performance compared to RL training.
Instead, we observe that varying the number of training epochs of the distilled model significantly influences the trend of entropy loss, thereby determining the exploration scope for RL. Based on our experiments, increasing the number of SFT training epochs leads to a rapid collapse of entropy. However, insufficient SFT training inevitably results in inferior performance. To systematically quantify the choice of optimal SFT epoch, we employ token efficiency to determine the suitable checkpoint for RL scaling.</p><h2 id=training-data>Training Data<a hidden class=anchor aria-hidden=true href=#training-data>#</a></h2><p>We follow a stringent data processing pipeline.</p><div style=text-align:center;margin:auto;width:100%><img src=./assets/data-pipeline.png alt="Image description"><p style=font-size:14px;color:gray>Data Pipeline</p></div><h2 id=training-pipeline>Training Pipeline<a hidden class=anchor aria-hidden=true href=#training-pipeline>#</a></h2><div style=text-align:center;margin:auto;width:100%><img src=./assets/0731-pipeline.png alt="Image description"><p style=font-size:14px;color:gray>Training Pipeline</p></div><h3 id=reasoning-rl>Reasoning RL<a hidden class=anchor aria-hidden=true href=#reasoning-rl>#</a></h3><p>Compared to our Ring-lite, we expand our reasoning dataset by incorporating more chanlleging math, coding and STEM dataset. Sepecifically, we adopted 67K math data, 32K coding data, 9.9K scientific data for reasoning RL training. In addition, we amplify our reasoning dataset by including more than 19K logical games, such as ARC-AGI, Countdown, sudoku, AlphaMaze, etc. For each type of problems, we specifically design the suitable reward function to make sure our training examples are verifiable. We apply RL on various reasoning tasks, including math, stem, code, logical games.</p><h3 id=general-rl>General RL<a hidden class=anchor aria-hidden=true href=#general-rl>#</a></h3><p>Except for reasoning tasks, our Ring-lite-0731 has significantly expanded the collection of general RL training dataset. Our general RL does not sacrifices performance on reasoning tasks, instead, it imporved the overal text understanding ability across a broad range of general benchmarks.
Our general RL training incorporates various sources of general tasks, including instruction following, question answering, text summarizaiton, ect. For open-formed question, we adopt a strong reward model to assign reward scores for the problems. Besides, we also incorporated rule-based verifier to tackle problems which can be easily verified, such as the instruction-following problems.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>lingteam2025ringlitescalablereasoningc3postabilized</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>title</span><span class=p>=</span><span class=s>{Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>      <span class=na>author</span><span class=p>=</span><span class=s>{Ling Team and Bin Hu and Cai Chen and Deng Zhao and Ding Liu and Dingnan Jin and Feng Zhu and Hao Dai and Hongzhi Luan and Jia Guo and Jiaming Liu and Jiewei Wu and Jun Mei and Jun Zhou and Junbo Zhao and Junwu Xiong and Kaihong Zhang and Kuan Xu and Lei Liang and Liang Jiang and Liangcheng Fu and Longfei Zheng and Qiang Gao and Qing Cui and Quan Wan and Shaomian Zheng and Shuaicheng Li and Tongkai Yang and Wang Ren and Xiaodong Yan and Xiaopei Wan and Xiaoyun Feng and Xin Zhao and Xinxing Yang and Xinyu Kong and Xuemin Yang and Yang Li and Yingting Wu and Yongkang Liu and Zhankai Xu and Zhenduo Zhang and Zhenglei Zhou and Zhenyu Huang and Zhiqiang Zhang and Zihao Wang and Zujie Wen}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>year</span><span class=p>=</span><span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>eprint</span><span class=p>=</span><span class=s>{2506.14731}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>archivePrefix</span><span class=p>=</span><span class=s>{arXiv}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>primaryClass</span><span class=p>=</span><span class=s>{cs.CL}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>url</span><span class=p>=</span><span class=s>{https://arxiv.org/abs/2506.14731}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
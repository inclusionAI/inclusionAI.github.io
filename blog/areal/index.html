<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>AReaL: Ant Reasoning Reinforcement Learning for LLMs | INCLUSION AI</title><meta name=keywords content><meta name=description content="| Paper | Documentation | Ask DeepWiki | ü§ó Models & Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/areal/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/areal/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="AReaL: Ant Reasoning Reinforcement Learning for LLMs"><meta property="og:description" content="| Paper | Documentation | Ask DeepWiki | ü§ó Models & Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/areal/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-04-01T00:00:03+08:00"><meta property="article:modified_time" content="2025-04-01T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="AReaL: Ant Reasoning Reinforcement Learning for LLMs"><meta name=twitter:description content="| Paper | Documentation | Ask DeepWiki | ü§ó Models & Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"AReaL: Ant Reasoning Reinforcement Learning for LLMs","item":"https://inclusionai.github.io/blog/areal/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"AReaL: Ant Reasoning Reinforcement Learning for LLMs","name":"AReaL: Ant Reasoning Reinforcement Learning for LLMs","description":"| Paper | Documentation | Ask DeepWiki | ü§ó Models \u0026 Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably.","keywords":[],"articleBody":" | Paper | Documentation | Ask DeepWiki | ü§ó Models \u0026 Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably. Our team loves milk tea because it‚Äôs delicious, customizable, and affordable. We hope you enjoy our project just like how you enjoy real-world milk tea (cheers).\nAReaL Highlights\nüî• [NEW] Asynchronous RL: With algorithm-system co-design, AReaL supports fully asynchronous RL for the fastest training! Experimental support for multi-turn agentic RL is also provided. üõ†Ô∏è Open \u0026 Reproducible: We continuously release all code, datasets, and training recipes for RL training of LLMs. üöÄ Scalability: AReaL can seamlessly adapt to different computational resource settings, ranging from a single node to 1K GPUs. üî™ Cutting-Edge Performance: AReaL can produce models with cutting-edge reasoning capabilities in math and coding. We are also actively working on agentic tasks. News [2025/06/03] (v0.3, boba¬≤) We release boba¬≤ (double-boba) for fully asynchronous RL training, which achieves a 2.77x speedup while obtaining on-par or even better training performance compared to synchronous systems. Moreover, asynchronous RL makes it extremely easy to set up multi-turn agentic RL training! Check out our v0.3 overview blog and the research paper.\n[2025/03/31] (v0.2, boba) Here comes our next milestone release - boba! Please call it A-ReaL-boba! This release includes much faster training with SGLang support and SOTA 7B and 32B models on math reasoning. Check our v0.2 technical blog.\n[2025/02/24] (v0.1) Our initial release includes reproducible results for 1.5B and 7B LRMs. Check our v0.1 technical blog.\nRelease Highlights In our AReaL-boba¬≤ (A-ReaL-double-boba) release, we highlight the top 3 most important features:\nA fully asynchronous RL training pipeline with system and RL algorithm co-design, achieving over 2.77x speedup without any performance drop. Check the benchmark scripts and instructions here.\nSOTA coding models, i.e., a 14B model with a 69.1 score on LCB-v5. To reproduce, check the configs and instructions.\nExperimental support for multi-turn agentic RL training. Check our complete example.\nFor the complete system design and more training details, please check our v0.3 blog and our research paper.\nJump to the quickstart section if you want to quickly run an experiment and get your hands dirty! üòà\nOverview of Asynchronous RL Training During the synchronous RL training process, a generation step must wait until the longest sequence completes within the batch of LLM outputs. Due to the varying output lengths for LRMs, a synchronous RL system suffers from massive GPU idle time, leading to training inefficiency. Some recent works (DeepCoder, Intellect) propose overlapping a single training step with a single generation step to accelerate training. However, the largest bottleneck remains unchanged: the samples within a batch are still from the same model version, leading to waiting and GPU idle time.\nFig.1. Left: Execution timeline of synchronous RL training. Right: Execution timeline of one-step overlap RL system.\nAReaL adopts a fully asynchronous RL training framework that completely decouples generation from training. In AReaL, LLM generation runs in a streaming manner, with each rollout worker continuously producing outputs without waiting. Meanwhile, trainer workers perform parallel model updates upon receiving training batches.\nFig 2. Execution timeline of our fully asynchronous RL system.\nAReaL follows a system-algorithm co-design principle: on the system side, AReaL efficiently syncs model parameters and carefully controls the staleness of each training sample; on the algorithm side, AReaL improves the objective of PPO to make async-RL stable.\nWe compare the scalability of asynchronous RL training based on our AReaL-boba¬≤ system with classical synchronous RL training (we adopt the fastest open-source system veRL, main branch on 05/07/2025) across different model sizes and different numbers of H800 GPUs. AReaL demonstrates much improved scaling capabilities with respect to training throughput. This is also partially due to AReaL decoupling training and generation, leading to much fewer GPU memory fragments.\nFig.3 The scaling trend of asynchronous RL (based on AReaL-boba2) and classical synchronous RL (based on veRL) with different model sizes. Dotted lines indicate ideal linear scaling.\nSOTA Code Generation Model by AReaL-boba¬≤ We use Qwen3 as our base model. After asynchronous RL training, we achieve SOTA results on LiveCodeBench, Codeforces, and CodeContests benchmarks.\nModel (8B) LiveCodeBench v5(2024.10-2025.2) Codeforces CodeContests Qwen3-8B 58.8 1879/96.7% 31.4 DeepSeek-R1-0528-Qwen3-8B 58.4 1945/97.3% 31.0 ü§ó AReaL-boba¬≤-8B-Open 62.0 1933/97.2% 41.4 ü§ó AReaL-boba¬≤-8B 63.0 1962/97.5% 40.8 Model (14B) LiveCodeBench v5(2024.10-2025.2) Codeforces CodeContests Qwen3-14B 65.4 1978/97.7% 38.3 DeepCoder-14B-Preview 60.6 1936/95.3% 40.1 ü§ó AReaL-boba¬≤-14B-Open 67.3 1990/97.8% 46.2 ü§ó AReal-boba¬≤-14B 69.1 2044/98.2% 46.1 Larger Models LiveCodeBench v5(2024.10-2025.2) Codeforces CodeContests Qwen3-235B 70.7 2056 - DeepSeek-R1 64.3 2029 - OpenAI-o3-mini (Medium) 66.3 2036 - Table 1: Coding Task Performance Comparison. AReaL-boba¬≤-8B/14B-Open denotes training results on open-source data. AReaL-boba¬≤-8B/14B models are trained with an additional small amount of internal data and achieve SOTA performance on LiveCodeBench, Codeforces \u0026 CodeContests.\nWe highlight the tutorials and code walkthroughs about the following key features for asynchronous training:\nStreaming generation and reward computation Interruptible rollout Data staleness control with the rollout controller The adoption of decoupled PPO loss RL Training for Multi-turn Agent AReaL-boba¬≤ allows you to independently customize the dataset, rollout behavior, and the training algorithm, without needing to modify the heavy system-level code.\nIn particular, we show a simple example to develop a multi-turn math agent for RL training. Please see the learning curve below and reference the step-by-step guide if you want to implement your own agentic RL project.\nGetting Started Obtain the training data:\nMath Code For code training data, a simple preprocessing script was provided in examples/data_preprocess/preprocess_training_data.py:\npython3 preprocess_training_data.py --data_path $original_data_path --output_path $training_data_path Train Qwen3 1.7B locally (Remember to modify dataset.path in the script below):\nbash examples/run_async_ppo.sh Evaluation:\ncd evaluation # Evaluate the model python eval_and_aggregate.py \\ --model_path ${MODEL_PATH} \\ --output_path ${OUTPUT_PATH} \\ --data_names aime24,aime25 \\ --max_gen_tokens 32768 \\ --data_names codeforces,lcb_v5 \\ --prompt_type qwen3-think-pure \\ --temperature 1.0 Resources Documentation Contributing Quickstart Installation Example: Improving the math capability of Qwen3 with PPO Benchmark and Reproduction Reproduce boba¬≤ Code Models ü§ó Model weights: 8B-code, 14B-code, 8B-code-open, 14B-code-open Evaluation Guide Training configs and instructions Scripts for Benchmark Training Throughput Customization Guide Use your own dataset Modifying the reward function and rollout behavior (multi-turn agentic RL) Modifying PPO to GRPO Developing the decoupled PPO loss System Code Walkthrough Trainer Model Backend and Algorithm Interface Rollout Controller Streaming generation and reward computation Future Plan AReaL is under active development. We plan to have minor releases weekly and major releases monthly. Community engagement and contributions are extremely welcome. We are also hiring interns and full-time employees with open positions in both the US and China.\nFor the research and development plan already in place, please see the following list:\nSystem Development Support for SGLang RL training with coding problems Asynchronous generation and RL training Optimizations for distributed training: expert parallel for MOE and zero-bubble pipelining RL for vision-language models (VLM) Multi-turn agentic RL Function calling and tool use Algorithm Development RL training recipes for 1.5B and 7B models A complete RL training recipe for 32B models Sample-efficient multi-task RL algorithms Agentic capabilities with end-to-end RL Stable RL training for larger MOE models Acknowledgement We would like to note that major contributors are from the RL Lab at Ant Research and the Institute for Interdisciplinary Information Sciences, Tsinghua University.\nOur team has also received invaluable assistance from the Data Intelligence Lab at Ant Research for data support and from the Super Computing Technology (SCT) team at Ant Group, particularly in the realm of large-scale cluster operations and maintenance.\nWe also appreciate all the pioneering works from the community, particularly the ReaLHF project from OpenPsi Inc. and other projects, including but not limited to DeepScaleR, Open-Reasoner-Zero, OpenRLHF, VeRL, SGLang, QwQ, Light-R1 and DAPO.\nCitation @inproceedings{mei2025real, author = {Mei, Zhiyu and Fu, Wei and Li, Kaiwei and Wang, Guangju and Zhang, Huanchen and Wu, Yi}, title = {ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation}, booktitle = {Proceedings of the Eighth Conference on Machine Learning and Systems, MLSys 2025, Santa Clara, CA, USA, May 12-15, 2025}, publisher = {mlsys.org}, year = {2025}, } @misc{fu2025areal, title={AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning}, author={Wei Fu and Jiaxuan Gao and Xujie Shen and Chen Zhu and Zhiyu Mei and Chuyi He and Shusheng Xu and Guo Wei and Jun Mei and Jiashu Wang and Tongkai Yang and Binhang Yuan and Yi Wu}, year={2025}, eprint={2505.24298}, archivePrefix={arXiv}, primaryClass={cs.LG}, url={https://arxiv.org/abs/2505.24298}, } ","wordCount":"1431","inLanguage":"en","datePublished":"2025-04-01T00:00:03+08:00","dateModified":"2025-04-01T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/areal/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>AReaL: Ant Reasoning Reinforcement Learning for LLMs</h1><div class=post-meta><span title='2025-04-01 00:00:03 +0800 +0800'>April 1, 2025</span>&nbsp;¬∑&nbsp;7 min&nbsp;¬∑&nbsp;1431 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group</div></div></div><main class=main><article class=post-single><div class=post-content><p align=center>| <a href=https://arxiv.org/pdf/2505.24298><b>Paper</b></a> | <a href=https://inclusionai.github.io/AReaL/><b>Documentation</b></a> | <a href=https://deepwiki.com/inclusionAI/AReaL><b>Ask DeepWiki</b></a> | <a href=https://huggingface.co/collections/inclusionAI/areal-boba-2-683f0e819ccb7bb2e1b2f2d5><b>ü§ó Models & Data</b></a> |
<a href=https://github.com/inclusionAI/AReaL/blob/main/assets/wechat_qrcode.png target=_blank><b>WeChat Group</b></a> |</p><p>AReaL (Ant Reasoning RL) is an open-source <strong>fully asynchronous reinforcement learning training system</strong> for large reasoning models developed at <strong>the RL Lab, Ant Research</strong>. Built upon the open-source project <a href=https://github.com/openpsi-project/ReaLHF>RealHF</a>, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably. Our team loves milk tea because it&rsquo;s delicious, customizable, and affordable. We hope you enjoy our project just like how you enjoy real-world milk tea (cheers).</p><p><strong>AReaL Highlights</strong></p><ul><li>üî• <span style=color:red;font-weight:700><strong>[NEW] Asynchronous RL:</strong></span> With algorithm-system co-design, AReaL supports fully asynchronous RL for <strong>the fastest training</strong>! Experimental support for multi-turn agentic RL is also provided.</li><li>üõ†Ô∏è <strong>Open & Reproducible</strong>: We continuously release <em>all code, datasets, and training recipes</em> for RL training of LLMs.</li><li>üöÄ <strong>Scalability</strong>: AReaL can seamlessly adapt to different computational resource settings, ranging from a single node to 1K GPUs.</li><li>üî™ <strong>Cutting-Edge Performance:</strong> AReaL can produce models with cutting-edge reasoning capabilities in math and coding. We are also actively working on agentic tasks.</li></ul><h2 id=news>News<a hidden class=anchor aria-hidden=true href=#news>#</a></h2><p><strong>[2025/06/03] (v0.3, boba¬≤)</strong> We release <strong>boba¬≤</strong> (double-boba) for fully asynchronous RL training, which achieves a <strong>2.77x speedup while obtaining on-par or even better training performance</strong> compared to synchronous systems. Moreover, asynchronous RL makes it extremely easy to set up multi-turn agentic RL training! Check out <a href=https://github.com/inclusionAI/AReaL/blob/main/blog/AReaL_v0_3.md>our v0.3 overview blog</a> and the <a href=https://arxiv.org/pdf/2505.24298>research paper</a>.</p><p><strong>[2025/03/31] (v0.2, boba)</strong> Here comes our next milestone release - boba! Please call it A-ReaL-boba! This release includes much faster training with SGLang support and SOTA 7B and 32B models on math reasoning. Check our <a href=https://github.com/inclusionAI/AReaL/blob/main/blog/AReaL_v0_2.md>v0.2 technical blog</a>.</p><p><strong>[2025/02/24] (v0.1)</strong> Our initial release includes reproducible results for 1.5B and 7B LRMs. Check our <a href=https://github.com/inclusionAI/AReaL/blob/main/blog/AReaL_v0_1.md>v0.1 technical blog</a>.</p><h2 id=release-highlights>Release Highlights<a hidden class=anchor aria-hidden=true href=#release-highlights>#</a></h2><p>In our AReaL-boba¬≤ (A-ReaL-double-boba) release, we highlight the top 3 most important features:</p><ul><li><p>A fully asynchronous RL training pipeline with <strong>system and RL algorithm co-design</strong>, achieving over 2.77x speedup without any performance drop. Check the <a href=https://github.com/inclusionAI/AReaL/tree/main/benchmark/verl_v0_3_0_post1_76084d3>benchmark scripts and instructions here</a>.</p></li><li><p>SOTA coding models, i.e., a 14B model with a <strong>69.1 score on LCB-v5</strong>. To reproduce, check the <a href=https://github.com/inclusionAI/AReaL/tree/main/examples/configs/v0.3-qwen3-code>configs</a> and <a href=https://inclusionai.github.io/AReaL/references/reproduce.html>instructions</a>.</p></li><li><p>Experimental support for <strong>multi-turn</strong> agentic RL training. Check our <a href=https://inclusionai.github.io/AReaL/customization/agent.html>complete example</a>.</p></li></ul><p>For the complete system design and more training details, please check <a href=https://github.com/inclusionAI/AReaL/blob/main/blog/AReaL_v0_3.md>our v0.3 blog</a> and our <a href=https://arxiv.org/pdf/2505.24298>research paper</a>.</p><p><strong>Jump to the <a href="https://github.com/inclusionAI/AReaL?tab=readme-ov-file#getting-started">quickstart section</a> if you want to quickly run an experiment and get your hands dirty!</strong> üòà</p><h3 id=overview-of-asynchronous-rl-training>Overview of Asynchronous RL Training<a hidden class=anchor aria-hidden=true href=#overview-of-asynchronous-rl-training>#</a></h3><p>During the synchronous RL training process, a generation step must wait until the longest sequence completes within the batch of LLM outputs. Due to the varying output lengths for LRMs, a synchronous RL system suffers from massive GPU idle time, leading to training inefficiency. Some recent works (<a href=https://pretty-radio-b75.notion.site/DeepCoder-A-Fully-Open-Source-14B-Coder-at-O3-mini-Level-1cf81902c14680b3bee5eb349a512a51>DeepCoder</a>, <a href=https://www.primeintellect.ai/blog/intellect-2>Intellect</a>) propose overlapping a single training step with a single generation step to accelerate training. However, the largest bottleneck remains unchanged: the samples within a batch are still from the same model version, leading to waiting and GPU idle time.</p><p><img loading=lazy src=https://github.com/inclusionAI/AReaL/raw/main/assets/sync_one_step_gen.png alt="Synchronous vs One-step Overlap RL"></p><p><em>Fig.1. Left: Execution timeline of synchronous RL training. Right: Execution timeline of one-step overlap RL system.</em></p><p>AReaL adopts a fully asynchronous RL training framework that completely decouples generation from training. In AReaL, LLM generation runs in a streaming manner, with each rollout worker continuously producing outputs without waiting. Meanwhile, trainer workers perform parallel model updates upon receiving training batches.</p><p><img loading=lazy src=https://github.com/inclusionAI/AReaL/raw/main/assets/async_timeline.png alt="Asynchronous RL Training"></p><p><em>Fig 2. Execution timeline of our fully asynchronous RL system.</em></p><p>AReaL follows a system-algorithm co-design principle: on the system side, AReaL efficiently syncs model parameters and carefully controls the staleness of each training sample; on the algorithm side, AReaL improves the objective of PPO to make async-RL stable.</p><p>We compare the scalability of <strong>asynchronous RL</strong> training based on our AReaL-boba¬≤ system with <strong>classical synchronous RL</strong> training (we adopt the fastest open-source system veRL, main branch on 05/07/2025) across different model sizes and different numbers of H800 GPUs. AReaL demonstrates much improved scaling capabilities with respect to training throughput. This is also partially due to AReaL decoupling training and generation, leading to much fewer GPU memory fragments.</p><p><img loading=lazy src=https://github.com/inclusionAI/AReaL/raw/main/assets/async_scaling_vs_verl.png alt="Scaling Comparison"></p><p><em>Fig.3 The scaling trend of asynchronous RL (based on AReaL-boba2) and classical synchronous RL (based on veRL) with different model sizes. Dotted lines indicate ideal linear scaling.</em></p><h3 id=sota-code-generation-model-by-areal-boba>SOTA Code Generation Model by AReaL-boba¬≤<a hidden class=anchor aria-hidden=true href=#sota-code-generation-model-by-areal-boba>#</a></h3><p>We use <strong>Qwen3</strong> as our base model. After asynchronous RL training, we achieve SOTA results on LiveCodeBench, Codeforces, and CodeContests benchmarks.</p><table><thead><tr><th style=text-align:center><strong>Model (8B)</strong></th><th style=text-align:center><strong>LiveCodeBench v5</strong><br><strong>(2024.10-2025.2)</strong></th><th style=text-align:center><strong>Codeforces</strong></th><th style=text-align:center><strong>CodeContests</strong></th></tr></thead><tbody><tr><td style=text-align:center>Qwen3-8B</td><td style=text-align:center>58.8</td><td style=text-align:center>1879/96.7%</td><td style=text-align:center>31.4</td></tr><tr><td style=text-align:center>DeepSeek-R1-0528-Qwen3-8B</td><td style=text-align:center>58.4</td><td style=text-align:center>1945/97.3%</td><td style=text-align:center>31.0</td></tr><tr><td style=text-align:center><a href=https://huggingface.co/inclusionAI/AReaL-boba-2-8B-subset>ü§ó AReaL-boba¬≤-8B-Open</a></td><td style=text-align:center>62.0</td><td style=text-align:center>1933/97.2%</td><td style=text-align:center><strong>41.4</strong></td></tr><tr><td style=text-align:center><a href=https://huggingface.co/inclusionAI/AReaL-boba-2-8B>ü§ó AReaL-boba¬≤-8B</a></td><td style=text-align:center><strong>63.0</strong></td><td style=text-align:center><strong>1962/97.5%</strong></td><td style=text-align:center>40.8</td></tr></tbody></table><table><thead><tr><th style=text-align:center><strong>Model (14B)</strong></th><th style=text-align:center><strong>LiveCodeBench v5</strong><br><strong>(2024.10-2025.2)</strong></th><th style=text-align:center><strong>Codeforces</strong></th><th style=text-align:center><strong>CodeContests</strong></th></tr></thead><tbody><tr><td style=text-align:center>Qwen3-14B</td><td style=text-align:center>65.4</td><td style=text-align:center>1978/97.7%</td><td style=text-align:center>38.3</td></tr><tr><td style=text-align:center>DeepCoder-14B-Preview</td><td style=text-align:center>60.6</td><td style=text-align:center>1936/95.3%</td><td style=text-align:center>40.1</td></tr><tr><td style=text-align:center><a href=https://huggingface.co/inclusionAI/AReaL-boba-2-14B-subset>ü§ó AReaL-boba¬≤-14B-Open</a></td><td style=text-align:center>67.3</td><td style=text-align:center>1990/97.8%</td><td style=text-align:center><strong>46.2</strong></td></tr><tr><td style=text-align:center><a href=https://huggingface.co/inclusionAI/AReaL-boba-2-14B>ü§ó AReal-boba¬≤-14B</a></td><td style=text-align:center><strong>69.1</strong></td><td style=text-align:center><strong>2044/98.2%</strong></td><td style=text-align:center>46.1</td></tr></tbody></table><table><thead><tr><th style=text-align:center><strong>Larger Models</strong></th><th style=text-align:center><strong>LiveCodeBench v5</strong><br><strong>(2024.10-2025.2)</strong></th><th style=text-align:center><strong>Codeforces</strong></th><th style=text-align:center><strong>CodeContests</strong></th></tr></thead><tbody><tr><td style=text-align:center>Qwen3-235B</td><td style=text-align:center>70.7</td><td style=text-align:center>2056</td><td style=text-align:center>-</td></tr><tr><td style=text-align:center>DeepSeek-R1</td><td style=text-align:center>64.3</td><td style=text-align:center>2029</td><td style=text-align:center>-</td></tr><tr><td style=text-align:center>OpenAI-o3-mini (Medium)</td><td style=text-align:center>66.3</td><td style=text-align:center>2036</td><td style=text-align:center>-</td></tr></tbody></table><p><em>Table 1: Coding Task Performance Comparison. AReaL-boba¬≤-8B/14B-Open denotes training results on open-source data. AReaL-boba¬≤-8B/14B models are trained with an additional small amount of internal data and achieve SOTA performance on LiveCodeBench, Codeforces & CodeContests.</em></p><p>We highlight the <a href=https://inclusionai.github.io/AReaL/customization/dataset.html>tutorials</a> and <a href=https://inclusionai.github.io/AReaL/developer/overview.html>code walkthroughs</a> about the following key features for asynchronous training:</p><ul><li><a href=https://inclusionai.github.io/AReaL/developer/rollout/rollout_worker.html>Streaming generation and reward computation</a></li><li><a href=https://inclusionai.github.io/AReaL/developer/rollout/gserver.html>Interruptible rollout</a></li><li><a href=https://inclusionai.github.io/AReaL/developer/rollout/gserver.html>Data staleness control with the rollout controller</a></li><li><a href=https://inclusionai.github.io/AReaL/customization/algorithm.html>The adoption of decoupled PPO loss</a></li></ul><h3 id=rl-training-for-multi-turn-agent>RL Training for Multi-turn Agent<a hidden class=anchor aria-hidden=true href=#rl-training-for-multi-turn-agent>#</a></h3><p>AReaL-boba¬≤ allows you to independently customize the <a href=https://inclusionai.github.io/AReaL/customization/dataset.html>dataset</a>, <a href=https://inclusionai.github.io/AReaL/customization/agent.html>rollout behavior</a>, and the <a href=https://inclusionai.github.io/AReaL/customization/algorithm.html>training algorithm</a>, without needing to modify the heavy system-level code.</p><p>In particular, we show a simple example to develop a multi-turn math agent for RL training. Please see the learning curve below and reference the <a href=https://inclusionai.github.io/AReaL/customization/agent.html>step-by-step guide</a> if you want to implement your own agentic RL project.</p><h2 id=getting-started>Getting Started<a hidden class=anchor aria-hidden=true href=#getting-started>#</a></h2><p>Obtain the training data:</p><ul><li><a href=https://huggingface.co/datasets/inclusionAI/AReaL-boba-Data>Math</a></li><li><a href=https://huggingface.co/datasets/inclusionAI/AReaL-boba-2-RL-Code>Code</a></li></ul><p>For code training data, a simple preprocessing script was provided in <code>examples/data_preprocess/preprocess_training_data.py</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>python3 preprocess_training_data.py --data_path <span class=nv>$original_data_path</span> --output_path <span class=nv>$training_data_path</span>
</span></span></code></pre></div><p>Train Qwen3 1.7B locally (Remember to modify <code>dataset.path</code> in the script below):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>bash examples/run_async_ppo.sh
</span></span></code></pre></div><p>Evaluation:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> evaluation
</span></span><span class=line><span class=cl><span class=c1># Evaluate the model</span>
</span></span><span class=line><span class=cl>python eval_and_aggregate.py <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --model_path <span class=si>${</span><span class=nv>MODEL_PATH</span><span class=si>}</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --output_path <span class=si>${</span><span class=nv>OUTPUT_PATH</span><span class=si>}</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data_names aime24,aime25 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --max_gen_tokens <span class=m>32768</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --data_names codeforces,lcb_v5 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --prompt_type qwen3-think-pure <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>  --temperature 1.0
</span></span></code></pre></div><h2 id=resources>Resources<a hidden class=anchor aria-hidden=true href=#resources>#</a></h2><ul><li><a href=https://inclusionai.github.io/AReaL/>Documentation</a></li><li><a href=https://inclusionai.github.io/AReaL/contrib.html>Contributing</a></li></ul><h3 id=quickstart>Quickstart<a hidden class=anchor aria-hidden=true href=#quickstart>#</a></h3><ul><li><a href=https://inclusionai.github.io/AReaL/tutorial/installation.html>Installation</a></li><li><a href=https://inclusionai.github.io/AReaL/tutorial/quickstart.html>Example: Improving the math capability of Qwen3 with PPO</a></li></ul><h3 id=benchmark-and-reproduction>Benchmark and Reproduction<a hidden class=anchor aria-hidden=true href=#benchmark-and-reproduction>#</a></h3><ul><li><strong>Reproduce boba¬≤ Code Models</strong><ul><li>ü§ó <strong>Model weights</strong>: <a href=https://huggingface.co/inclusionAI/AReaL-boba-2-8B>8B-code</a>, <a href=https://huggingface.co/inclusionAI/AReaL-boba-2-14B>14B-code</a>, <a href=https://huggingface.co/inclusionAI/AReaL-boba-2-8B-subset>8B-code-open</a>, <a href=https://huggingface.co/inclusionAI/AReaL-boba-2-14B-subset>14B-code-open</a></li><li><a href=https://inclusionai.github.io/AReaL/tutorial/eval.html>Evaluation Guide</a></li><li><a href=https://github.com/inclusionAI/AReaL/tree/main/examples/configs/v0.3-qwen3-code>Training configs</a> and <a href=https://inclusionai.github.io/AReaL/references/reproduce.html>instructions</a></li></ul></li><li><a href=https://github.com/inclusionAI/AReaL/tree/main/benchmark/verl_v0_3_0_post1_76084d3>Scripts for Benchmark Training Throughput</a></li></ul><h3 id=customization-guide>Customization Guide<a hidden class=anchor aria-hidden=true href=#customization-guide>#</a></h3><ul><li><a href=https://inclusionai.github.io/AReaL/customization/dataset.html>Use your own dataset</a></li><li><a href=https://inclusionai.github.io/AReaL/customization/agent.html>Modifying the reward function and rollout behavior (multi-turn agentic RL)</a></li><li><a href=https://inclusionai.github.io/AReaL/customization/algorithm.html#grouped-advantage-normalization>Modifying PPO to GRPO</a></li><li><a href=https://inclusionai.github.io/AReaL/customization/algorithm.html#the-decoupled-ppo-loss>Developing the decoupled PPO loss</a></li></ul><h3 id=system-code-walkthrough>System Code Walkthrough<a hidden class=anchor aria-hidden=true href=#system-code-walkthrough>#</a></h3><ul><li><a href=https://inclusionai.github.io/AReaL/developer/trainer/model_worker.html>Trainer</a></li><li><a href=https://inclusionai.github.io/AReaL/developer/trainer/algo_interface.html>Model Backend and Algorithm Interface</a></li><li><a href=https://inclusionai.github.io/AReaL/developer/rollout/gserver.html>Rollout Controller</a></li><li><a href=https://inclusionai.github.io/AReaL/developer/rollout/rollout_worker.html>Streaming generation and reward computation</a></li></ul><h2 id=future-plan>Future Plan<a hidden class=anchor aria-hidden=true href=#future-plan>#</a></h2><p>AReaL is under active development. We plan to have minor releases weekly and major releases monthly. Community engagement and contributions are extremely welcome. We are also <strong>hiring interns and full-time employees</strong> with open positions in both the US and China.</p><p>For the research and development plan already in place, please see the following list:</p><h3 id=system-development>System Development<a hidden class=anchor aria-hidden=true href=#system-development>#</a></h3><ul><li><input checked disabled type=checkbox> Support for SGLang</li><li><input checked disabled type=checkbox> RL training with coding problems</li><li><input checked disabled type=checkbox> Asynchronous generation and RL training</li><li><input disabled type=checkbox> Optimizations for distributed training: expert parallel for MOE and zero-bubble pipelining</li><li><input disabled type=checkbox> RL for vision-language models (VLM)</li><li><input checked disabled type=checkbox> Multi-turn agentic RL</li><li><input disabled type=checkbox> Function calling and tool use</li></ul><h3 id=algorithm-development>Algorithm Development<a hidden class=anchor aria-hidden=true href=#algorithm-development>#</a></h3><ul><li><input checked disabled type=checkbox> RL training recipes for 1.5B and 7B models</li><li><input checked disabled type=checkbox> A complete RL training recipe for 32B models</li><li><input disabled type=checkbox> Sample-efficient multi-task RL algorithms</li><li><input disabled type=checkbox> Agentic capabilities with end-to-end RL</li><li><input disabled type=checkbox> Stable RL training for larger MOE models</li></ul><h2 id=acknowledgement>Acknowledgement<a hidden class=anchor aria-hidden=true href=#acknowledgement>#</a></h2><p>We would like to note that major contributors are from the RL Lab at Ant Research and the Institute for Interdisciplinary Information Sciences, Tsinghua University.</p><p>Our team has also received invaluable assistance from the Data Intelligence Lab at Ant Research for data support and from the Super Computing Technology (SCT) team at Ant Group, particularly in the realm of large-scale cluster operations and maintenance.</p><p>We also appreciate all the pioneering works from the community, particularly the <a href=https://github.com/openpsi-project/ReaLHF>ReaLHF</a> project from OpenPsi Inc. and other projects, including but not limited to <a href=https://github.com/agentica-project/deepscaler>DeepScaleR</a>, <a href=https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main>Open-Reasoner-Zero</a>, <a href=https://github.com/OpenRLHF/OpenRLHF>OpenRLHF</a>, <a href=https://github.com/volcengine/verl>VeRL</a>, <a href=https://github.com/sgl-project/sglang>SGLang</a>, <a href=https://github.com/QwenLM/QwQ>QwQ</a>, <a href=https://github.com/Qihoo360/Light-R1>Light-R1</a> and <a href=https://github.com/BytedTsinghua-SIA/DAPO>DAPO</a>.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@inproceedings</span><span class=p>{</span><span class=nl>mei2025real</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>       <span class=p>=</span> <span class=s>{Mei, Zhiyu and Fu, Wei and Li, Kaiwei and Wang, Guangju and Zhang, Huanchen and Wu, Yi}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>        <span class=p>=</span> <span class=s>{ReaL: Efficient RLHF Training of Large Language Models with Parameter Reallocation}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>booktitle</span>    <span class=p>=</span> <span class=s>{Proceedings of the Eighth Conference on Machine Learning and Systems,
</span></span></span><span class=line><span class=cl><span class=s>                  MLSys 2025, Santa Clara, CA, USA, May 12-15, 2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>publisher</span>    <span class=p>=</span> <span class=s>{mlsys.org}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>         <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>fu2025areal</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>title</span><span class=p>=</span><span class=s>{AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>      <span class=na>author</span><span class=p>=</span><span class=s>{Wei Fu and Jiaxuan Gao and Xujie Shen and Chen Zhu and Zhiyu Mei and Chuyi He and Shusheng Xu and Guo Wei and Jun Mei and Jiashu Wang and Tongkai Yang and Binhang Yuan and Yi Wu}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>year</span><span class=p>=</span><span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>eprint</span><span class=p>=</span><span class=s>{2505.24298}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>archivePrefix</span><span class=p>=</span><span class=s>{arXiv}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>primaryClass</span><span class=p>=</span><span class=s>{cs.LG}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>url</span><span class=p>=</span><span class=s>{https://arxiv.org/abs/2505.24298}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
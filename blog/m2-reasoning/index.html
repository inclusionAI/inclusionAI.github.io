<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning | INCLUSION AI</title><meta name=keywords content><meta name=description content="ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/m2-reasoning/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/m2-reasoning/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/m2-reasoning/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning"><meta property="og:description" content="ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/m2-reasoning/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-11T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-11T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning"><meta name=twitter:description content="ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning","item":"https://inclusionai.github.io/blog/m2-reasoning/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning","name":"M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning","description":"ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope\nIntroduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals.","keywords":[],"articleBody":"ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope\nIntroduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals. This combination of curated data and advanced training allows M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks, showcasing superior performance in both general and spatial reasoning domains. ðŸ“Œ Updates [2025.07.14] ðŸ”¥ Our Technical Report is in public on arxiv. [2025.07.11] ðŸ”¥ We release M2-Reasoning on ðŸ¤— Hugging Face and ðŸ¤– ModelScope. Key Features A High-quality Data Construction Pipeline: We design and implement a multi-stage data synthesis and curation pipeline that generates vast amounts of reasoning data. A Dynamic Multi-Task Training Strategy: We propose a sophisticated training strategy that effectively handles data heterogeneity. It features step-wise dynamic optimization to mitigate conflicts between different data sources and a task-specific reward formulation to provide tailored incentive signals. Unified General and Spatial Reasoning Model: We propose M2-Reasoning-7B, an MLLM uniquely engineered for both abstract and spatial reasoning. Extensive evaluations on 8 distinctbenchmarks demonstrate that, by leveraging our custom data and training pipelines, M2-Reasoning establishes new state-of-the-art (SOTA) results across both general and spatial reasoning domains. Evaluation We conduct a comprehensive evaluation of our models across two key domains: general and spatial reasoning. Our evaluation utilizes a diverse set of public benchmarks, grouped by the primary capability they measure:\nGeneral Reasoning (Mathematical \u0026 Logical): To evaluate this capability, we employ six benchmarks: MathVista, MathVision, MathVerse, DynaMath, WeMath, and LogicVista. Models MathVista MathVision MathVerse DynaMath WeMath LogicVista Avg. (Î”) Base-Scale General Models InternVL3-8B 70.5 30.0 38.5 25.7 39.5 44.5 41.4 InternVL3-9B 69.0 29.3 37.9 25.1 34.8 49.0 40.8 Qwen2.5-VL-7B 68.1 25.4 41.1 21.8 36.2 47.9 40.1 MUG-U-7B 74.8 26.1 35.4 17.2 26.5 39.8 36.6 SAIL-VL-1.6-8B 74.2 23.2 33.4 14.0 29.6 41.4 36.0 Base-Scale Reasoning Models WeThink-VL-7B 71.6 26.0 44.2 24.8 48.0 51.2 44.3 (+4.2) Taichu-VLR-7B 72.3 27.1 46.7 23.0 44.0 48.3 43.6 VLAA-Thinker-7B 68.0 26.4 48.2 22.4 41.5 48.5 42.5 (+2.4) URSA-8B-PS-GRPO 67.8 31.8 41.5 22.4 38.3 44.7 41.1 (+8.2) Ovis2-8B 71.8 25.9 42.3 20.4 27.2 39.4 37.8 Our Models Base Model 70.2 25.9 30.5 20.2 27.2 37.8 35.5 M2-Reasoning-CI-7B 71.7 29.2 42.1 25.0 42.8 46.8 42.9 (+7.4) M2-Reasoning-7B 75.0 31.5 44.7 26.8 41.8 50.0 45.0 (+9.5) Spatial Reasoning: We assess this skill using 2 benchmarks: CV-Bench and VSI-Bench\nCV-Bench: Models Count Relation Depth Distance Avg. Large-Scale Models GPT-4O 65.9 85.7 87.8 78.2 78.9 Gemini-1.5-pro 70.4 85.2 82.4 72.8 77.4 Base-Scale Models InternVL3-8B 74.0 90.6 84.3 81.0 82.0 Qwen2.5-VL-7B-Instruct 65.2 86.6 70.6 79.8 75.0 LLava-NEXT-Video-7B 59.3 77.0 71.3 54.7 65.2 Our Models M2-Reasoning-7B 66.6 92.8 89.3 84.3 82.3 VSI-Bench: OC AD OS RS RDs RDr RP AO Avg. Large-Scale Models Gemini-1.5-pro 56.2 30.9 64.1 43.6 51.3 46.3 36.0 34.6 45.4 GPT-4O 46.2 5.3 43.8 38.2 37.0 41.3 31.5 28.5 34.0 Base-Scale Models InternVL3-8B 68.1 39.0 48.4 33.6 48.3 36.4 27.3 35.4 42.1 Video-R1-7B - - - - - - - - 37.1 Qwen2.5-VL-7B-Instruct 37.7 20.1 49.7 37.4 38.5 40.4 31.4 32.0 35.9 LLava-NeXT-Video-7B 48.5 14.0 47.8 24.2 43.5 42.4 34.0 30.6 35.6 Our Models M2-Reasoning-7B 41.0 34.0 60.9 55.4 40.7 47.3 29.9 28.8 42.3 Model Downloads You can download the model from both Hugging Face and ModelScope.\nIf youâ€™re in mainland China, we strongly recommend you to download our model from ModelScope.\nExample Usage The basic environment is python=3.10, torch=2.6.0+cu124, transformers=4.49.0\nWe provide a small example on the usage of this repo.\nimport os import torch from transformers import ( AutoProcessor, AutoTokenizer, ) import warnings import argparse from modeling_bailing_qwen2_5 import Bailing_qwen2_5NativeForConditionalGeneration from processing_bailing_qwen2_5 import Bailing_qwen2_5Processor warnings.filterwarnings(\"ignore\") class BailingMMInfer: def __init__(self, model_name_or_path, device=\"cuda\", max_pixels=None, min_pixels=None, video_max_pixels=768 * 28 * 28, video_min_pixels=128 * 28 * 28, generation_config=None ): super().__init__() self.model_name_or_path = model_name_or_path self.device = device self.device_map = device self.video_max_pixels = video_max_pixels if video_max_pixels is not None else 768 * 28 * 28 self.video_min_pixels = video_min_pixels if video_min_pixels is not None else 128 * 28 * 28 self.model, self.tokenizer, self.processor = self.load_model_processor() if max_pixels is not None: self.processor.max_pixels = max_pixels if min_pixels is not None: self.processor.min_pixels = min_pixels if generation_config is None: generation_config = { \"num_beams\": 1, \"do_sample\": True, \"temperature\": 0.9 } self.generation_config = generation_config def load_model_processor(self): model = Bailing_qwen2_5NativeForConditionalGeneration.from_pretrained( self.model_name_or_path, torch_dtype=torch.bfloat16, device_map=self.device_map, _attn_implementation=\"flash_attention_2\" ).eval() tokenizer = AutoTokenizer.from_pretrained(self.model_name_or_path, add_bos_token=True, trust_remote_code=True) processor = Bailing_qwen2_5Processor.from_pretrained(self.model_name_or_path, trust_remote_code=True) return model, tokenizer, processor def generate(self, messages, max_new_tokens=512): text = self.processor.apply_chat_template( messages, tokenize=False, add_generation_prompt=True, use_system=True ) image_inputs, video_inputs = self.processor.process_vision_info(messages) inputs = self.processor( text=[text], images=image_inputs, videos=video_inputs, return_tensors=\"pt\", ) # print(inputs) print(self.tokenizer.decode(inputs['input_ids'][0])) inputs = inputs.to(self.device) for k in inputs.keys(): if k == \"pixel_values\" or k == \"pixel_values_videos\": inputs[k] = inputs[k].to(dtype=torch.bfloat16) with torch.no_grad(): generated_ids = self.model.generate( inputs, max_new_tokens=max_new_tokens, eos_token_id=self.processor.tokenizer.eos_token_id, **self.generation_config, ) generated_ids_trimmed = [ out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] output_text = self.processor.batch_decode( generated_ids_trimmed, skip_special_tokens=False, clean_up_tokenization_spaces=False )[0] return output_text if __name__ == '__main__': parser = argparse.ArgumentParser() parser.add_argument('--model_name_or_path', type=str, default=\"inclusionAI/M2-Reasoning\") parser.add_argument('--max_pixels', type=int, default=401408) parser.add_argument('--min_pixels', type=int, default=401408) parser.add_argument('--max_new_tokens', type=int, default=4096) args = parser.parse_args() device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # model_name_or_path = os.path.join(args.input_dir, args.model_name_or_path) bailing2 = BailingMMInfer( args.model_name_or_path, device=device, max_pixels=args.max_pixels, min_pixels=args.min_pixels ) messages = [ { \"role\": \"system\", \"content\": [ {\"type\": \"text\", \"text\": \"You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in ... tags, then the final answer enclosed in ... tags. The critical answer or key result should be placed within \\\\boxed{}.\"}]}, { \"role\": \"user\", \"content\": [ {\"type\": \"image\", \"image\": \"./assets/example1.png\"}, {\"type\": \"text\", \"text\": \"\\nQuestion:\\n\\nRhombus $QRST$ has an area of 137.9 square meters. If $RT$ is 12.2 meters, find $QS$.\\nA. 11.3\\nB. 22.4\\nC. 22.6\\nD. 25.6\"}, ], }, ] output_text = bailing2.generate(messages, max_new_tokens=args.max_new_tokens) print(output_text) ''' [Output]: To find the length of \\( QS \\) in the rhombus \\( QRST \\), we can use the formula for the area of a rhombus, which is given by: \\[ \\text{Area} = \\frac{1}{2} \\times d_1 \\times d_2 \\] where \\( d_1 \\) and \\( d_2 \\) are the lengths of the diagonals. In this problem, we are given: - The area of the rhombus is 137.9 square meters. - One of the diagonals, ","wordCount":"1052","inLanguage":"en","datePublished":"2025-07-11T00:00:03+08:00","dateModified":"2025-07-11T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/m2-reasoning/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</h1><div class=post-meta><span title='2025-07-11 00:00:03 +0800 +0800'>July 11, 2025</span>&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;1052 words&nbsp;Â·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/m2-reasoning/>ç®€ä½“ä¸­æ–‡</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p>ðŸ“– <a href=https://arxiv.org/abs/2507.08306>Technical Report</a> | ðŸ¤— <a href=https://huggingface.co/inclusionAI/M2-Reasoning>Hugging Face</a>ï½œ ðŸ¤– <a href=https://www.modelscope.cn/models/inclusionAI/M2-Reasoning>ModelScope</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals. This combination of curated data and advanced training allows M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks, showcasing superior performance in both general and spatial reasoning domains.
<img loading=lazy src=assets/teaser.png alt></p><h2 id=-updates>ðŸ“Œ Updates<a hidden class=anchor aria-hidden=true href=#-updates>#</a></h2><ul><li>[2025.07.14] ðŸ”¥ Our <a href=https://arxiv.org/abs/2507.08306>Technical Report</a> is in public on arxiv.</li><li>[2025.07.11] ðŸ”¥ We release M2-Reasoning on ðŸ¤— <a href=https://huggingface.co/inclusionAI/M2-Reasoning>Hugging Face</a> and ðŸ¤– <a href=https://www.modelscope.cn/models/inclusionAI/M2-Reasoning>ModelScope</a>.</li></ul><h2 id=key-features>Key Features<a hidden class=anchor aria-hidden=true href=#key-features>#</a></h2><ul><li>A High-quality Data Construction Pipeline: We design and implement a multi-stage data synthesis and curation pipeline that generates vast amounts of reasoning data.</li><li>A Dynamic Multi-Task Training Strategy: We propose a sophisticated training strategy that effectively handles data heterogeneity. It features step-wise dynamic optimization to mitigate conflicts between different data sources and a task-specific reward formulation to provide tailored incentive signals.</li><li>Unified General and Spatial Reasoning Model: We propose M2-Reasoning-7B, an MLLM uniquely engineered for both abstract and spatial reasoning. Extensive evaluations on 8 distinctbenchmarks demonstrate that, by leveraging our custom data and training pipelines, M2-Reasoning establishes new state-of-the-art (SOTA) results across both general and spatial reasoning domains.</li></ul><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><p>We conduct a comprehensive evaluation of our models across two key domains: general and spatial
reasoning. Our evaluation utilizes a diverse set of public benchmarks, grouped by the primary
capability they measure:</p><ul><li>General Reasoning (Mathematical & Logical): To evaluate this capability, we employ six benchmarks: MathVista, MathVision, MathVerse, DynaMath, WeMath, and LogicVista.</li></ul><table><thead><tr><th style=text-align:center>Models</th><th style=text-align:center>MathVista</th><th style=text-align:center>MathVision</th><th style=text-align:center>MathVerse</th><th style=text-align:center>DynaMath</th><th style=text-align:center>WeMath</th><th style=text-align:center>LogicVista</th><th style=text-align:center>Avg. (Î”)</th></tr></thead><tbody><tr><td style=text-align:center><em><strong>Base-Scale General Models</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style=text-align:center>InternVL3-8B</td><td style=text-align:center>70.5</td><td style=text-align:center>30.0</td><td style=text-align:center>38.5</td><td style=text-align:center>25.7</td><td style=text-align:center>39.5</td><td style=text-align:center>44.5</td><td style=text-align:center>41.4</td></tr><tr><td style=text-align:center>InternVL3-9B</td><td style=text-align:center>69.0</td><td style=text-align:center>29.3</td><td style=text-align:center>37.9</td><td style=text-align:center>25.1</td><td style=text-align:center>34.8</td><td style=text-align:center>49.0</td><td style=text-align:center>40.8</td></tr><tr><td style=text-align:center>Qwen2.5-VL-7B</td><td style=text-align:center>68.1</td><td style=text-align:center>25.4</td><td style=text-align:center>41.1</td><td style=text-align:center>21.8</td><td style=text-align:center>36.2</td><td style=text-align:center>47.9</td><td style=text-align:center>40.1</td></tr><tr><td style=text-align:center>MUG-U-7B</td><td style=text-align:center>74.8</td><td style=text-align:center>26.1</td><td style=text-align:center>35.4</td><td style=text-align:center>17.2</td><td style=text-align:center>26.5</td><td style=text-align:center>39.8</td><td style=text-align:center>36.6</td></tr><tr><td style=text-align:center>SAIL-VL-1.6-8B</td><td style=text-align:center>74.2</td><td style=text-align:center>23.2</td><td style=text-align:center>33.4</td><td style=text-align:center>14.0</td><td style=text-align:center>29.6</td><td style=text-align:center>41.4</td><td style=text-align:center>36.0</td></tr><tr><td style=text-align:center><em><strong>Base-Scale Reasoning Models</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style=text-align:center>WeThink-VL-7B</td><td style=text-align:center>71.6</td><td style=text-align:center>26.0</td><td style=text-align:center>44.2</td><td style=text-align:center>24.8</td><td style=text-align:center><strong>48.0</strong></td><td style=text-align:center><strong>51.2</strong></td><td style=text-align:center>44.3 (+4.2)</td></tr><tr><td style=text-align:center>Taichu-VLR-7B</td><td style=text-align:center>72.3</td><td style=text-align:center>27.1</td><td style=text-align:center>46.7</td><td style=text-align:center>23.0</td><td style=text-align:center>44.0</td><td style=text-align:center>48.3</td><td style=text-align:center>43.6</td></tr><tr><td style=text-align:center>VLAA-Thinker-7B</td><td style=text-align:center>68.0</td><td style=text-align:center>26.4</td><td style=text-align:center><strong>48.2</strong></td><td style=text-align:center>22.4</td><td style=text-align:center>41.5</td><td style=text-align:center>48.5</td><td style=text-align:center>42.5 (+2.4)</td></tr><tr><td style=text-align:center>URSA-8B-PS-GRPO</td><td style=text-align:center>67.8</td><td style=text-align:center><strong>31.8</strong></td><td style=text-align:center>41.5</td><td style=text-align:center>22.4</td><td style=text-align:center>38.3</td><td style=text-align:center>44.7</td><td style=text-align:center>41.1 (+8.2)</td></tr><tr><td style=text-align:center>Ovis2-8B</td><td style=text-align:center>71.8</td><td style=text-align:center>25.9</td><td style=text-align:center>42.3</td><td style=text-align:center>20.4</td><td style=text-align:center>27.2</td><td style=text-align:center>39.4</td><td style=text-align:center>37.8</td></tr><tr><td style=text-align:center><em><strong>Our Models</strong></em></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td style=text-align:center>Base Model</td><td style=text-align:center>70.2</td><td style=text-align:center>25.9</td><td style=text-align:center>30.5</td><td style=text-align:center>20.2</td><td style=text-align:center>27.2</td><td style=text-align:center>37.8</td><td style=text-align:center>35.5</td></tr><tr><td style=text-align:center>M2-Reasoning-CI-7B</td><td style=text-align:center>71.7</td><td style=text-align:center>29.2</td><td style=text-align:center>42.1</td><td style=text-align:center>25.0</td><td style=text-align:center>42.8</td><td style=text-align:center>46.8</td><td style=text-align:center>42.9 (+7.4)</td></tr><tr><td style=text-align:center>M2-Reasoning-7B</td><td style=text-align:center><strong>75.0</strong></td><td style=text-align:center>31.5</td><td style=text-align:center>44.7</td><td style=text-align:center><strong>26.8</strong></td><td style=text-align:center>41.8</td><td style=text-align:center>50.0</td><td style=text-align:center><strong>45.0 (+9.5)</strong></td></tr></tbody></table><ul><li><p>Spatial Reasoning: We assess this skill using 2 benchmarks: CV-Bench and VSI-Bench</p><ul><li>CV-Bench:</li></ul><table><thead><tr><th style=text-align:left>Models</th><th style=text-align:center>Count</th><th style=text-align:center>Relation</th><th style=text-align:center>Depth</th><th style=text-align:center>Distance</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td style=text-align:left><em><strong>Large-Scale Models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>GPT-4O</td><td style=text-align:center>65.9</td><td style=text-align:center>85.7</td><td style=text-align:center>87.8</td><td style=text-align:center>78.2</td><td style=text-align:center>78.9</td></tr><tr><td style=text-align:left>Gemini-1.5-pro</td><td style=text-align:center>70.4</td><td style=text-align:center>85.2</td><td style=text-align:center>82.4</td><td style=text-align:center>72.8</td><td style=text-align:center>77.4</td></tr><tr><td style=text-align:left><em><strong>Base-Scale Models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>InternVL3-8B</td><td style=text-align:center><strong>74.0</strong></td><td style=text-align:center>90.6</td><td style=text-align:center>84.3</td><td style=text-align:center>81.0</td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>Qwen2.5-VL-7B-Instruct</td><td style=text-align:center>65.2</td><td style=text-align:center>86.6</td><td style=text-align:center>70.6</td><td style=text-align:center>79.8</td><td style=text-align:center>75.0</td></tr><tr><td style=text-align:left>LLava-NEXT-Video-7B</td><td style=text-align:center>59.3</td><td style=text-align:center>77.0</td><td style=text-align:center>71.3</td><td style=text-align:center>54.7</td><td style=text-align:center>65.2</td></tr><tr><td style=text-align:left><em><strong>Our Models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>M2-Reasoning-7B</td><td style=text-align:center>66.6</td><td style=text-align:center><strong>92.8</strong></td><td style=text-align:center><strong>89.3</strong></td><td style=text-align:center><strong>84.3</strong></td><td style=text-align:center><strong>82.3</strong></td></tr></tbody></table><ul><li>VSI-Bench:</li></ul><table><thead><tr><th style=text-align:left></th><th style=text-align:center>OC</th><th style=text-align:center>AD</th><th style=text-align:center>OS</th><th style=text-align:center>RS</th><th style=text-align:center>RDs</th><th style=text-align:center>RDr</th><th style=text-align:center>RP</th><th style=text-align:center>AO</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td style=text-align:left><em><strong>Large-Scale Models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>Gemini-1.5-pro</td><td style=text-align:center>56.2</td><td style=text-align:center>30.9</td><td style=text-align:center>64.1</td><td style=text-align:center>43.6</td><td style=text-align:center>51.3</td><td style=text-align:center>46.3</td><td style=text-align:center>36.0</td><td style=text-align:center>34.6</td><td style=text-align:center>45.4</td></tr><tr><td style=text-align:left>GPT-4O</td><td style=text-align:center>46.2</td><td style=text-align:center>5.3</td><td style=text-align:center>43.8</td><td style=text-align:center>38.2</td><td style=text-align:center>37.0</td><td style=text-align:center>41.3</td><td style=text-align:center>31.5</td><td style=text-align:center>28.5</td><td style=text-align:center>34.0</td></tr><tr><td style=text-align:left><em><strong>Base-Scale Models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>InternVL3-8B</td><td style=text-align:center><strong>68.1</strong></td><td style=text-align:center><strong>39.0</strong></td><td style=text-align:center>48.4</td><td style=text-align:center>33.6</td><td style=text-align:center><strong>48.3</strong></td><td style=text-align:center>36.4</td><td style=text-align:center>27.3</td><td style=text-align:center><strong>35.4</strong></td><td style=text-align:center>42.1</td></tr><tr><td style=text-align:left>Video-R1-7B</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>37.1</td></tr><tr><td style=text-align:left>Qwen2.5-VL-7B-Instruct</td><td style=text-align:center>37.7</td><td style=text-align:center>20.1</td><td style=text-align:center>49.7</td><td style=text-align:center>37.4</td><td style=text-align:center>38.5</td><td style=text-align:center>40.4</td><td style=text-align:center>31.4</td><td style=text-align:center>32.0</td><td style=text-align:center>35.9</td></tr><tr><td style=text-align:left>LLava-NeXT-Video-7B</td><td style=text-align:center>48.5</td><td style=text-align:center>14.0</td><td style=text-align:center>47.8</td><td style=text-align:center>24.2</td><td style=text-align:center>43.5</td><td style=text-align:center>42.4</td><td style=text-align:center><strong>34.0</strong></td><td style=text-align:center>30.6</td><td style=text-align:center>35.6</td></tr><tr><td style=text-align:left><em><strong>Our Models</strong></em></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td></tr><tr><td style=text-align:left>M2-Reasoning-7B</td><td style=text-align:center>41.0</td><td style=text-align:center>34.0</td><td style=text-align:center><strong>60.9</strong></td><td style=text-align:center><strong>55.4</strong></td><td style=text-align:center>40.7</td><td style=text-align:center><strong>47.3</strong></td><td style=text-align:center>29.9</td><td style=text-align:center>28.8</td><td style=text-align:center><strong>42.3</strong></td></tr></tbody></table></li></ul><h2 id=model-downloads>Model Downloads<a hidden class=anchor aria-hidden=true href=#model-downloads>#</a></h2><p>You can download the model from both <a href=https://huggingface.co/inclusionAI/M2-Reasoning>Hugging Face</a> and <a href=https://www.modelscope.cn/models/inclusionAI/M2-Reasoning>ModelScope</a>.</p><p>If you&rsquo;re in mainland China, we strongly recommend you to download our model from <a href=https://www.modelscope.cn/models/inclusionAI/M2-Reasoning>ModelScope</a>.</p><h2 id=example-usage>Example Usage<a hidden class=anchor aria-hidden=true href=#example-usage>#</a></h2><p>The basic environment is <code>python=3.10</code>, <code>torch=2.6.0+cu124</code>, <code>transformers=4.49.0</code></p><p>We provide a small example on the usage of this repo.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>AutoProcessor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>AutoTokenizer</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>warnings</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>argparse</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>modeling_bailing_qwen2_5</span> <span class=kn>import</span> <span class=n>Bailing_qwen2_5NativeForConditionalGeneration</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>processing_bailing_qwen2_5</span> <span class=kn>import</span> <span class=n>Bailing_qwen2_5Processor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>warnings</span><span class=o>.</span><span class=n>filterwarnings</span><span class=p>(</span><span class=s2>&#34;ignore&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>BailingMMInfer</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>model_name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>device</span><span class=o>=</span><span class=s2>&#34;cuda&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_pixels</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>min_pixels</span><span class=o>=</span><span class=kc>None</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>video_max_pixels</span><span class=o>=</span><span class=mi>768</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>video_min_pixels</span><span class=o>=</span><span class=mi>128</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>generation_config</span><span class=o>=</span><span class=kc>None</span>
</span></span><span class=line><span class=cl>    <span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model_name_or_path</span> <span class=o>=</span> <span class=n>model_name_or_path</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device</span> <span class=o>=</span> <span class=n>device</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>device_map</span> <span class=o>=</span> <span class=n>device</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>video_max_pixels</span> <span class=o>=</span> <span class=n>video_max_pixels</span> <span class=k>if</span> <span class=n>video_max_pixels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=mi>768</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>video_min_pixels</span> <span class=o>=</span> <span class=n>video_min_pixels</span> <span class=k>if</span> <span class=n>video_min_pixels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span> <span class=k>else</span> <span class=mi>128</span> <span class=o>*</span> <span class=mi>28</span> <span class=o>*</span> <span class=mi>28</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=p>,</span> <span class=bp>self</span><span class=o>.</span><span class=n>processor</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>load_model_processor</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>max_pixels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=o>.</span><span class=n>max_pixels</span> <span class=o>=</span> <span class=n>max_pixels</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>min_pixels</span> <span class=ow>is</span> <span class=ow>not</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=o>.</span><span class=n>min_pixels</span> <span class=o>=</span> <span class=n>min_pixels</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>generation_config</span> <span class=ow>is</span> <span class=kc>None</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>generation_config</span> <span class=o>=</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;num_beams&#34;</span><span class=p>:</span> <span class=mi>1</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;do_sample&#34;</span><span class=p>:</span> <span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=s2>&#34;temperature&#34;</span><span class=p>:</span> <span class=mf>0.9</span>
</span></span><span class=line><span class=cl>            <span class=p>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>generation_config</span> <span class=o>=</span> <span class=n>generation_config</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>load_model_processor</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>model</span> <span class=o>=</span> <span class=n>Bailing_qwen2_5NativeForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>model_name_or_path</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>device_map</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>device_map</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>_attn_implementation</span><span class=o>=</span><span class=s2>&#34;flash_attention_2&#34;</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model_name_or_path</span><span class=p>,</span> <span class=n>add_bos_token</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>processor</span> <span class=o>=</span> <span class=n>Bailing_qwen2_5Processor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>model_name_or_path</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>model</span><span class=p>,</span> <span class=n>tokenizer</span><span class=p>,</span> <span class=n>processor</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>generate</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>messages</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>text</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>messages</span><span class=p>,</span> <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>use_system</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>image_inputs</span><span class=p>,</span> <span class=n>video_inputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=o>.</span><span class=n>process_vision_info</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>text</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span>
</span></span><span class=line><span class=cl>            <span class=n>images</span><span class=o>=</span><span class=n>image_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>videos</span><span class=o>=</span><span class=n>video_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># print(inputs)</span>
</span></span><span class=line><span class=cl>        <span class=nb>print</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>decode</span><span class=p>(</span><span class=n>inputs</span><span class=p>[</span><span class=s1>&#39;input_ids&#39;</span><span class=p>][</span><span class=mi>0</span><span class=p>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>inputs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values_videos&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>with</span> <span class=n>torch</span><span class=o>.</span><span class=n>no_grad</span><span class=p>():</span>
</span></span><span class=line><span class=cl>            <span class=n>generated_ids</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>                <span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>max_new_tokens</span><span class=o>=</span><span class=n>max_new_tokens</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=n>eos_token_id</span><span class=o>=</span><span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=o>.</span><span class=n>tokenizer</span><span class=o>.</span><span class=n>eos_token_id</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=o>**</span><span class=bp>self</span><span class=o>.</span><span class=n>generation_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>generated_ids_trimmed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=n>out_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>in_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>in_ids</span><span class=p>,</span> <span class=n>out_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>output_text</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>generated_ids_trimmed</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl>        <span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>output_text</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=vm>__name__</span> <span class=o>==</span> <span class=s1>&#39;__main__&#39;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span> <span class=o>=</span> <span class=n>argparse</span><span class=o>.</span><span class=n>ArgumentParser</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s1>&#39;--model_name_or_path&#39;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>str</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=s2>&#34;inclusionAI/M2-Reasoning&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s1>&#39;--max_pixels&#39;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>int</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=mi>401408</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s1>&#39;--min_pixels&#39;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>int</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=mi>401408</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>parser</span><span class=o>.</span><span class=n>add_argument</span><span class=p>(</span><span class=s1>&#39;--max_new_tokens&#39;</span><span class=p>,</span> <span class=nb>type</span><span class=o>=</span><span class=nb>int</span><span class=p>,</span> <span class=n>default</span><span class=o>=</span><span class=mi>4096</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>args</span> <span class=o>=</span> <span class=n>parser</span><span class=o>.</span><span class=n>parse_args</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>device</span> <span class=o>=</span> <span class=s2>&#34;cuda&#34;</span> <span class=k>if</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>is_available</span><span class=p>()</span> <span class=k>else</span> <span class=s2>&#34;cpu&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># model_name_or_path = os.path.join(args.input_dir, args.model_name_or_path)</span>
</span></span><span class=line><span class=cl>    <span class=n>bailing2</span> <span class=o>=</span> <span class=n>BailingMMInfer</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>args</span><span class=o>.</span><span class=n>model_name_or_path</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>max_pixels</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>max_pixels</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>min_pixels</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>min_pixels</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>            <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in &lt;think&gt;...&lt;/think&gt; tags, then the final answer enclosed in &lt;answer&gt;...&lt;/answer&gt; tags. The critical answer or key result should be placed within </span><span class=se>\\</span><span class=s2>boxed</span><span class=si>{}</span><span class=s2>.&#34;</span><span class=p>}]},</span>
</span></span><span class=line><span class=cl>        <span class=p>{</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>            <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span> <span class=s2>&#34;image&#34;</span><span class=p>:</span> <span class=s2>&#34;./assets/example1.png&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>                <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;</span><span class=se>\n</span><span class=s2>Question:</span><span class=se>\n\n</span><span class=s2>Rhombus $QRST$ has an area of 137.9 square meters. If $RT$ is 12.2 meters, find $QS$.</span><span class=se>\n</span><span class=s2>A. 11.3</span><span class=se>\n</span><span class=s2>B. 22.4</span><span class=se>\n</span><span class=s2>C. 22.6</span><span class=se>\n</span><span class=s2>D. 25.6&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>],</span>
</span></span><span class=line><span class=cl>        <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>output_text</span> <span class=o>=</span> <span class=n>bailing2</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=n>args</span><span class=o>.</span><span class=n>max_new_tokens</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>output_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=s1>&#39;&#39;&#39;
</span></span></span><span class=line><span class=cl><span class=s1>[Output]:
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>&lt;think&gt;
</span></span></span><span class=line><span class=cl><span class=s1>To find the length of \( QS \) in the rhombus \( QRST \), we can use the formula for the area of a rhombus, which is given by:
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>\[
</span></span></span><span class=line><span class=cl><span class=s1></span><span class=se>\t</span><span class=s1>ext</span><span class=si>{Area}</span><span class=s1> = </span><span class=se>\f</span><span class=s1>rac</span><span class=si>{1}{2}</span><span class=s1> </span><span class=se>\t</span><span class=s1>imes d_1 </span><span class=se>\t</span><span class=s1>imes d_2
</span></span></span><span class=line><span class=cl><span class=s1>\]
</span></span></span><span class=line><span class=cl><span class=s1>
</span></span></span><span class=line><span class=cl><span class=s1>where \( d_1 \) and \( d_2 \) are the lengths of the diagonals. In this problem, we are given:
</span></span></span><span class=line><span class=cl><span class=s1>- The area of the rhombus is 137.9 square meters.
</span></span></span><span class=line><span class=cl><span class=s1>- One of the diagonals,
</span></span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
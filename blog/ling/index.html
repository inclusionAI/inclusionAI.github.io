<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ling: A MoE LLM Provided and Open-sourced by InclusionAI | INCLUSION AI</title><meta name=keywords content><meta name=description content="ðŸ¤— Hugging Face&nbsp&nbsp | &nbsp&nbspðŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ling/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ling/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ling: A MoE LLM Provided and Open-sourced by InclusionAI"><meta property="og:description" content="ðŸ¤— Hugging Face&nbsp&nbsp | &nbsp&nbspðŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ling/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-08T00:00:03+08:00"><meta property="article:modified_time" content="2025-05-08T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ling: A MoE LLM Provided and Open-sourced by InclusionAI"><meta name=twitter:description content="ðŸ¤— Hugging Face&nbsp&nbsp | &nbsp&nbspðŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ling: A MoE LLM Provided and Open-sourced by InclusionAI","item":"https://inclusionai.github.io/blog/ling/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ling: A MoE LLM Provided and Open-sourced by InclusionAI","name":"Ling: A MoE LLM Provided and Open-sourced by InclusionAI","description":"ðŸ¤— Hugging Face\u0026nbsp\u0026nbsp | \u0026nbsp\u0026nbspðŸ¤– ModelScope\nIntroduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.\nTheir structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems.","keywords":[],"articleBody":"ðŸ¤— Hugging FaceÂ | ðŸ¤– ModelScope\nIntroduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.\nTheir structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems. Furthermore, the open-source nature of Ling promotes collaboration and innovation within the AI community, fostering a diverse range of use cases and enhancements.\nAs more developers and researchers engage with the platform, we can expect rapid advancements and improvements, leading to even more sophisticated applications. This collaborative approach accelerates development and ensures that the models remain at the forefront of technology, addressing emerging challenges in various fields.\nUpdate [2025-5-10] Ling-lite-1.5 has been released! It achieves significant progress in reasoning ability compared with previous Ling-lite. [2025-4-15] Ling-lite is upgraded to Ling-lite-0415. The new model demonstrates notable improvements over its predecessor, Ling-lite-0220, especially on code and math. Model Downloads You can download the following table to see the various parameters for your use case. If you are located in mainland China, we also provide the model on ModelScope.cn to speed up the download process.\nModel #Total Params #Activated Params Context Length Download Ling-lite-base-1.5 16.8B 2.75B 128K ðŸ¤— HuggingFace ðŸ¤– ModelScope Ling-lite-1.5 16.8B 2.75B 128K ðŸ¤— HuggingFace ðŸ¤– ModelScope Ling-plus-base 290B 28.8B 64K ðŸ¤— HuggingFace ðŸ¤– ModelScope Ling-plus 290B 28.8B 64K ðŸ¤— HuggingFace ðŸ¤– ModelScope Ling-coder-lite-base 16.8B 2.75B 16K ðŸ¤— HuggingFace ðŸ¤– ModelScope Ling-coder-lite 16.8B 2.75B 16K ðŸ¤— HuggingFace ðŸ¤– ModelScope Note: If you are interested in previous version, please visit the past model collections in Huggingface or ModelScope.\nEvaluation Ling-lite Standard Benchmarks Benchmark #shots Ling-lite-1.5 Ling-lite Qwen3-4B-Instruct Qwen3-8B-Instruct Moonlight-16B-A3B-Instruct LLaMA3.1-8B MMLU(EM) 5 74.33 71.27 70.09 75.97 70.74 68.67 GPQA(Pass@1) 0 36.55 29.73 40.4 47.10 19.51 27.59 HumanEval(Pass@1) 0 87.27 84.38 81.94 85.29 72.94 67.23 LiveCodeBench 2408-2502 (Pass@1) 0 22.7 18.94 21.8 26.88 14.76 18.41 LCBench(pass@1) 0 60.37 46.57 48.61 60.03 28.39 23.13 Math(EM) 0 82.62 72.80 81.46 82.70 67.1 52.42 AIME2024(pass@1) 0 21.88 10.21 20.62 26.25 6.88 7.29 OlympiadBench(pass@1) 0 52.30 36.44 54.33 56.11 32.85 17.04 BBH(EM) 0 75.75 66.38 78.21 79.33 63.45 68.05 IFEval(Prompt Strict) 0 77.70 77.99 81.06 83.55 49.01 73.01 BFCL_live 0 72.15 67.93 65.35 69.83 47.14 49.98 Context Window Evaluation results on the Needle In A Haystack (NIAH) tests. Ling-lite-1.5 has improved long text generation capability and performs well across most context window lengths up to 128K.\nQuickstart ðŸ¤— Hugging Face Transformers Here is a code snippet to show you how to use the chat model with transformers:\nfrom transformers import AutoModelForCausalLM, AutoTokenizer model_name = \"inclusionAI/Ling-lite-1.5\" model = AutoModelForCausalLM.from_pretrained( model_name, torch_dtype=\"auto\", device_map=\"auto\" ) tokenizer = AutoTokenizer.from_pretrained(model_name) prompt = \"Give me a short introduction to large language models.\" messages = [ {\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"}, {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device) generated_ids = model.generate( **model_inputs, max_new_tokens=512 ) generated_ids = [ output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids) ] response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] ðŸ¤– ModelScope If youâ€™re in mainland China, we strongly recommend you to use our model from ðŸ¤– ModelScope.\nDeployment vLLM vLLM supports offline batched inference or launching an OpenAI-Compatible API Service for online inference.\nEnvironment Preparation Since the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below:\ngit clone -b v0.7.3 https://github.com/vllm-project/vllm.git cd vllm git apply Ling/inference/vllm/bailing_moe.patch pip install -e . Offline Inference: from transformers import AutoTokenizer from vllm import LLM, SamplingParams tokenizer = AutoTokenizer.from_pretrained(\"inclusionAI/Ling-lite-1.5\") sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=16384) llm = LLM(model=\"inclusionAI/Ling-lite\", dtype='bfloat16') prompt = \"Give me a short introduction to large language models.\" messages = [ {\"role\": \"system\", \"content\": \"You are Ling, an assistant created by inclusionAI\"}, {\"role\": \"user\", \"content\": prompt} ] text = tokenizer.apply_chat_template( messages, tokenize=False, add_generation_prompt=True ) outputs = llm.generate([text], sampling_params) Online Inference: vllm serve inclusionAI/Ling-lite \\ --tensor-parallel-size 2 \\ --pipeline-parallel-size 1 \\ --use-v2-block-manager \\ --gpu-memory-utilization 0.90 To handle long context in vLLM using YaRN, we need to follow these two steps:\nAdd a rope_scaling field to the modelâ€™s config.json file, for example: { ..., \"rope_scaling\": { \"factor\": 4.0, \"original_max_position_embeddings\": 32768, \"type\": \"yarn\" } } Use an additional parameter --max-model-len to specify the desired maximum context length when starting the vLLM service. For detailed guidance, please refer to the vLLM instructions.\nMindIE This subject outlines the primary processes for executing a Ling MoE model with specified hardware and the MindIE inference framework.\nConfigure preparation Create a model directory on the host for downloading, the directory example is: /root/modelsâ€™, which is used to mount the docker container later.\nDownload the mindie-related configuration from github:\ncd /root/models git clone git@github.com:inclusionAI/Ling.git Machine network environment check # Check the physical link for i in {0..7}; do hccn_tool -i $i -lldp -g | grep Ifname; done # Check the links for i in {0..7}; do hccn_tool -i $i -link -g ; done # Check your network health for i in {0..7}; do hccn_tool -i $i -net_health -g ; done # Check whether the detected IP address is correctly configured for i in {0..7}; do hccn_tool -i $i -netdetect -g ; done # Check whether the gateway is configured correctly for i in {0..7}; do hccn_tool -i $i -gateway -g ; done # Check the consistency of the underlying TLS verification behavior of the NPU, recommend that all 0 be for i in {0..7}; do hccn_tool -i $i -tls -g ; done | grep switch # The underlying TLS check line of the NPU is set to 0 for i in {0..7}; do hccn_tool -i $i -tls -s enable 0; done Pull the image Go to Ascend Community/Development Resources and pull the mindie image\nImage version: 1.0.0-800I-A2-py311-openeuler24.03-lts\nThe versions of each component are as follows:\nComponent Version MindIE 1.0.0 CANN 8.0.0 PTA 6.0.0.beta1 HDK 24.1.0 Container startup and configuration changes Start the container Execute the following startup command (reference):\ndocker run -itd --privileged --name=container name --net=host \\ --shm-size 500g \\ --device=/dev/davinci0 \\ --device=/dev/davinci1 \\ --device=/dev/davinci2 \\ --device=/dev/davinci3 \\ --device=/dev/davinci4 \\ --device=/dev/davinci5 \\ --device=/dev/davinci6 \\ --device=/dev/davinci7 \\ --device=/dev/davinci_manager \\ --device=/dev/hisi_hdc \\ --device /dev/devmm_svm \\ -v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\ -v /usr/local/Ascend/firmware:/usr/local/Ascend/firmware \\ -v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi \\ -v /usr/local/sbin:/usr/local/sbin \\ -v /etc/hccn.conf:/etc/hccn.conf \\ -v /root/models:/home/HwHiAiUser/Ascend \\ mindie: 1.0.0-XXX-800I-A2-arm64-py3.11 (modified according to the name of the loaded image) \\ bash Download the model In this case, we use ModelScope to download the model, and install ModelScope first:\npip install modelscope Download the model:\n# The model takes a long time to download and can be executed in the background nohup modelscope download --model inclusionAI/Ling-plus --local_dir /home/HwHiAiUser/Ascend/Ling_plus 2\u003e\u00261 \u003e /tmp/ling_plus.log \u0026 nohup modelscope download --model inclusionAI/Ling-plus-base --local_dir /home/HwHiAiUser/Ascend/Ling_plus_base 2\u003e\u00261 \u003e /tmp/ling_plus_base.log \u0026 nohup modelscope download --model inclusionAI/Ling-lite --local_dir /home/HwHiAiUser/Ascend/Ling_lite 2\u003e\u00261 \u003e /tmp/ling_lite.log \u0026 nohup modelscope download --model inclusionAI/Ling-lite-base --local_dir /home/HwHiAiUser/Ascend/Ling_lite_base 2\u003e\u00261 \u003e /tmp/ling_lite_base.log \u0026 After the download is completed, you need to change the file permissions, otherwise an error will be reported when MindIE-Service is started:\nchmod -R 750 *.json *.py Model weight format conversion This section applies to the Ling Lite model, the Ling Plus model does not need to worry about this chapter\nmindie supports safetensors format weights, if the download weights are not in safetensors format, you need to convert the weights, take Ling Lite as an example, the conversion command is as follows:\n# Convert Ling lite python /home/HwHiAiUser/Ascend/Ling/inference/mindie/convert_bin_to_safetensor.py cd /home/HwHiAiUser/Ascend/Ling_lite cp README.md configuration.json config.json special_tokens_map.json modeling_bailing_moe.py tokenizer.json tokenizer_config.json ../Ling_lite_safetensor/ # Convert Ling lite base python /home/HwHiAiUser/Ascend/Ling/inference/mindie/convert_bin_to_safetensor_base.py cd /home/HwHiAiUser/Ascend/Ling_lite_base cp README.md configuration.json config.json special_tokens_map.json modeling_bailing_moe.py tokenizer.json tokenizer_config.json ../Ling_lite_base_safetensor/ The path of loading the Ling Lite model is changed to â€˜/home/HwHiAiUser/Ascend/Ling_lite_safetensorâ€™, and the path of the Ling Lite Base model is changed to â€˜/home/HwHiAiUser/Ascend/Ling_lite_base_safetensorâ€™\nChange the model configuration The default model configuration file (config.json) mindie cannot be loaded directly, and needs to be changed:\n# Adapt to mindie's Ling lite model configuration cp /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json.bak cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/model_chat_config.json /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json chmod 750 /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json # Adapt to mindie's Ling lite base model configuration cp /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json.bak cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/model_base_config.json /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json chmod 750 /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json # Adapt to mindie's Ling plus model configuration cp /home/HwHiAiUser/Ascend/Ling_plus/config.json /home/HwHiAiUser/Ascend/Ling_plus/config.json.bak cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/plus/model_chat_config.json /home/HwHiAiUser/Ascend/Ling_plus/config.json chmod 750 /home/HwHiAiUser/Ascend/Ling_plus/config.json # Adapt to mindie's Ling plus base model configuration cp /home/HwHiAiUser/Ascend/Ling_plus_base/config.json /home/HwHiAiUser/Ascend/Ling_plus_base/config.json.bak cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/plus/model_base_config.json /home/HwHiAiUser/Ascend/Ling_plus_base/config.json chmod 750 /home/HwHiAiUser/Ascend/Ling_plus_base/config.json Execute the shell script that adapts the mindie to the Ling model:\nbash /home/HwHiAiUser/Ascend/Ling/inference/mindie/patch_atb_llm.sh Stand-alone Servitization Inference (Ling lite) Set the underlying environment variables:\nsource /usr/local/Ascend/atb-models/set_env.sh Set different mindie configurations according to the model type:\n# Ling Lite cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/config.json /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json # Ling Lite base cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/config.base.json /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json Start the mindie service:\nchmod 640 /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json cd $MIES_INSTALL_PATH nohup ./bin/mindieservice_daemon \u003e /tmp/service.log 2\u003e\u00261 \u0026 Check /tmp/service.log to check whether the output is Daemon start success!, if so, it means that MindIE-Service has started successfully.\nTest if the request is correct:\n# Chat model wget -O- --post-data=\"{\\\"messages\\\":[{\\\"role\\\": \\\"system\\\", \\\"content\\\": \\\"You are a helpful assistant.\\\"}, {\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Who are you?\\\"}], \\\"stream\\\": false, \\\"max_tokens\\\":100, \\\"model\\\": \\\"bailing_moe\\\", \\\"temperature\\\":0}\" \\ --header='Content-Type:application/json' \\ 'http://127.0.0.1:1025/v1/chat/completions' # base model wget -O- --post-data='{\"inputs\":\"My name is Olivier and I\",\"stream\":false,\"parameters\":{\"temperature\":1,\"max_new_tokens\":100,\"do_sample\":false}}' \\ --header='Content-Type:application/json' \\ 'http://127.0.0.1:1025/infer' Multi-machine service-based inference (Ling plus) All of the following commands need to be executed simultaneously on all machines.\nTo enable multi-machine service-based inference, you need to configure a multi-machine ranktable file.\nGet the ","wordCount":"1574","inLanguage":"en","datePublished":"2025-05-08T00:00:03+08:00","dateModified":"2025-05-08T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ling/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ling: A MoE LLM Provided and Open-sourced by InclusionAI</h1><div class=post-meta><span title='2025-05-08 00:00:03 +0800 +0800'>May 8, 2025</span>&nbsp;Â·&nbsp;8 min&nbsp;Â·&nbsp;1574 words&nbsp;Â·&nbsp;inclusionAI, Ant Group</div></div></div><main class=main><article class=post-single><div class=post-content><p align=center>ðŸ¤— <a href=https://huggingface.co/inclusionAI>Hugging Face</a>&nbsp&nbsp | &nbsp&nbspðŸ¤– <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.</p><p>Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems. Furthermore, the open-source nature of Ling promotes collaboration and innovation within the AI community, fostering a diverse range of use cases and enhancements.</p><p>As more developers and researchers engage with the platform, we can expect rapid advancements and improvements, leading to even more sophisticated applications. This collaborative approach accelerates development and ensures that the models remain at the forefront of technology, addressing emerging challenges in various fields.</p><h2 id=update>Update<a hidden class=anchor aria-hidden=true href=#update>#</a></h2><ul><li>[2025-5-10] Ling-lite-1.5 has been released! It achieves significant progress in reasoning ability compared with previous Ling-lite.</li><li>[2025-4-15] Ling-lite is upgraded to Ling-lite-0415. The new model demonstrates notable improvements over its predecessor, Ling-lite-0220, especially on code and math.</li></ul><h2 id=model-downloads>Model Downloads<a hidden class=anchor aria-hidden=true href=#model-downloads>#</a></h2><p>You can download the following table to see the various parameters for your use case. If you are located in mainland China, we also provide the model on ModelScope.cn to speed up the download process.</p><table><thead><tr><th style=text-align:center><strong>Model</strong></th><th style=text-align:center><strong>#Total Params</strong></th><th style=text-align:center><strong>#Activated Params</strong></th><th style=text-align:center><strong>Context Length</strong></th><th style=text-align:center><strong>Download</strong></th></tr></thead><tbody><tr><td style=text-align:center>Ling-lite-base-1.5</td><td style=text-align:center>16.8B</td><td style=text-align:center>2.75B</td><td style=text-align:center>128K</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ling-lite-base-1.5>ðŸ¤— HuggingFace</a><br><a href=https://www.modelscope.cn/models/inclusionAI/Ling-lite-base-1.5>ðŸ¤– ModelScope</a></td></tr><tr><td style=text-align:center>Ling-lite-1.5</td><td style=text-align:center>16.8B</td><td style=text-align:center>2.75B</td><td style=text-align:center>128K</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ling-lite-1.5>ðŸ¤— HuggingFace</a><br><a href=https://www.modelscope.cn/models/inclusionAI/Ling-lite-1.5>ðŸ¤– ModelScope</a></td></tr><tr><td style=text-align:center>Ling-plus-base</td><td style=text-align:center>290B</td><td style=text-align:center>28.8B</td><td style=text-align:center>64K</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ling-plus-base>ðŸ¤— HuggingFace</a><br><a href=https://www.modelscope.cn/models/inclusionAI/Ling-plus-base>ðŸ¤– ModelScope</a></td></tr><tr><td style=text-align:center>Ling-plus</td><td style=text-align:center>290B</td><td style=text-align:center>28.8B</td><td style=text-align:center>64K</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ling-plus>ðŸ¤— HuggingFace</a><br><a href=https://www.modelscope.cn/models/inclusionAI/Ling-plus>ðŸ¤– ModelScope</a></td></tr><tr><td style=text-align:center>Ling-coder-lite-base</td><td style=text-align:center>16.8B</td><td style=text-align:center>2.75B</td><td style=text-align:center>16K</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ling-Coder-lite-base>ðŸ¤— HuggingFace</a><br><a href=https://modelscope.cn/models/inclusionAI/Ling-Coder-lite-base>ðŸ¤– ModelScope</a></td></tr><tr><td style=text-align:center>Ling-coder-lite</td><td style=text-align:center>16.8B</td><td style=text-align:center>2.75B</td><td style=text-align:center>16K</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ling-Coder-lite>ðŸ¤— HuggingFace</a><br><a href=https://modelscope.cn/models/inclusionAI/Ling-Coder-lite>ðŸ¤– ModelScope</a></td></tr></tbody></table><p>Note: If you are interested in previous version, please visit the past model collections in <a href=https://huggingface.co/inclusionAI>Huggingface</a> or <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a>.</p><h2 id=evaluation>Evaluation<a hidden class=anchor aria-hidden=true href=#evaluation>#</a></h2><h3 id=ling-lite>Ling-lite<a hidden class=anchor aria-hidden=true href=#ling-lite>#</a></h3><h4 id=standard-benchmarks>Standard Benchmarks<a hidden class=anchor aria-hidden=true href=#standard-benchmarks>#</a></h4><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>#shots</strong></th><th style=text-align:center><strong>Ling-lite-1.5</strong></th><th style=text-align:center><strong>Ling-lite</strong></th><th style=text-align:center><strong>Qwen3-4B-Instruct</strong></th><th style=text-align:center><strong>Qwen3-8B-Instruct</strong></th><th style=text-align:center><strong>Moonlight-16B-A3B-Instruct</strong></th><th style=text-align:center><strong>LLaMA3.1-8B</strong></th></tr></thead><tbody><tr><td style=text-align:center>MMLU(EM)</td><td style=text-align:center>5</td><td style=text-align:center><strong>74.33</strong></td><td style=text-align:center>71.27</td><td style=text-align:center>70.09</td><td style=text-align:center>75.97</td><td style=text-align:center>70.74</td><td style=text-align:center>68.67</td></tr><tr><td style=text-align:center>GPQA(Pass@1)</td><td style=text-align:center>0</td><td style=text-align:center><strong>36.55</strong></td><td style=text-align:center>29.73</td><td style=text-align:center>40.4</td><td style=text-align:center>47.10</td><td style=text-align:center>19.51</td><td style=text-align:center>27.59</td></tr><tr><td style=text-align:center>HumanEval(Pass@1)</td><td style=text-align:center>0</td><td style=text-align:center><strong>87.27</strong></td><td style=text-align:center>84.38</td><td style=text-align:center>81.94</td><td style=text-align:center>85.29</td><td style=text-align:center>72.94</td><td style=text-align:center>67.23</td></tr><tr><td style=text-align:center>LiveCodeBench 2408-2502 (Pass@1)</td><td style=text-align:center>0</td><td style=text-align:center><strong>22.7</strong></td><td style=text-align:center>18.94</td><td style=text-align:center>21.8</td><td style=text-align:center>26.88</td><td style=text-align:center>14.76</td><td style=text-align:center>18.41</td></tr><tr><td style=text-align:center>LCBench(pass@1)</td><td style=text-align:center>0</td><td style=text-align:center><strong>60.37</strong></td><td style=text-align:center>46.57</td><td style=text-align:center>48.61</td><td style=text-align:center>60.03</td><td style=text-align:center>28.39</td><td style=text-align:center>23.13</td></tr><tr><td style=text-align:center>Math(EM)</td><td style=text-align:center>0</td><td style=text-align:center><strong>82.62</strong></td><td style=text-align:center>72.80</td><td style=text-align:center>81.46</td><td style=text-align:center>82.70</td><td style=text-align:center>67.1</td><td style=text-align:center>52.42</td></tr><tr><td style=text-align:center>AIME2024(pass@1)</td><td style=text-align:center>0</td><td style=text-align:center><strong>21.88</strong></td><td style=text-align:center>10.21</td><td style=text-align:center>20.62</td><td style=text-align:center>26.25</td><td style=text-align:center>6.88</td><td style=text-align:center>7.29</td></tr><tr><td style=text-align:center>OlympiadBench(pass@1)</td><td style=text-align:center>0</td><td style=text-align:center><strong>52.30</strong></td><td style=text-align:center>36.44</td><td style=text-align:center>54.33</td><td style=text-align:center>56.11</td><td style=text-align:center>32.85</td><td style=text-align:center>17.04</td></tr><tr><td style=text-align:center>BBH(EM)</td><td style=text-align:center>0</td><td style=text-align:center><strong>75.75</strong></td><td style=text-align:center>66.38</td><td style=text-align:center>78.21</td><td style=text-align:center>79.33</td><td style=text-align:center>63.45</td><td style=text-align:center>68.05</td></tr><tr><td style=text-align:center>IFEval(Prompt Strict)</td><td style=text-align:center>0</td><td style=text-align:center><strong>77.70</strong></td><td style=text-align:center>77.99</td><td style=text-align:center>81.06</td><td style=text-align:center>83.55</td><td style=text-align:center>49.01</td><td style=text-align:center>73.01</td></tr><tr><td style=text-align:center>BFCL_live</td><td style=text-align:center>0</td><td style=text-align:center><strong>72.15</strong></td><td style=text-align:center>67.93</td><td style=text-align:center>65.35</td><td style=text-align:center>69.83</td><td style=text-align:center>47.14</td><td style=text-align:center>49.98</td></tr></tbody></table><h4 id=context-window>Context Window<a hidden class=anchor aria-hidden=true href=#context-window>#</a></h4><p><img loading=lazy src=https://github.com/inclusionAI/Ling/blob/master/figures/needle_testing.png alt></p><p>Evaluation results on the <code>Needle In A Haystack</code> (NIAH) tests. Ling-lite-1.5 has improved long text generation capability and performs well across most context window lengths up to <strong>128K</strong>.</p><h2 id=quickstart>Quickstart<a hidden class=anchor aria-hidden=true href=#quickstart>#</a></h2><h3 id=-hugging-face-transformers>ðŸ¤— Hugging Face Transformers<a hidden class=anchor aria-hidden=true href=#-hugging-face-transformers>#</a></h3><p>Here is a code snippet to show you how to use the chat model with <code>transformers</code>:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoModelForCausalLM</span><span class=p>,</span> <span class=n>AutoTokenizer</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_name</span> <span class=o>=</span> <span class=s2>&#34;inclusionAI/Ling-lite-1.5&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>AutoModelForCausalLM</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>model_name</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>device_map</span><span class=o>=</span><span class=s2>&#34;auto&#34;</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>tokenizer</span> <span class=o>=</span> <span class=n>AutoTokenizer</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;Give me a short introduction to large language models.&#34;</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;system&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=s2>&#34;You are Ling, an assistant created by inclusionAI&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span><span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;user&#34;</span><span class=p>,</span> <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=n>prompt</span><span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>messages</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>tokenize</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model_inputs</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=p>([</span><span class=n>text</span><span class=p>],</span> <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>model_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=n>output_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>input_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>input_ids</span><span class=p>,</span> <span class=n>output_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>model_inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>response</span> <span class=o>=</span> <span class=n>tokenizer</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span><span class=n>generated_ids</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span></code></pre></div><h3 id=-modelscope>ðŸ¤– ModelScope<a hidden class=anchor aria-hidden=true href=#-modelscope>#</a></h3><p>If you&rsquo;re in mainland China, we strongly recommend you to use our model from ðŸ¤– <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a>.</p><h2 id=deployment>Deployment<a hidden class=anchor aria-hidden=true href=#deployment>#</a></h2><h3 id=vllm>vLLM<a hidden class=anchor aria-hidden=true href=#vllm>#</a></h3><p>vLLM supports offline batched inference or launching an OpenAI-Compatible API Service for online inference.</p><h4 id=environment-preparation>Environment Preparation<a hidden class=anchor aria-hidden=true href=#environment-preparation>#</a></h4><p>Since the Pull Request (PR) has not been submitted to the vLLM community at this stage, please prepare the environment by following the steps below:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>git clone -b  v0.7.3 https://github.com/vllm-project/vllm.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> vllm
</span></span><span class=line><span class=cl>git apply Ling/inference/vllm/bailing_moe.patch
</span></span><span class=line><span class=cl>pip install -e .
</span></span></code></pre></div><h4 id=offline-inference>Offline Inference:<a hidden class=anchor aria-hidden=true href=#offline-inference>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>from transformers import AutoTokenizer
</span></span><span class=line><span class=cl>from vllm import LLM, SamplingParams
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>tokenizer</span> <span class=o>=</span> AutoTokenizer.from_pretrained<span class=o>(</span><span class=s2>&#34;inclusionAI/Ling-lite-1.5&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>sampling_params</span> <span class=o>=</span> SamplingParams<span class=o>(</span><span class=nv>temperature</span><span class=o>=</span>0.7, <span class=nv>top_p</span><span class=o>=</span>0.8, <span class=nv>repetition_penalty</span><span class=o>=</span>1.05, <span class=nv>max_tokens</span><span class=o>=</span>16384<span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>llm</span> <span class=o>=</span> LLM<span class=o>(</span><span class=nv>model</span><span class=o>=</span><span class=s2>&#34;inclusionAI/Ling-lite&#34;</span>, <span class=nv>dtype</span><span class=o>=</span><span class=s1>&#39;bfloat16&#39;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>prompt</span> <span class=o>=</span> <span class=s2>&#34;Give me a short introduction to large language models.&#34;</span>
</span></span><span class=line><span class=cl><span class=nv>messages</span> <span class=o>=</span> <span class=o>[</span>
</span></span><span class=line><span class=cl>    <span class=o>{</span><span class=s2>&#34;role&#34;</span>: <span class=s2>&#34;system&#34;</span>, <span class=s2>&#34;content&#34;</span>: <span class=s2>&#34;You are Ling, an assistant created by inclusionAI&#34;</span><span class=o>}</span>,
</span></span><span class=line><span class=cl>    <span class=o>{</span><span class=s2>&#34;role&#34;</span>: <span class=s2>&#34;user&#34;</span>, <span class=s2>&#34;content&#34;</span>: prompt<span class=o>}</span>
</span></span><span class=line><span class=cl><span class=o>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nv>text</span> <span class=o>=</span> tokenizer.apply_chat_template<span class=o>(</span>
</span></span><span class=line><span class=cl>    messages,
</span></span><span class=line><span class=cl>    <span class=nv>tokenize</span><span class=o>=</span>False,
</span></span><span class=line><span class=cl>    <span class=nv>add_generation_prompt</span><span class=o>=</span>True
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=nv>outputs</span> <span class=o>=</span> llm.generate<span class=o>([</span>text<span class=o>]</span>, sampling_params<span class=o>)</span>
</span></span></code></pre></div><h4 id=online-inference>Online Inference:<a hidden class=anchor aria-hidden=true href=#online-inference>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>vllm serve inclusionAI/Ling-lite <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>              --tensor-parallel-size <span class=m>2</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>              --pipeline-parallel-size <span class=m>1</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>              --use-v2-block-manager <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>              --gpu-memory-utilization 0.90
</span></span></code></pre></div><p>To handle long context in vLLM using YaRN, we need to follow these two steps:</p><ol><li>Add a <code>rope_scaling</code> field to the model&rsquo;s <code>config.json</code> file, for example:</li></ol><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=err>...,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;rope_scaling&#34;</span><span class=p>:</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;factor&#34;</span><span class=p>:</span> <span class=mf>4.0</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;original_max_position_embeddings&#34;</span><span class=p>:</span> <span class=mi>32768</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=nt>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;yarn&#34;</span>
</span></span><span class=line><span class=cl>  <span class=p>}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><ol start=2><li>Use an additional parameter <code>--max-model-len</code> to specify the desired maximum context length when starting the vLLM service.</li></ol><p>For detailed guidance, please refer to the vLLM <a href=https://docs.vllm.ai/en/latest/><code>instructions</code></a>.</p><h3 id=mindie>MindIE<a hidden class=anchor aria-hidden=true href=#mindie>#</a></h3><p>This subject outlines the primary processes for executing a Ling MoE model with specified hardware and the MindIE inference framework.</p><h4 id=configure-preparation>Configure preparation<a hidden class=anchor aria-hidden=true href=#configure-preparation>#</a></h4><p>Create a model directory on the host for downloading, the directory example is: /root/models&rsquo;, which is used to mount the docker container later.</p><p>Download the mindie-related configuration from github:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>cd</span> /root/models
</span></span><span class=line><span class=cl>git clone git@github.com:inclusionAI/Ling.git
</span></span></code></pre></div><h4 id=machine-network-environment-check>Machine network environment check<a hidden class=anchor aria-hidden=true href=#machine-network-environment-check>#</a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Check the physical link</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -lldp -g <span class=p>|</span> grep Ifname<span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=c1># Check the links</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -link -g <span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=c1># Check your network health</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -net_health -g <span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=c1># Check whether the detected IP address is correctly configured</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -netdetect -g <span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=c1># Check whether the gateway is configured correctly</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -gateway -g <span class=p>;</span> <span class=k>done</span>
</span></span><span class=line><span class=cl><span class=c1># Check the consistency of the underlying TLS verification behavior of the NPU, recommend that all 0 be</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -tls -g <span class=p>;</span> <span class=k>done</span> <span class=p>|</span> grep switch
</span></span><span class=line><span class=cl><span class=c1># The underlying TLS check line of the NPU is set to 0</span>
</span></span><span class=line><span class=cl><span class=k>for</span> i in <span class=o>{</span>0..7<span class=o>}</span><span class=p>;</span> <span class=k>do</span> hccn_tool -i <span class=nv>$i</span> -tls -s <span class=nb>enable</span> 0<span class=p>;</span> <span class=k>done</span>
</span></span></code></pre></div><h4 id=pull-the-image>Pull the image<a hidden class=anchor aria-hidden=true href=#pull-the-image>#</a></h4><p>Go to <a href=https://www.hiascend.com/developer/ascendhub>Ascend Community/Development Resources</a> and pull the mindie image</p><p>Image version: 1.0.0-800I-A2-py311-openeuler24.03-lts</p><p>The versions of each component are as follows:</p><table><thead><tr><th>Component</th><th>Version</th></tr></thead><tbody><tr><td>MindIE</td><td>1.0.0</td></tr><tr><td>CANN</td><td>8.0.0</td></tr><tr><td>PTA</td><td>6.0.0.beta1</td></tr><tr><td>HDK</td><td>24.1.0</td></tr></tbody></table><h4 id=container-startup-and-configuration-changes>Container startup and configuration changes<a hidden class=anchor aria-hidden=true href=#container-startup-and-configuration-changes>#</a></h4><h5 id=start-the-container>Start the container<a hidden class=anchor aria-hidden=true href=#start-the-container>#</a></h5><p>Execute the following startup command (reference):</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>docker run -itd --privileged --name<span class=o>=</span>container name --net<span class=o>=</span>host <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--shm-size 500g <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci0 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci1 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci2 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci3 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci4 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci5 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci6 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci7 <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/davinci_manager <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device<span class=o>=</span>/dev/hisi_hdc <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--device /dev/devmm_svm <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-v /usr/local/Ascend/driver:/usr/local/Ascend/driver <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-v /usr/local/Ascend/firmware:/usr/local/Ascend/firmware <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-v /usr/local/sbin/npu-smi:/usr/local/sbin/npu-smi <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-v /usr/local/sbin:/usr/local/sbin <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-v /etc/hccn.conf:/etc/hccn.conf <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>-v /root/models:/home/HwHiAiUser/Ascend <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>mindie: 1.0.0-XXX-800I-A2-arm64-py3.11 <span class=o>(</span>modified according to the name of the loaded image<span class=o>)</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>bash
</span></span></code></pre></div><h5 id=download-the-model>Download the model<a hidden class=anchor aria-hidden=true href=#download-the-model>#</a></h5><p>In this case, we use ModelScope to download the model, and install ModelScope first:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>pip install modelscope
</span></span></code></pre></div><p>Download the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># The model takes a long time to download and can be executed in the background</span>
</span></span><span class=line><span class=cl>nohup modelscope download --model inclusionAI/Ling-plus --local_dir /home/HwHiAiUser/Ascend/Ling_plus 2&gt;<span class=p>&amp;</span><span class=m>1</span> &gt; /tmp/ling_plus.log <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>nohup modelscope download --model inclusionAI/Ling-plus-base --local_dir /home/HwHiAiUser/Ascend/Ling_plus_base 2&gt;<span class=p>&amp;</span><span class=m>1</span> &gt; /tmp/ling_plus_base.log <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>nohup modelscope download --model inclusionAI/Ling-lite --local_dir /home/HwHiAiUser/Ascend/Ling_lite 2&gt;<span class=p>&amp;</span><span class=m>1</span> &gt; /tmp/ling_lite.log <span class=p>&amp;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>nohup modelscope download --model inclusionAI/Ling-lite-base --local_dir /home/HwHiAiUser/Ascend/Ling_lite_base 2&gt;<span class=p>&amp;</span><span class=m>1</span> &gt; /tmp/ling_lite_base.log <span class=p>&amp;</span>
</span></span></code></pre></div><p>After the download is completed, you need to change the file permissions, otherwise an error will be reported when MindIE-Service is started:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>chmod -R <span class=m>750</span> *.json *.py
</span></span></code></pre></div><h5 id=model-weight-format-conversion>Model weight format conversion<a hidden class=anchor aria-hidden=true href=#model-weight-format-conversion>#</a></h5><blockquote><p>This section applies to the Ling Lite model, the Ling Plus model does not need to worry about this chapter</p></blockquote><p>mindie supports safetensors format weights, if the download weights are not in safetensors format, you need to convert the weights, take Ling Lite as an example, the conversion command is as follows:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Convert Ling lite</span>
</span></span><span class=line><span class=cl>python /home/HwHiAiUser/Ascend/Ling/inference/mindie/convert_bin_to_safetensor.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>cd</span> /home/HwHiAiUser/Ascend/Ling_lite
</span></span><span class=line><span class=cl>cp README.md configuration.json config.json special_tokens_map.json modeling_bailing_moe.py tokenizer.json tokenizer_config.json ../Ling_lite_safetensor/
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Convert Ling lite base</span>
</span></span><span class=line><span class=cl>python /home/HwHiAiUser/Ascend/Ling/inference/mindie/convert_bin_to_safetensor_base.py
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>cd</span> /home/HwHiAiUser/Ascend/Ling_lite_base
</span></span><span class=line><span class=cl>cp README.md configuration.json config.json special_tokens_map.json modeling_bailing_moe.py tokenizer.json tokenizer_config.json ../Ling_lite_base_safetensor/
</span></span></code></pre></div><p>The path of loading the Ling Lite model is changed to &lsquo;/home/HwHiAiUser/Ascend/Ling_lite_safetensor&rsquo;, and the path of the Ling Lite Base model is changed to &lsquo;/home/HwHiAiUser/Ascend/Ling_lite_base_safetensor&rsquo;</p><h5 id=change-the-model-configuration>Change the model configuration<a hidden class=anchor aria-hidden=true href=#change-the-model-configuration>#</a></h5><p>The default model configuration file (config.json) mindie cannot be loaded directly, and needs to be changed:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Adapt to mindie&#39;s Ling lite model configuration</span>
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json.bak
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/model_chat_config.json /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json
</span></span><span class=line><span class=cl>chmod <span class=m>750</span> /home/HwHiAiUser/Ascend/Ling_lite_safetensor/config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Adapt to mindie&#39;s Ling lite base model configuration</span>
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json.bak
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/model_base_config.json /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json
</span></span><span class=line><span class=cl>chmod <span class=m>750</span> /home/HwHiAiUser/Ascend/Ling_lite_base_safetensor/config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Adapt to mindie&#39;s Ling plus model configuration</span>
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling_plus/config.json /home/HwHiAiUser/Ascend/Ling_plus/config.json.bak
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/plus/model_chat_config.json /home/HwHiAiUser/Ascend/Ling_plus/config.json
</span></span><span class=line><span class=cl>chmod <span class=m>750</span> /home/HwHiAiUser/Ascend/Ling_plus/config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Adapt to mindie&#39;s Ling plus base model configuration</span>
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling_plus_base/config.json /home/HwHiAiUser/Ascend/Ling_plus_base/config.json.bak
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/plus/model_base_config.json /home/HwHiAiUser/Ascend/Ling_plus_base/config.json
</span></span><span class=line><span class=cl>chmod <span class=m>750</span> /home/HwHiAiUser/Ascend/Ling_plus_base/config.json
</span></span></code></pre></div><p>Execute the shell script that adapts the mindie to the Ling model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>bash /home/HwHiAiUser/Ascend/Ling/inference/mindie/patch_atb_llm.sh
</span></span></code></pre></div><h4 id=stand-alone-servitization-inference-ling-lite>Stand-alone Servitization Inference (Ling lite)<a hidden class=anchor aria-hidden=true href=#stand-alone-servitization-inference-ling-lite>#</a></h4><p>Set the underlying environment variables:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=nb>source</span> /usr/local/Ascend/atb-models/set_env.sh
</span></span></code></pre></div><p>Set different mindie configurations according to the model type:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Ling Lite</span>
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/config.json /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># Ling Lite base</span>
</span></span><span class=line><span class=cl>cp /home/HwHiAiUser/Ascend/Ling/inference/mindie/lite/config.base.json /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json
</span></span></code></pre></div><p>Start the mindie service:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl>chmod <span class=m>640</span> /usr/local/Ascend/mindie/latest/mindie-service/conf/config.json
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nb>cd</span> <span class=nv>$MIES_INSTALL_PATH</span>
</span></span><span class=line><span class=cl>nohup ./bin/mindieservice_daemon &gt; /tmp/service.log 2&gt;<span class=p>&amp;</span><span class=m>1</span> <span class=p>&amp;</span>
</span></span></code></pre></div><p>Check /tmp/service.log to check whether the output is Daemon start success!, if so, it means that MindIE-Service has started successfully.</p><p>Test if the request is correct:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bash data-lang=bash><span class=line><span class=cl><span class=c1># Chat model</span>
</span></span><span class=line><span class=cl>wget -O- --post-data<span class=o>=</span><span class=s2>&#34;{\&#34;messages\&#34;:[{\&#34;role\&#34;: \&#34;system\&#34;, \&#34;content\&#34;: \&#34;You are a helpful assistant.\&#34;}, {\&#34;role\&#34;: \&#34;user\&#34;, \&#34;content\&#34;: \&#34;Who are you?\&#34;}], \&#34;stream\&#34;: false, \&#34;max_tokens\&#34;:100, \&#34;model\&#34;: \&#34;bailing_moe\&#34;, \&#34;temperature\&#34;:0}&#34;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--header<span class=o>=</span><span class=s1>&#39;Content-Type:application/json&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=s1>&#39;http://127.0.0.1:1025/v1/chat/completions&#39;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># base model</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>wget -O- --post-data<span class=o>=</span><span class=s1>&#39;{&#34;inputs&#34;:&#34;My name is Olivier and I&#34;,&#34;stream&#34;:false,&#34;parameters&#34;:{&#34;temperature&#34;:1,&#34;max_new_tokens&#34;:100,&#34;do_sample&#34;:false}}&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span>--header<span class=o>=</span><span class=s1>&#39;Content-Type:application/json&#39;</span> <span class=se>\
</span></span></span><span class=line><span class=cl><span class=se></span><span class=s1>&#39;http://127.0.0.1:1025/infer&#39;</span>
</span></span></code></pre></div><h4 id=multi-machine-service-based-inference-ling-plus>Multi-machine service-based inference (Ling plus)<a hidden class=anchor aria-hidden=true href=#multi-machine-service-based-inference-ling-plus>#</a></h4><p>All of the following commands need to be executed simultaneously on all machines.</p><p>To enable multi-machine service-based inference, you need to configure a multi-machine ranktable file.</p><ul><li>Get the</li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
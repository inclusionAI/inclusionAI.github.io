<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Omnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0&rsquo;s sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-flash-omni-preview/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-flash-omni-preview/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-flash-omni-preview/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation"><meta property="og:description" content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Omnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0&rsquo;s sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-flash-omni-preview/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-10-28T00:00:03+08:00"><meta property="article:modified_time" content="2025-10-28T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation"><meta name=twitter:description content="GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope
Omnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0&rsquo;s sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation","item":"https://inclusionai.github.io/blog/ming-flash-omni-preview/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation","name":"Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation","description":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nOmnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0\u0026rsquo;s sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition.","keywords":[],"articleBody":"GITHUB ü§ó Hugging FaceÔΩú ü§ñ ModelScope\nOmnimodal Ming-omni series update! Ming-flash-omni-Preview is the first open-source omnimodal large model with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0‚Äôs sparse MoE architecture, Ming-flash-omni-Preview has a total of 103B parameters with 9B activated. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a leading level among open-source omnimodal models, with particularly outstanding performance in controllable image generation, streaming video understanding, and speech recognition.\nCapability Overview Controllable Image Generation For image generation, Ming-flash-omni-Preview pioneers the Generative Segmentation Paradigm, reframing ‚Äúimage segmentation‚Äù as a semantic-preserving editing task (Generative Segmentation-as-Editing), achieving fine-grained spatial semantic control. Ming-flash-omni-Preview achieved a score of 0.90 on the GenEval benchmark, surpassing all non-reinforcement learning generation methods and demonstrating exceptional controllability. Streaming Video Understanding Users often have a need to engage in continuous dialogue with AI based on real-world scenarios and to use AI to understand those scenarios. Ming-flash-omni-Preview can effectively fulfill these needs. As shown in the video below, Ming-flash-omni-Preview can achieve fine-grained understanding of streaming video, recognizing objects and interactions within the video, and providing relevant understanding and explanations in real-time to support users in practical scenarios. Speech and Dialect Understanding Ming-flash-omni-Preview can achieve Context-Aware Speech Recognition (ContextASR) and dialect recognition, achieving SOTA across all 12 ContextASR subtasks. Its understanding ability for 15 Chinese dialects, including Hunanese, Minnanese, and Cantonese, is significantly enhanced, effectively providing translation and real-time understanding support for users who might be lost in an unfamiliar dialect. Voice Cloning Ming-flash-omni-Preview‚Äôs speech generation has been upgraded from discrete tokenizers to continuous tokenizers, significantly enhancing voice cloning capabilities. It exhibits high stability in mixed Chinese-English pronunciation, and can effectively clone the voice from the original conversation into newly generated dialogue. The seed-tts-zh WER metric is 0.99, surpassing Qwen3-omni and seed-tts. Model Architecture and Capability Introduction Model structure diagram of Ming-flash-omni-Preview:\nCompared to Ming-lite-omni-1.5, Ming-flash-omni-Preview primarily features the following technical optimizations:\nOmnimodal Training Based on Sparse Expert Architecture Ming-flash-omni-Preview extends the Ling-Flash-2.0 sparse MoE architecture to the omni-modality. It models the distribution and routing strategy of each modality based on the modality-level routing proposed by Ming-lite-omni, achieving ‚Äúlarge capacity, small activation‚Äù for each modality. By introducing VideoRoPE in the Attention layer, it enhances spatiotemporal modeling for long videos, improving video interaction capability. Additionally, in terms of training strategy:\nStable Sparse Training: Utilizes a mixed expert balancing scheme (combining auxiliary load balancing loss with router bias updates) to ensure uniform activation and convergence of omnimodal training under the sparse MoE architecture; Context-Aware ASR Training Paradigm: For speech training tasks, task/domain information input is used as the decoding condition, significantly improving proper noun recognition and transcription consistency. It also introduces high-quality dialect training corpora, leading to a significant increase in recognition accuracy for 15 Chinese dialects, including Hunanese, Minnanese, and Cantonese. Unified Generative Segmentation and Editing The core challenge in building a unified multimodal model lies in how to efficiently integrate image understanding and generation capabilities. Our Ming-lite-omni-1.5 achieved this by freezing the language pathway and injecting hierarchical semantics using multi-scale QueryTokens, thereby allowing the generation objective to better integrate with the understanding task while preserving understanding performance. Although this training strategy improved stability, the fundamental differences between the learning objectives of understanding and generation mean that even with the introduction of hierarchical semantics, fine-grained visual knowledge (such as object attributes and spatial relationships) remains difficult to efficiently transfer to high-precision generation and editing tasks, thus limiting the improvement in model generation quality and controllability. To overcome this bottleneck, Ming-flash-omni-Preview proposes the ‚ÄúGenerative Segmentation-as-Editing‚Äù collaborative training paradigm. This paradigm reframes image segmentation as a semantic-preserving editing task (e.g., ‚Äúpaint the banana purple‚Äù). The key assistance provided by this design is: It forcibly unifies the understanding and generation objectives ‚Äî successful editing must rely on precise understanding of the object‚Äôs outline, and the editing quality directly provides supervision signals for understanding. This paradigm directly enhances the model‚Äôs fine-grained spatiotemporal semantic control ability and indirectly solves the compositionality problem in pure text-to-image generation. In the GenEval benchmark, Ming-flash-omni-Preview achieved a score of 0.90, surpassing all leading non-reinforcement learning (non-RL) methods; in the GEdit benchmark, the average score for precise editing tasks such as object deletion and object replacement improved from 6.9 to 7.9. These two results collectively prove that the fine-grained spatiotemporal semantic control capability gained through the ‚ÄúGenerative Segmentation-as-Editing‚Äù training not only significantly improves performance in precise editing tasks but can also effectively generalize to pure text-driven image generation tasks.\nEfficient Omnimodal Training Architecture Training omnimodal foundation models faces two major challenges: data heterogeneity (varied shapes of multi-modal inputs) and model heterogeneity (difficulty in parallelizing modality-specific encoders). These issues lead to load imbalance, memory fragmentation, and pipeline bubbles, severely slowing down the training speed. To address these problems, we made two key optimizations based on the Megatron-LM framework when training the Ming-flash-omni-Preview model:\nSequence Packing: Solves data heterogeneity. Varied-length sequences are densely packed into fixed-length batches, significantly improving memory utilization and computational density; Flexible Encoder Sharding: Solves model heterogeneity. Extends Megatron-LM to support fine-grained sharding of modality encoders across DP/PP/TP, eliminating pipeline bubbles and achieving load balancing. These optimization measures resulted in a doubling of the training throughput of Ming-flash-omni-Preview compared to the baseline. Getting Started with Ming-flash-omni-Preview Our model and code are open source. We welcome everyone to try, provide feedback, and exchange ideas:\nGitHub: https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-flash-omni-Preview ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview Future Plan The version released this time is the Ming-flash-omni-Preview, and the current version has some imperfections:\nVisual-Text Understanding Capability: Although Ming-flash-omni-Preview‚Äôs overall performance is leading among omnimodal models, there is still a gap compared to SOTA dedicated VL large models. We will continue to explore the performance upper limit of omnimodal models. Speech Capability: Overall performance in speech recognition and speech synthesis is leading. The effects of multi-turn speech dialogue and high-quality voice cloning are our next optimization priorities. Image Generation Capability: The model achieved a score of 0.90 on the GenEval benchmark, demonstrating good controllability, and already possesses text generation and editing capabilities. However, there is still room for improvement in rendering and editing text with complex layouts, as well as generating specific IP characters. We are continuously optimizing the user experience of Ming-flash-omni-Preview. We welcome you to provide feedback via community discussion or issues. The official version will be released soon.\n","wordCount":"1062","inLanguage":"en","datePublished":"2025-10-28T00:00:03+08:00","dateModified":"2025-10-28T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-flash-omni-preview/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-flash-omni-Preview: The Hundred Billion-Scale MoE Unifying Perception and Generation</h1><div class=post-meta><span title='2025-10-28 00:00:03 +0800 +0800'>October 28, 2025</span>&nbsp;¬∑&nbsp;5 min&nbsp;¬∑&nbsp;1062 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-flash-omni-preview/>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> ü§ó <a href=https://huggingface.co/inclusionAI/Ming-flash-omni-Preview>Hugging Face</a>ÔΩú ü§ñ <a href=https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview>ModelScope</a></p><p>Omnimodal Ming-omni series update! <strong>Ming-flash-omni-Preview</strong> is the <strong>first open-source omnimodal large model</strong> with a parameter scale reaching the hundred billion-Scale level. Based on Ling 2.0&rsquo;s sparse MoE architecture, Ming-flash-omni-Preview has a total of <strong>103B parameters</strong> with <strong>9B activated</strong>. Compared to the previous version Ming-lite-omni-1.5, Ming-flash-omni-Preview has improved in both omnimodal understanding and generation capabilities. The overall performance across various modalities has reached a <strong>leading level among open-source omnimodal models</strong>, with particularly outstanding performance in <strong>controllable image generation, streaming video understanding, and speech recognition</strong>.</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_drbxn1/afts/img/5hflRY595xwAAAAAgBAAAAgADkliAQFr/original alt=performance></p><h2 id=capability-overview>Capability Overview<a hidden class=anchor aria-hidden=true href=#capability-overview>#</a></h2><h3 id=controllable-image-generation>Controllable Image Generation<a hidden class=anchor aria-hidden=true href=#controllable-image-generation>#</a></h3><p>For image generation, Ming-flash-omni-Preview pioneers the <strong>Generative Segmentation Paradigm</strong>, reframing &ldquo;image segmentation&rdquo; as a <strong>semantic-preserving editing task (Generative Segmentation-as-Editing)</strong>, achieving fine-grained spatial semantic control. Ming-flash-omni-Preview achieved a score of <strong>0.90</strong> on the GenEval benchmark, surpassing all non-reinforcement learning generation methods and demonstrating exceptional controllability.
<video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/cb4mSp1jTwQAAAAAgIAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h3 id=streaming-video-understanding>Streaming Video Understanding<a hidden class=anchor aria-hidden=true href=#streaming-video-understanding>#</a></h3><p>Users often have a need to engage in continuous dialogue with AI based on real-world scenarios and to use AI to understand those scenarios. Ming-flash-omni-Preview can effectively fulfill these needs. As shown in the video below, Ming-flash-omni-Preview can achieve <strong>fine-grained understanding of streaming video</strong>, recognizing objects and interactions within the video, and providing relevant understanding and explanations in real-time to support users in practical scenarios.
<video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/n6k6SqtCCqMAAAAAgJAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h3 id=speech-and-dialect-understanding>Speech and Dialect Understanding<a hidden class=anchor aria-hidden=true href=#speech-and-dialect-understanding>#</a></h3><p>Ming-flash-omni-Preview can achieve <strong>Context-Aware Speech Recognition (ContextASR)</strong> and <strong>dialect recognition</strong>, achieving <strong>SOTA</strong> across all 12 ContextASR subtasks. Its understanding ability for <strong>15 Chinese dialects</strong>, including Hunanese, Minnanese, and Cantonese, is significantly enhanced, effectively providing translation and real-time understanding support for users who might be lost in an unfamiliar dialect.
<video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/iEf7QK3W3m4AAAAAgBAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h3 id=voice-cloning>Voice Cloning<a hidden class=anchor aria-hidden=true href=#voice-cloning>#</a></h3><p>Ming-flash-omni-Preview&rsquo;s speech generation has been upgraded from discrete tokenizers to <strong>continuous tokenizers</strong>, significantly enhancing voice cloning capabilities. It exhibits high stability in <strong>mixed Chinese-English pronunciation</strong>, and can effectively clone the voice from the original conversation into newly generated dialogue. The seed-tts-zh WER metric is <strong>0.99</strong>, surpassing Qwen3-omni and seed-tts.
<video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/Ru5dTrMPb30AAAAAgBAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h2 id=model-architecture-and-capability-introduction>Model Architecture and Capability Introduction<a hidden class=anchor aria-hidden=true href=#model-architecture-and-capability-introduction>#</a></h2><p>Model structure diagram of Ming-flash-omni-Preview:</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_drbxn1/afts/img/MdHMSqYQCqAAAAAAVcAAAAgADkliAQFr/fmt.avif alt=architecture></p><p>Compared to Ming-lite-omni-1.5, Ming-flash-omni-Preview primarily features the following technical optimizations:</p><h3 id=omnimodal-training-based-on-sparse-expert-architecture>Omnimodal Training Based on Sparse Expert Architecture<a hidden class=anchor aria-hidden=true href=#omnimodal-training-based-on-sparse-expert-architecture>#</a></h3><p>Ming-flash-omni-Preview extends the <strong>Ling-Flash-2.0 sparse MoE architecture</strong> to the omni-modality. It models the distribution and routing strategy of each modality based on the <strong>modality-level routing</strong> proposed by Ming-lite-omni, achieving <strong>&ldquo;large capacity, small activation&rdquo;</strong> for each modality. By introducing <strong>VideoRoPE</strong> in the Attention layer, it enhances spatiotemporal modeling for long videos, improving video interaction capability. Additionally, in terms of training strategy:</p><ol><li><strong>Stable Sparse Training:</strong> Utilizes a <strong>mixed expert balancing scheme</strong> (combining auxiliary load balancing loss with router bias updates) to ensure uniform activation and convergence of omnimodal training under the sparse MoE architecture;</li><li><strong>Context-Aware ASR Training Paradigm:</strong> For speech training tasks, task/domain information input is used as the decoding condition, significantly improving proper noun recognition and transcription consistency. It also introduces high-quality dialect training corpora, leading to a significant increase in recognition accuracy for <strong>15 Chinese dialects</strong>, including Hunanese, Minnanese, and Cantonese.</li></ol><h3 id=unified-generative-segmentation-and-editing>Unified Generative Segmentation and Editing<a hidden class=anchor aria-hidden=true href=#unified-generative-segmentation-and-editing>#</a></h3><p>The core challenge in building a unified multimodal model lies in how to efficiently integrate image understanding and generation capabilities. Our Ming-lite-omni-1.5 achieved this by freezing the language pathway and injecting hierarchical semantics using multi-scale QueryTokens, thereby allowing the generation objective to better integrate with the understanding task while preserving understanding performance. Although this training strategy improved stability, the fundamental differences between the learning objectives of understanding and generation mean that even with the introduction of hierarchical semantics, fine-grained visual knowledge (such as object attributes and spatial relationships) remains difficult to efficiently transfer to high-precision generation and editing tasks, thus limiting the improvement in model generation quality and controllability.
To overcome this bottleneck, Ming-flash-omni-Preview proposes the <strong>&ldquo;Generative Segmentation-as-Editing&rdquo; collaborative training paradigm</strong>. This paradigm reframes image segmentation as a semantic-preserving editing task (e.g., &ldquo;paint the banana purple&rdquo;). The key assistance provided by this design is: <strong>It forcibly unifies the understanding and generation objectives</strong> ‚Äî successful editing must rely on precise understanding of the object&rsquo;s outline, and the editing quality directly provides supervision signals for understanding. This paradigm directly enhances the model&rsquo;s fine-grained spatiotemporal semantic control ability and indirectly solves the compositionality problem in pure text-to-image generation.
In the GenEval benchmark, Ming-flash-omni-Preview achieved a score of <strong>0.90</strong>, surpassing all leading non-reinforcement learning (non-RL) methods; in the GEdit benchmark, the average score for precise editing tasks such as object deletion and object replacement improved from <strong>6.9 to 7.9</strong>. These two results collectively prove that the fine-grained spatiotemporal semantic control capability gained through the &ldquo;Generative Segmentation-as-Editing&rdquo; training not only significantly improves performance in precise editing tasks but can also effectively generalize to pure text-driven image generation tasks.</p><h3 id=efficient-omnimodal-training-architecture>Efficient Omnimodal Training Architecture<a hidden class=anchor aria-hidden=true href=#efficient-omnimodal-training-architecture>#</a></h3><p>Training omnimodal foundation models faces two major challenges: data heterogeneity (varied shapes of multi-modal inputs) and model heterogeneity (difficulty in parallelizing modality-specific encoders). These issues lead to load imbalance, memory fragmentation, and pipeline bubbles, severely slowing down the training speed.
To address these problems, we made two key optimizations based on the Megatron-LM framework when training the Ming-flash-omni-Preview model:</p><ol><li><strong>Sequence Packing:</strong> Solves data heterogeneity. Varied-length sequences are densely packed into fixed-length batches, significantly improving memory utilization and computational density;</li><li><strong>Flexible Encoder Sharding:</strong> Solves model heterogeneity. Extends Megatron-LM to support fine-grained sharding of modality encoders across DP/PP/TP, eliminating pipeline bubbles and achieving load balancing.
These optimization measures resulted in a <strong>doubling of the training throughput</strong> of Ming-flash-omni-Preview compared to the baseline.</li></ol><h2 id=getting-started-with-ming-flash-omni-preview>Getting Started with Ming-flash-omni-Preview<a hidden class=anchor aria-hidden=true href=#getting-started-with-ming-flash-omni-preview>#</a></h2><p>Our model and code are open source. We welcome everyone to try, provide feedback, and exchange ideas:</p><ul><li>GitHub: <a href=https://github.com/inclusionAI/Ming>https://github.com/inclusionAI/Ming</a></li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-flash-omni-Preview>https://huggingface.co/inclusionAI/Ming-flash-omni-Preview</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview>https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview</a></li></ul><h2 id=future-plan>Future Plan<a hidden class=anchor aria-hidden=true href=#future-plan>#</a></h2><p>The version released this time is the Ming-flash-omni-Preview, and the current version has some imperfections:</p><ol><li><strong>Visual-Text Understanding Capability:</strong> Although Ming-flash-omni-Preview&rsquo;s overall performance is leading among omnimodal models, there is still a gap compared to SOTA dedicated VL large models. We will continue to explore the performance upper limit of omnimodal models.</li><li><strong>Speech Capability:</strong> Overall performance in speech recognition and speech synthesis is leading. The effects of multi-turn speech dialogue and high-quality voice cloning are our next optimization priorities.</li><li><strong>Image Generation Capability:</strong> The model achieved a score of <strong>0.90</strong> on the GenEval benchmark, demonstrating good controllability, and already possesses text generation and editing capabilities. However, there is still room for improvement in rendering and editing text with complex layouts, as well as generating specific IP characters.</li></ol><p>We are continuously optimizing the user experience of Ming-flash-omni-Preview. We welcome you to provide feedback via community discussion or issues. The official version will be released soon.</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
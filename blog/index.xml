<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Blog on INCLUSION AI</title>
    <link>https://inclusionai.github.io/blog/</link>
    <description>Recent content in Blog on INCLUSION AI</description>
    <image>
      <url>https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 28 Oct 2025 00:00:03 +0800</lastBuildDate><atom:link href="https://inclusionai.github.io/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Segmentation-as-Editing for Unified Multimodal AI</title>
      <link>https://inclusionai.github.io/blog/ming-flash-omni-preview/</link>
      <pubDate>Tue, 28 Oct 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-flash-omni-preview/</guid>
      <description>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope</description>
    </item>
    
    <item>
      <title>Ming-UniAudio: Speech LLM for Joint Understanding, Generation and Editing with Unified Representation</title>
      <link>https://inclusionai.github.io/blog/ming-uniaudio/</link>
      <pubDate>Wed, 01 Oct 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-uniaudio/</guid>
      <description>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
The Introduction Video of Ming-UniAudio Audio Edit Demo Editing Tasks Video demos ðŸš€ Technical Highlights First unified continuous speech tokenizer for both understanding and generation tasks: MingTok-Audio is a unified continuous speech tokenizer MingTok-Audio based on a VAE framework with a causal Transformer architecture, the first continuous speech tokenizer to effectively integrate semantic and acoustic features, and enables a closed-loop system with LLMs through hierarchical feature representations, makes it suitable for both understanding and generation tasks.</description>
    </item>
    
    <item>
      <title>Ming-UniVision: Joint Image Understanding and Generation via a Unified Continuous Tokenizer</title>
      <link>https://inclusionai.github.io/blog/mingtok/</link>
      <pubDate>Wed, 01 Oct 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/mingtok/</guid>
      <description>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
ðŸš€ Technical Highlights First Continuous Unified Tokenizer for Vision: MingTok seamlessly supports both image understanding and generation within a single continuous latent spaceâ€”eliminating quantization and bridging modalities. First NTP-style Autoregressive MLLM with Unified Continuous Visual Tokens: By building on MingTok, Ming-UniVision unifies vision and language under a shared next-token prediction framework, enabling end-to-end autoregressive modeling of diverse vision tasks. Reduced Representational Competition â†’ 3.5Ã— Faster Convergence: The unified continuous representation aligns semantic understanding and generative dynamics, significantly accelerating joint training without performance trade-offs.</description>
    </item>
    
    <item>
      <title>Segmentation-as-Editing for Unified Multimodal AI</title>
      <link>https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/</link>
      <pubDate>Sat, 13 Sep 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/</guid>
      <description>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Ming-lite-omni 1.5: Segmentation-as-Editing for Unified Multimodal AI The Hype and the Hidden Question The multimodal AI world has been thriving.
From the debut of Qwen-Image to the interactive editing hype sparked by Nano Banana, image editing has rapidly become the next battlefield for generative AI.
Editing fundamentally requires two distinct skill sets:
Know where, what, and how to change (understanding the image) Produce the change with high visual quality (generating the image) Its rich gameplay and strong interactivity have pulled in users, developers, and creators alike.</description>
    </item>
    
    <item>
      <title>Introducing Ring-lite-2507</title>
      <link>https://inclusionai.github.io/blog/ring-lite-2507/</link>
      <pubDate>Tue, 05 Aug 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ring-lite-2507/</guid>
      <description>ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Overview We present Ring-lite-2507, an upgraded version of our previously released lightweight reasoning model, Ring-lite (2506). Built upon a 16.8B Mixture-of-Experts (MoE) large language model with 2.75B activated parameters, Ring-lite-2507 further advances its reasoning capabilities while demonstrating superior performance across a comprehensive range of LLM benchmarks, including general text understanding, alignment, coding, logical, and agentic tasks. Thanks to our innovative and robust reinforcement learning training pipeline, Ring-lite-2507 distinguishes itself from the latest public dense models under 10B parameters by offering competitive performance across various tasks, despite activating only 1/3 of their parameter size.</description>
    </item>
    
    <item>
      <title>Introducing Ming-Lite-Omni V1.5</title>
      <link>https://inclusionai.github.io/blog/ming-lite-omni-1_5/</link>
      <pubDate>Mon, 21 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-lite-omni-1_5/</guid>
      <description>GITHUB ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Overview Ming-lite-omni v1.5 is a comprehensive upgrade to the full-modal capabilities of Ming-lite-omni(Github). It significantly improves performance across tasks including image-text understanding, document understanding, video understanding, speech understanding and synthesis, and image generation and editing. Built upon Ling-lite-1.5, Ming-lite-omni v1.5 has a total of 20.3 billion parameters, with 3 billion active parameters in its MoE (Mixture-of-Experts) section. It demonstrates highly competitive results in various modal benchmarks compared to industry-leading models.</description>
    </item>
    
    <item>
      <title>M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning</title>
      <link>https://inclusionai.github.io/blog/m2-reasoning/</link>
      <pubDate>Fri, 11 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/m2-reasoning/</guid>
      <description>ðŸ“– Technical Report | ðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Introduction We introduce M2-Reasoning-7B, a model designed to excel in both general and spatial reasoning. Our approach integrates two key innovations: (1) a novel data pipeline that generates 294.2K high-quality data samples (168K for cold-start fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning trajectories and have undergone comprehensive assessment; and (2) a dynamic multi-task training strategy with step-wise optimization to mitigate conflicts between data, and task-specific rewards for delivering tailored incentive signals.</description>
    </item>
    
    <item>
      <title>ABench: An Evolving Open-Source Benchmark</title>
      <link>https://inclusionai.github.io/blog/abench/</link>
      <pubDate>Tue, 08 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/abench/</guid>
      <description>GITHUB ðŸŒŸ Overview ABench is an evolving open-source benchmark suite designed to rigorously evaluate and enhance Large Language Models (LLMs) on complex cross-domain tasks. By targeting current model weaknesses, ABench provides systematic challenges in high-difficulty specialized domains, including physics, actuarial science, logical reasoning, law, and psychology.
ðŸŽ¯ Core Objectives Address Evaluation Gaps: Design high-differentiation assessment tasks targeting underperforming question types Establish Unified Standards: Create reliable, comparable benchmarks for multi-domain LLM evaluation Expand Capability Boundaries: Drive continuous optimization of knowledge systems and reasoning mechanisms through challenging innovative problems ðŸ“Š Dataset Release Status Domain Description Status Physics 500 university/competition-level physics problems (400 static + 100 dynamic parametric variants) covering 10+ fields from classical mechanics to modern physics âœ… Released Actuary Curated actuarial exam problems covering core topics: probability statistics, financial mathematics, life/non-life insurance, actuarial models, and risk management âœ… Released Logic High-differentiation logical reasoning problems from authoritative tests (LSAT/GMAT/GRE/SBI/Chinese Civil Service Exam) ðŸ”„ In Preparation Psychology Psychological case studies and research questions (objective/subjective) evaluating understanding of human behavior and theories ðŸ”„ In Preparation Law Authoritative judicial exam materials covering core legal domains: criminal/civil/administrative/procedural/international law ðŸ”„ In Preparation </description>
    </item>
    
    <item>
      <title>AWorld: The Agent Runtime for Self-Improvement</title>
      <link>https://inclusionai.github.io/blog/aworld/</link>
      <pubDate>Mon, 07 Jul 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/aworld/</guid>
      <description>&amp;ldquo;Self-awareness: the hardest problem isn&amp;rsquo;t solving within limits, it&amp;rsquo;s discovering the own limitations&amp;rdquo; Table of Contents News â€” Latest updates and announcements. Introduction â€” Overview and purpose of the project. Installation â€” Step-by-step setup instructions. Quick Start â€” Get started with usage examples. Architecture â€” Explore the multi-agent system design. Demo â€” See the project in action with demonstrations. Contributing â€” How to get involved and contribute. License â€” Project licensing details.</description>
    </item>
    
    <item>
      <title>Ming-Omni: A Unified Multimodal Model for Perception and Generation</title>
      <link>https://inclusionai.github.io/blog/ming-omni/</link>
      <pubDate>Wed, 11 Jun 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-omni/</guid>
      <description>GITHUB ðŸ“‘ Technical Reportï½œðŸ“–Project Page ï½œðŸ¤— Hugging Faceï½œ ðŸ¤– ModelScope
Introduction Ming-lite-omni, a light version of Ming-omni, which is derived from Ling-lite and features 2.8 billion activated parameter. Ming-lite-omni is a unified multimodal model capable of processing images, text, audio, and video, while demonstrating strong proficiency in both speech and image generation. Ming-lite-omni employs dedicated encoders to extract tokens from different modalities, which are then processed by Ling, an MoE architecture equipped with newly proposed modality-specific routers.</description>
    </item>
    
    <item>
      <title>Ling: A MoE LLM Provided and Open-sourced by InclusionAI</title>
      <link>https://inclusionai.github.io/blog/ling/</link>
      <pubDate>Thu, 08 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ling/</guid>
      <description>ðŸ¤— Hugging Face&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspðŸ¤– ModelScope
Introduction Ling is a MoE LLM provided and open-sourced by InclusionAI. We introduce two different sizes, which are Ling-lite and Ling-plus. Ling-lite has 16.8 billion parameters with 2.75 billion activated parameters, while Ling-plus has 290 billion parameters with 28.8 billion activated parameters. Both models demonstrate impressive performance compared to existing models in the industry.
Their structure makes it easy to scale up and down and adapt to different tasks, so users can use these models for a wide range of tasks, from processing natural language to solving complex problems.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</title>
      <link>https://inclusionai.github.io/blog/ming-lite-uni/</link>
      <pubDate>Wed, 07 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-lite-uni/</guid>
      <description>GITHUB ðŸ“‘ Paperï½œðŸ¤— Hugging Faceï½œðŸ¤– ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension.</description>
    </item>
    
    <item>
      <title>Ming-Lite-Omni-Preview: A MoE Model Designed to Perceive a Wide Range of Modalities</title>
      <link>https://inclusionai.github.io/blog/ming-lite-omni-preview/</link>
      <pubDate>Mon, 05 May 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ming-lite-omni-preview/</guid>
      <description>GITHUB ðŸ¤— Hugging Face | ðŸ¤– ModelScope
Introduction Ming-Lite-Omni-Preview is built upon Ling-Lite, which is a MoE model designed to perceive a wide range of modalities, including text, images, audio, and video, while generating text and natural speech in a streaming manner. To naturely handle the diverse modalities, we have enhanced Ling-Lite by incorporating modality-specific routers for each modality. As a result, Ming-Omni excels at handling information from diverse modalities and is highly scalable.</description>
    </item>
    
    <item>
      <title>Agentic Learning</title>
      <link>https://inclusionai.github.io/blog/agenticlearning/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/agenticlearning/</guid>
      <description>Introduction Agent exhibits powerful capabilities by interacting with the external environment and making decisions based on the feedback it receives from the environment. For complex problems, it is often necessary for an agent to have multi-turn interactions with the environment to reach a solution. The complexity and dynamism of environments, coupled with the necessity for multi-turn interactions, pose numerous challenges in training agents.
We introduce AgenticLearning, an open-source agent training paradigm designed to empower researchers to train and evaluate autonomous agents effectively.</description>
    </item>
    
    <item>
      <title>AReaL: Ant Reasoning Reinforcement Learning for LLMs</title>
      <link>https://inclusionai.github.io/blog/areal/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/areal/</guid>
      <description>| Paper | Documentation | Ask DeepWiki | ðŸ¤— Models &amp; Data | WeChat Group | AReaL (Ant Reasoning RL) is an open-source fully asynchronous reinforcement learning training system for large reasoning models developed at the RL Lab, Ant Research. Built upon the open-source project RealHF, we are fully committed to open-source by providing training details, data, and infrastructure required to reproduce results along with the model itself. AReaL aims to help everyone build their own AI agents easily and affordably.</description>
    </item>
    
    <item>
      <title>PromptCoT &amp; PromptCoT-Mamba: Advancing the Frontiers of Reasoning</title>
      <link>https://inclusionai.github.io/blog/promptcot/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/promptcot/</guid>
      <description>News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba.</description>
    </item>
    
    <item>
      <title>Ring: A Reasoning MoE LLM Provided and Open-sourced by InclusionAI</title>
      <link>https://inclusionai.github.io/blog/ring/</link>
      <pubDate>Tue, 01 Apr 2025 00:00:03 +0800</pubDate>
      
      <guid>https://inclusionai.github.io/blog/ring/</guid>
      <description>ðŸ¤— Hugging Face&amp;nbsp&amp;nbsp | &amp;nbsp&amp;nbspðŸ¤– ModelScope News [2025-06]:ðŸŽ‰ Add Ring-lite Model [2025-04]:ðŸŽ‰ Add Ring-lite-linear-preview Model Introduction Ring is a reasoning MoE LLM provided and open-sourced by InclusionAI, derived from Ling. We introduce Ring-lite-distill-preview, which has 16.8 billion parameters with 2.75 billion activated parameters. This model demonstrates impressive reasoning performance compared to existing models in the industry.
Model Downloads You can download the following table to see the various parameters for your use case.</description>
    </item>
    
  </channel>
</rss>

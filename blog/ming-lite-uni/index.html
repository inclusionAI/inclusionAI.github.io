<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction | INCLUSION AI</title><meta name=keywords content><meta name=description content="

        GITHUB üìë PaperÔΩúü§ó Hugging FaceÔΩúü§ñ ModelScope

Introduction
Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/ming-lite-uni/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-uni/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-uni/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction"><meta property="og:description" content="

        GITHUB üìë PaperÔΩúü§ó Hugging FaceÔΩúü§ñ ModelScope

Introduction
Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/ming-lite-uni/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-07T00:00:03+08:00"><meta property="article:modified_time" content="2025-05-07T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction"><meta name=twitter:description content="

        GITHUB üìë PaperÔΩúü§ó Hugging FaceÔΩúü§ñ ModelScope

Introduction
Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.
This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction","item":"https://inclusionai.github.io/blog/ming-lite-uni/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction","name":"Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction","description":" GITHUB üìë PaperÔΩúü§ó Hugging FaceÔΩúü§ñ ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.\nThis project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement.\n","keywords":[],"articleBody":" GITHUB üìë PaperÔΩúü§ó Hugging FaceÔΩúü§ñ ModelScope Introduction Ming-Lite-Uni is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.\nThis project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative multi-scale learnable tokens and multi-scale representation alignment strategy. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement.\nWe appreciate everyone‚Äôs ongoing support and attention! We sincerely value your patience as we progressively enhance our solutions and model efficacy. We are now achieving significant progress and observing favorable outcomes, with forthcoming updates anticipated‚Äîremain attentive!\nüìå Updates [2025.05.03] üî• Our Technical Report is in public on arxiv. [2025.05.03] üî• We release the fisrt version of Ming-Lite-Uni. Why It Matters Ming-Lite-Uni‚Äôs unified architecture overcomes fundamental limitations of conventional approaches:\nConventional Methods Ming-Lite-Uni‚Äôs Advantages Modular Pipelines\n(CLIP/SigLIP + Diffusion Models) End-to-End Unified Model\nSeamless understanding-generation integration Discrete Token AR\n(Limited visual grounding) Continuous Token Space\nNative support for fine-grained visual concepts Fixed-Resolution Processing\n(Artifacts in upscaling) Multi-Scale Adaptation\nConsistent quality across resolutions Separate Editing Workflows\n(Manual alignment required) Dialog-Driven Control\nNatural language guided pixel-level editing Understanding Bottlenecks\n(Visual-semantic mismatch) Joint Representation Learning\nMutually enhanced comprehension and generation Key Enhancements Unified Visual Understanding \u0026 Generation Architecture. Ming-Lite-Uni achieves an average understanding score of 69.7 on the OpenCompass leaderboard, surpassing DeepSeek-VL2 (66.4). At the same time, it achieves an image generation score of 0.62 on the GenEval benchmark, outperforming SDXL (0.55). Multi-Scale Learnable Tokens. We employ a novel mechanism to establish feature correlations across resolutions of 4√ó/8√ó/16√ó. By introducing hierarchical tokens, the model captures global layout (low-res), object structures (mid-res), and fine textures (high-res), improving GenEval by 3.5%. Multi-Scale Representation Alignment. We introduce a novel scale wised consistency loss to enforce alignment between hierarchical representations and final outputs through native-resolution optimization. This strategy directly enhances the high-res reconstruction quality (\u003e2dB PSNR) and boosts GenEval by 1.5%. AGI-Capable System. Our model supports complex chained operations, such as ‚Äúgenerate castle ‚Üí add sunset ‚Üí adjust perspective‚Äù, with a swift response time of under 1 second (benchmarked with RTX 4090). The system is designed to handle instruction-driven generation-editing and is synchronized with ChatGPT-4o(aligned with the industry milestone of March 2025). Empowering Multimodal Interaction with Ming-Lite-Uni Ming-Lite-Uni acts as a unified model for multimodal understanding, extending beyond traditional NLP tasks and multimodal comprehension to enable interactive multimodal generation. This includes capabilities such as image generation, image editing, and style transfer.\nModel Structure Ming-Lite-Uni is a unified multimodal model designed for both image understanding and high-fidelity image generation. It achieves this by compressing image representations into continuous visual tokens, which are processed alongside discrete text tokens using a scaled auto-regressive Transformer. The generation capability is powered by an externally trained diffusion model (SANA), conditioned on tokens produced by the Transformer.\nBenchmark Evaluations We conduct separate quantitative evaluations of Ming-Lite-Uni on multimodal understanding and text-to-image generation using public benchmarks. For multimodal understanding, we compare against traditional models that take images and text as input and output text, as well as against recent models with visual generative capabilities. For multimodal generation, we evaluate text-to-image performance on GenEval. Please refer to our TechReport for details.\nMultimodal Understanding\nType Model Avg. MMB MMS MMMU MathV Hall AI2D MM-Vet Und. Only LLaVA-72B 68.0 84.5 65.8 56.6 68.4 47.9 86.2 60.6 Qwen2.5-VL-7B 76.2 87.8 71.1 67.9 70.8 58.8 88.2 76.7 Emu3-Chat - 58.5 - 31.6 - - - 37.2 InternVL2.5-78B 75.2 87.5 69.5 70 71.4 57.4 89.1 71.8 DeepSeek-VL2 66.4 81.2 61.0 50.7 59.4 51.5 84.5 60.0 GPT-4o-20241120 (closed) 72.0 84.3 65.1 70.7 59.9 56.2 84.9 74.5 Step-1o (closed) 77.7 87.3 69.3 69.9 74.7 55.8 89.1 82.8 Und. and Gen. TokenFlow-XL - 68.9 - 38.7 - - - 40.7 Janus-Pro-7B - 79.2 - 41.0 - - - 50.0 Ours (Ming-Lite-Uni) 69.7 80.7 60.5 51.2 68.3 51.8 84.5 72.3 Image Generation\nType Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Gen. Only LlamaGen 0.71 0.34 0.21 0.58 0.07 0.04 0.32 SDv2.1 0.98 0.51 0.44 0.85 0.07 0.17 0.50 Emu3-Gen 0.98 0.71 0.34 0.81 0.17 0.21 0.54 SDXL 0.98 0.74 0.39 0.85 0.15 0.23 0.55 DALL-E 3 0.96 0.87 0.47 0.83 0.43 0.45 0.67 SD3-Medium 0.99 0.94 0.72 0.89 0.33 0.60 0.74 Und. and Gen. Show-o 0.95 0.52 0.49 0.82 0.11 0.28 0.53 TokenFlow-XL 0.95 0.60 0.41 0.81 0.16 0.24 0.55 Janus-Pro-1B 0.98 0.82 0.51 0.89 0.65 0.56 0.73 Ours (Ming-Lite-Uni) 0.99 0.76 0.53 0.87 0.26 0.30 0.62 Example Usage System Requirements Python: \u003e= 3.8 PyTorch: \u003e= 2.4.1+cu12.2 (CUDA 12.2 compatible) flash-attn: \u003e= 2.6.3 Installation We recommend installing the following versions to set up your environment using pip:\npip install -r requirements.txt Usage Guided Below is an example of how to load and use the model:\nimport torch import os from Ming_Uni.MingUniInference import Ming_Uni_Inference from Ming_Uni.process import MyProcessor device = torch.cuda.current_device() device = torch.device(device) model_path='../Ming-Lite-Uni/' model = Ming_Uni_Inference(model_path) model.to(torch.bfloat16) model.to(device) model.eval() llm_model=os.path.join(model_path, 'qwen2_5_llm') my_proc=MyProcessor(llm_model) image_file = \"tests/cake.jpg\" prompt = \"add a candle on top of the cake\" inputs = my_proc.process(image_file=image_file, prompt=prompt, device=device) result = model.image_gen_generate(inputs, steps=30, seed=42, cfg=5.0, height=512, width=512)[1] result.save(\"result.png\") For more advanced usage, such as fine-tuning or generating images, refer to the documentation.\nAcknowledgments The project is currently in its early stages. While some preliminary results have been promising, substantial progress is needed to achieve seamless integration of understanding and generation. Both the code and models require further refinement and optimization, which is why we have chosen to open-source the project. We invite contributions from the community to help enhance and develop it collaboratively. If you have any suggestions or identify issues within the code, please contribute via Pull Requests. Thank you for your support and interest!\nOpen Collaboration We‚Äôre open-sourcing Ming-Lite-Uni to accelerate progress toward AGI, featuring:\nüìÇ Full model weights \u0026 test code üß© Modular architecture for easy extension üìä Comprehensive benchmarks (vs GPT-4V, SDXL, etc.) ‚ÄúThe simultaneous release of ChatGPT-4‚Äôs image generation in March 2025 confirms our vision of unified multimodal AI as the next paradigm.‚Äù\nContact Information If you require assistance or encounter troubles while utilizing our project, please open a GitHub issue.\nLicense and Legal Disclaimer Ming is licensed under the MIT License, and the Legal Disclaimer is located in the LEGAL.md file under the project‚Äôs root directory.\nCitation If you find our work helpful, feel free to give us a cite.\n@article{Mingunify2025, title = {Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction}, author = {Inclusion AI, Ant Group}, journal = {arXiv preprint}, year = {2025} } ","wordCount":"1133","inLanguage":"en","datePublished":"2025-05-07T00:00:03+08:00","dateModified":"2025-05-07T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/ming-lite-uni/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction</h1><div class=post-meta>&lt;span title='2025-05-07 00:00:03 +0800 +0800'>May 7, 2025&lt;/span>&amp;nbsp;¬∑&amp;nbsp;6 min&amp;nbsp;¬∑&amp;nbsp;1133 words&amp;nbsp;¬∑&amp;nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;Translations:<ul class=i18n_list><li><a href=https://inclusionai.github.io/zh/blog/ming-lite-uni/>ÁÆÄ‰Ωì‰∏≠Êñá</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p align=left><a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify class="btn external" target=_blank>GITHUB</a> üìë <a href=https://arxiv.org/pdf/2505.02471>Paper</a>ÔΩúü§ó <a href=https://huggingface.co/inclusionAI/Ming-Lite-Uni>Hugging Face</a>ÔΩúü§ñ <a href=https://modelscope.cn/models/inclusionAI/Ming-Lite-Uni>ModelScope</a></p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p><code>Ming-Lite-Uni</code> is an open-source multimodal framework that includes a newly developed unified visual generator, and a native multimodal autoregressive model meant to integrate vision and language.</p><p>This project offers an open-source implementation of the integrated MetaQueries and M2-omni framework, while offering the innovative <strong>multi-scale learnable tokens</strong> and <strong>multi-scale representation alignment strategy</strong>. Ming-Lite-Uni utilizes a fixed MLLM and a learnable diffusion model, allowing native multimodal AR models to execute text-to-image production and instruction-based image editing tasks, hence enhancing their functionalities beyond mere visual comprehension. Our experimental findings demonstrate the robust efficacy of Ming-Lite-Uni and highlight the remarkable fluidity of its interactive process. Ming-Lite-Uni is now in the alpha phase and will soon undergo additional refinement.</p><p>We appreciate everyone&rsquo;s ongoing support and attention! We sincerely value your patience as we progressively enhance our solutions and model efficacy. We are now achieving significant progress and observing favorable outcomes, with forthcoming updates anticipated‚Äîremain attentive!</p><h2 id=x1f4cc-updates>üìå Updates<a hidden class=anchor aria-hidden=true href=#x1f4cc-updates>#</a></h2><ul><li>[2025.05.03] üî• Our <a href=https://arxiv.org/pdf/2505.02471>Technical Report</a> is in public on arxiv.</li><li>[2025.05.03] üî• We release the fisrt version of <a href=https://github.com/inclusionAI/Ming>Ming-Lite-Uni</a>.</li></ul><h2 id=why-it-matters>Why It Matters<a hidden class=anchor aria-hidden=true href=#why-it-matters>#</a></h2><p>Ming-Lite-Uni&rsquo;s unified architecture overcomes fundamental limitations of conventional approaches:</p><table><thead><tr><th>Conventional Methods</th><th>Ming-Lite-Uni&rsquo;s Advantages</th></tr></thead><tbody><tr><td><strong>Modular Pipelines</strong><br>(CLIP/SigLIP + Diffusion Models)</td><td><strong>End-to-End Unified Model</strong><br>Seamless understanding-generation integration</td></tr><tr><td><strong>Discrete Token AR</strong><br>(Limited visual grounding)</td><td><strong>Continuous Token Space</strong><br>Native support for fine-grained visual concepts</td></tr><tr><td><strong>Fixed-Resolution Processing</strong><br>(Artifacts in upscaling)</td><td><strong>Multi-Scale Adaptation</strong><br>Consistent quality across resolutions</td></tr><tr><td><strong>Separate Editing Workflows</strong><br>(Manual alignment required)</td><td><strong>Dialog-Driven Control</strong><br>Natural language guided pixel-level editing</td></tr><tr><td><strong>Understanding Bottlenecks</strong><br>(Visual-semantic mismatch)</td><td><strong>Joint Representation Learning</strong><br>Mutually enhanced comprehension and generation</td></tr></tbody></table><h2 id=key-enhancements>Key Enhancements<a hidden class=anchor aria-hidden=true href=#key-enhancements>#</a></h2><ul><li><strong>Unified Visual Understanding & Generation Architecture.</strong> Ming-Lite-Uni achieves an average understanding score of 69.7 on the OpenCompass leaderboard, surpassing DeepSeek-VL2 (66.4). At the same time, it achieves an image generation score of 0.62 on the GenEval benchmark, outperforming SDXL (0.55).</li><li><strong>Multi-Scale Learnable Tokens.</strong> We employ a novel mechanism to establish feature correlations across resolutions of 4√ó/8√ó/16√ó. By introducing <strong>hierarchical tokens</strong>, the model captures global layout (low-res), object structures (mid-res), and fine textures (high-res), improving GenEval by 3.5%.</li><li><strong>Multi-Scale Representation Alignment.</strong> We introduce a novel <strong>scale wised consistency loss</strong> to enforce alignment between hierarchical representations and final outputs through native-resolution optimization. This strategy directly enhances the high-res reconstruction quality (>2dB PSNR) and boosts GenEval by 1.5%.</li><li><strong>AGI-Capable System.</strong> Our model supports complex chained operations, such as &ldquo;generate castle ‚Üí add sunset ‚Üí adjust perspective&rdquo;, with a swift response time of under 1 second (benchmarked with RTX 4090). The system is designed to handle instruction-driven generation-editing and is synchronized with ChatGPT-4o(aligned with the industry milestone of March 2025).</li></ul><h2 id=empowering-multimodal-interaction-with-ming-lite-uni>Empowering Multimodal Interaction with Ming-Lite-Uni<a hidden class=anchor aria-hidden=true href=#empowering-multimodal-interaction-with-ming-lite-uni>#</a></h2><p><strong>Ming-Lite-Uni</strong> acts as a unified model for multimodal understanding, extending beyond traditional NLP tasks and multimodal comprehension to enable interactive multimodal generation. This includes capabilities such as image generation, image editing, and style transfer.</p><h2 id=model-structure>Model Structure<a hidden class=anchor aria-hidden=true href=#model-structure>#</a></h2><p><strong>Ming-Lite-Uni</strong> is a unified multimodal model designed for both image understanding and high-fidelity image generation. It achieves this by compressing image representations into continuous visual tokens, which are processed alongside discrete text tokens using a scaled auto-regressive Transformer. The generation capability is powered by an externally trained diffusion model (SANA), conditioned on tokens produced by the Transformer.</p><img width=1034 alt=B106FE9E-5839-48c3-A175-AE8A4D2D8BB8 src=https://github.com/user-attachments/assets/927e090e-7cda-4f32-81de-774466973077><h2 id=benchmark-evaluations>Benchmark Evaluations<a hidden class=anchor aria-hidden=true href=#benchmark-evaluations>#</a></h2><p>We conduct separate quantitative evaluations of Ming-Lite-Uni on multimodal understanding and text-to-image generation using public benchmarks. For multimodal understanding, we compare against traditional models that take images and text as input and output text, as well as against recent models with visual generative capabilities. For multimodal generation, we evaluate text-to-image performance on GenEval. Please refer to our TechReport for details.</p><p><strong>Multimodal Understanding</strong></p><table><thead><tr><th>Type</th><th>Model</th><th>Avg.</th><th>MMB</th><th>MMS</th><th>MMMU</th><th>MathV</th><th>Hall</th><th>AI2D</th><th>MM-Vet</th></tr></thead><tbody><tr><td><strong>Und. Only</strong></td><td>LLaVA-72B</td><td>68.0</td><td>84.5</td><td>65.8</td><td>56.6</td><td>68.4</td><td>47.9</td><td>86.2</td><td>60.6</td></tr><tr><td></td><td>Qwen2.5-VL-7B</td><td>76.2</td><td>87.8</td><td>71.1</td><td>67.9</td><td>70.8</td><td>58.8</td><td>88.2</td><td>76.7</td></tr><tr><td></td><td>Emu3-Chat</td><td>-</td><td>58.5</td><td>-</td><td>31.6</td><td>-</td><td>-</td><td>-</td><td>37.2</td></tr><tr><td></td><td>InternVL2.5-78B</td><td>75.2</td><td>87.5</td><td>69.5</td><td>70</td><td>71.4</td><td>57.4</td><td>89.1</td><td>71.8</td></tr><tr><td></td><td>DeepSeek-VL2</td><td>66.4</td><td>81.2</td><td>61.0</td><td>50.7</td><td>59.4</td><td>51.5</td><td>84.5</td><td>60.0</td></tr><tr><td></td><td>GPT-4o-20241120 (closed)</td><td>72.0</td><td>84.3</td><td>65.1</td><td>70.7</td><td>59.9</td><td>56.2</td><td>84.9</td><td>74.5</td></tr><tr><td></td><td>Step-1o (closed)</td><td>77.7</td><td>87.3</td><td>69.3</td><td>69.9</td><td>74.7</td><td>55.8</td><td>89.1</td><td>82.8</td></tr><tr><td><strong>Und. and Gen.</strong></td><td>TokenFlow-XL</td><td>-</td><td>68.9</td><td>-</td><td>38.7</td><td>-</td><td>-</td><td>-</td><td>40.7</td></tr><tr><td></td><td>Janus-Pro-7B</td><td>-</td><td>79.2</td><td>-</td><td>41.0</td><td>-</td><td>-</td><td>-</td><td>50.0</td></tr><tr><td></td><td><strong>Ours (Ming-Lite-Uni)</strong></td><td>69.7</td><td>80.7</td><td>60.5</td><td>51.2</td><td>68.3</td><td>51.8</td><td>84.5</td><td>72.3</td></tr></tbody></table><p><strong>Image Generation</strong></p><table><thead><tr><th>Type</th><th>Method</th><th>Single Obj.</th><th>Two Obj.</th><th>Counting</th><th>Colors</th><th>Position</th><th>Color Attri.</th><th>Overall</th></tr></thead><tbody><tr><td><strong>Gen. Only</strong></td><td>LlamaGen</td><td>0.71</td><td>0.34</td><td>0.21</td><td>0.58</td><td>0.07</td><td>0.04</td><td>0.32</td></tr><tr><td></td><td>SDv2.1</td><td>0.98</td><td>0.51</td><td>0.44</td><td>0.85</td><td>0.07</td><td>0.17</td><td>0.50</td></tr><tr><td></td><td>Emu3-Gen</td><td>0.98</td><td>0.71</td><td>0.34</td><td>0.81</td><td>0.17</td><td>0.21</td><td>0.54</td></tr><tr><td></td><td>SDXL</td><td>0.98</td><td>0.74</td><td>0.39</td><td>0.85</td><td>0.15</td><td>0.23</td><td>0.55</td></tr><tr><td></td><td>DALL-E 3</td><td>0.96</td><td>0.87</td><td>0.47</td><td>0.83</td><td>0.43</td><td>0.45</td><td>0.67</td></tr><tr><td></td><td>SD3-Medium</td><td>0.99</td><td>0.94</td><td>0.72</td><td>0.89</td><td>0.33</td><td>0.60</td><td>0.74</td></tr><tr><td><strong>Und. and Gen.</strong></td><td>Show-o</td><td>0.95</td><td>0.52</td><td>0.49</td><td>0.82</td><td>0.11</td><td>0.28</td><td>0.53</td></tr><tr><td></td><td>TokenFlow-XL</td><td>0.95</td><td>0.60</td><td>0.41</td><td>0.81</td><td>0.16</td><td>0.24</td><td>0.55</td></tr><tr><td></td><td>Janus-Pro-1B</td><td>0.98</td><td>0.82</td><td>0.51</td><td>0.89</td><td>0.65</td><td>0.56</td><td>0.73</td></tr><tr><td></td><td><strong>Ours (Ming-Lite-Uni)</strong></td><td>0.99</td><td>0.76</td><td>0.53</td><td>0.87</td><td>0.26</td><td>0.30</td><td>0.62</td></tr></tbody></table><h2 id=example-usage>Example Usage<a hidden class=anchor aria-hidden=true href=#example-usage>#</a></h2><h4 id=system-requirements>System Requirements<a hidden class=anchor aria-hidden=true href=#system-requirements>#</a></h4><ul><li><strong>Python:</strong> >= 3.8</li><li><strong>PyTorch:</strong> >= 2.4.1+cu12.2 (CUDA 12.2 compatible)</li><li><strong>flash-attn:</strong> >= 2.6.3</li></ul><h4 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h4><p>We recommend installing the following versions to set up your environment using pip:</p><pre tabindex=0><code>pip install -r requirements.txt
</code></pre><ul><li><h3 id=usage-guided>Usage Guided<a hidden class=anchor aria-hidden=true href=#usage-guided>#</a></h3></li></ul><p>Below is an example of how to load and use the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>Ming_Uni.MingUniInference</span> <span class=kn>import</span> <span class=n>Ming_Uni_Inference</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>Ming_Uni.process</span> <span class=kn>import</span> <span class=n>MyProcessor</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_device</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span><span class=o>=</span><span class=s1>&#39;../Ming-Lite-Uni/&#39;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Ming_Uni_Inference</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm_model</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>model_path</span><span class=p>,</span> <span class=s1>&#39;qwen2_5_llm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>my_proc</span><span class=o>=</span><span class=n>MyProcessor</span><span class=p>(</span><span class=n>llm_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image_file</span> <span class=o>=</span> <span class=s2>&#34;tests/cake.jpg&#34;</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;add a candle on top of the cake&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>my_proc</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>image_file</span><span class=o>=</span><span class=n>image_file</span><span class=p>,</span> <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>image_gen_generate</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>cfg</span><span class=o>=</span><span class=mf>5.0</span><span class=p>,</span> <span class=n>height</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mi>512</span><span class=p>)[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>result</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;result.png&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>For more advanced usage, such as fine-tuning or generating images, refer to the documentation.</p><h2 id=acknowledgments>Acknowledgments<a hidden class=anchor aria-hidden=true href=#acknowledgments>#</a></h2><p>The project is currently in its early stages. While some preliminary results have been promising, substantial progress is needed to achieve seamless integration of understanding and generation. Both the code and models require further refinement and optimization, which is why we have chosen to open-source the project. We invite contributions from the community to help enhance and develop it collaboratively. If you have any suggestions or identify issues within the code, please contribute via Pull Requests. Thank you for your support and interest!</p><h2 id=open-collaboration>Open Collaboration<a hidden class=anchor aria-hidden=true href=#open-collaboration>#</a></h2><p>We&rsquo;re open-sourcing Ming-Lite-Uni to accelerate progress toward AGI, featuring:</p><ul><li>üìÇ Full model weights & test code</li><li>üß© Modular architecture for easy extension</li><li>üìä Comprehensive benchmarks (vs GPT-4V, SDXL, etc.)</li></ul><p><em>&ldquo;The simultaneous release of ChatGPT-4&rsquo;s image generation in March 2025 confirms our vision of unified multimodal AI as the next paradigm.&rdquo;</em></p><h2 id=contact-information>Contact Information<a hidden class=anchor aria-hidden=true href=#contact-information>#</a></h2><p>If you require assistance or encounter troubles while utilizing our project, please open a GitHub issue.</p><h2 id=license-and-legal-disclaimer>License and Legal Disclaimer<a hidden class=anchor aria-hidden=true href=#license-and-legal-disclaimer>#</a></h2><p>Ming is licensed under the <a href=../LICENSE>MIT License</a>, and the Legal Disclaimer is located in the <a href=../LEGAL.md>LEGAL.md file</a> under the project&rsquo;s root directory.</p><h2 id=citation>Citation<a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you find our work helpful, feel free to give us a cite.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>Mingunify2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>title</span>   <span class=p>=</span> <span class=s>{Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>author</span>  <span class=p>=</span> <span class=s>{Inclusion AI, Ant Group}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>journal</span> <span class=p>=</span> <span class=s>{arXiv preprint}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>year</span>    <span class=p>=</span> <span class=s>{2025}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
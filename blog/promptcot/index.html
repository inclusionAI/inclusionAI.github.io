<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>PromptCoT & PromptCoT-Mamba: Advancing the Frontiers of Reasoning | INCLUSION AI</title><meta name=keywords content><meta name=description content="News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/blog/promptcot/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/promptcot/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="PromptCoT & PromptCoT-Mamba: Advancing the Frontiers of Reasoning"><meta property="og:description" content="News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/blog/promptcot/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-04-01T00:00:03+08:00"><meta property="article:modified_time" content="2025-04-01T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="PromptCoT & PromptCoT-Mamba: Advancing the Frontiers of Reasoning"><meta name=twitter:description content="News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/blog/"},{"@type":"ListItem","position":2,"name":"PromptCoT \u0026 PromptCoT-Mamba: Advancing the Frontiers of Reasoning","item":"https://inclusionai.github.io/blog/promptcot/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"PromptCoT \u0026 PromptCoT-Mamba: Advancing the Frontiers of Reasoning","name":"PromptCoT \u0026 PromptCoT-Mamba: Advancing the Frontiers of Reasoning","description":"News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba.","keywords":[],"articleBody":"News May 30, 2025: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks. Apr 11, 2025: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results. Mar 7, 2025: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets. Overview This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): PromptCoT and PromptCoT-Mamba.\nPromptCoT (Synthesizing Olympiad-Level Problems for Mathematical Reasoning in Large Language Models) addresses the critical challenge of acquiring high-quality, complex problems for training advanced LLMs. It introduces a novel methodology to systematically generate Olympiad-level mathematical problems by modeling the rationale behind expert problem design. This approach not only enhances problem diversity and difficulty but also ensures logical consistency in problem construction, providing a scalable solution for creating robust training datasets.\nPromptCoT-Mamba (Scaling Reasoning without Attention) leverages the problem generation capabilities of the PromptCoT pipeline to train PromptCoT-Mamba-7B, the first attention-free foundation model based on the Mamba-2 architecture. This model demonstrates that structured training curricula can enable attention-free models to surpass strong Transformer baselines on a wide array of competition-level math and code reasoning tasks, all while maintaining constant-memory inference without KV caching.\nTogether, these projects offer a powerful suite of tools, models, and datasets for researchers and developers working on the cutting edge of AI reasoning.\nHighlights \u0026 Key Results 1. PromptCoT: Problem Generation \u0026 Distilled Models ‚ú® The Missing Piece for Test-Time Scaling: A lightweight yet powerful problem generation model enabling the construction of prompt sets at any scale with sufficient quality, perfect for SFT or RL post-training. üìñ A Fully Open Project: All models (generation, distilled LLMs) and datasets (generation inputs, SFT data) are open-sourced. üèÜ Superior Performance of Distilled Models: PromptCoT-DS-7B consistently surpasses its base model, DeepSeek-R1-Distill-Qwen-7B, with significant gains: +0.9% on MATH-500 (93.7%) +3.2% on AIME2024 (58.7%) +9.2% on AIME2025 (49.2%) PromptCoT-DS-7B (7B parameters) achieves results comparable to larger 32B models like S1-32B and LIMO-32B. PromptCoT-QwQ-32B sets a new standard, outperforming other 32B models by a significant margin: MATH-500: 96.7% ¬± 0.5% AIME2024: 83.8% ¬± 2.8% AIME2025: 75.4% ¬± 4.7% PromptCoT-DS-1.5B demonstrates competitive performance against RL-based models purely through distillation. ‚ö° Efficiency Without Compromise: PromptCoT-DS-1.5B achieves 40+% AIME scores using over 15√ó fewer A100 GPU hours compared to models like DeepScaleR-1.5B-Preview. 2. PromptCoT-Mamba: Attention-Free Reasoning üöÄ First Attention-Free SOTA: PromptCoT-Mamba-7B is the first attention-free model (Mamba-2 architecture) to outperform strong Transformer baselines in math and code reasoning. üß† Trained with PromptCoT Pipeline: Utilizes a structured, two-stage curriculum with data generated by PromptCoT. üí™ Strong General Performance: PromptCoT-Mamba-7B consistently outperforms 7B-scale Transformer and hybrid Mamba-Transformer baselines. MATH-500: 84.6% AIME 2024: 35.2% AIME 2025: 24.6% Livecodebench: 29.9% üéØ Math Specialization: The math-specialized variant, PromptCoT-Mamba-Math-7B, further boosts math performance: MATH-500: 88.0% AIME 2024: 42.9% (+7.7% over generalist) AIME 2025: 30.8% (+6.2% over generalist) ‚ö° Inference Efficiency: Offers substantial speedups (e.g., 3.66√ó faster on 24GB GPU for long sequences) and constant-memory inference, ideal for cost-sensitive or long-context workloads. Performance Details PromptCoT Series Performance Model GSM8K MATH-500 AIME2024 AIME2025 üîπ 1.5B Models DeepSeek-R1-Distill-Qwen-1.5B - 83.9% 28.9% 28.1% STILL-3-1.5B-preview - 85.5% 39.3% - DeepScaleR-1.5B-Preview - üü¢ 87.8% üü¢ 43.1% üü¢ 37.1% PromptCoT-DS-1.5B (ours) üü¢ 87.6% ¬± 0.5% 85.3% ¬± 1.1% 41.2% ¬± 6.9% 36.7% ¬± 6.2% üîπ 7B Models DeepSeek-R1-Distill-Qwen-7B - 92.8% 55.5% 40.0% Qwen2.5-7B-SimpleRL - 82.4% 26.7% - OpenThinker-7B - 89.6% 30.0% 33.3% OpenR1-Qwen-7B - 90.6% 36.7% 40.0% PromptCoT-DS-7B (ours) üî• 92.8% ¬± 0.5% üî• 93.7% ¬± 0.7% üî• 58.7% ¬± 3.1% üî• 49.2% ¬± 7.9% üîπ 32B Models DeepSeek-R1-Distill-Qwen-32B - 94.3% 72.6% - S1-32B - 93.0% 56.7% 26.6% LIMO-32B - 94.8% 57.1% 46.6% QwQ-32B - - 82.1% 70.8% PromptCoT-QwQ-32B (ours) üî•üî• 96.4% ¬± 0.2% üî•üî• 96.7% ¬± 0.5% üî•üî• 83.8% ¬± 2.8% üî•üî• 75.4% ¬± 4.7% PromptCoT-Mamba Performance General Performance:\nModel MATH-500 AIME 24 AIME 25 OlympiadBench HumanEval HumanEval+ Livecodebench PromptCoT-Mamba-7B 84.6 üî•üî•35.2 üî•üî•24.6 50.7 81.7 75.0 üî•üî•29.9 Gemma3-27B 89.0 32.6 24.0 54.2 86.0 78.0 26.9 Gemma3-12B 83.8 22.9 19.2 49.9 81.1 73.2 22.2 Sky-T1-7B 85.0 19.2 19.2 49.2 41.5 37.2 18.3 S1.1-7B 82.0 19.2 17.5 43.1 64.0 56.7 13.3 Bespoke-Stratos-7B 81.2 18.3 16.3 45.0 73.2 68.3 8.6 Nemotron-H-8B 77.6 ‚Äì ‚Äì ‚Äì 79.3 74.4 ‚Äì M1-3B 81.7 23.0 22.0 43.6 ‚Äì ‚Äì ‚Äì Math Specialization vs. Generalist:\nModel MATH-500 AIME 24 AIME 25 OlympiadBench HumanEval HumanEval+ Livecodebench PromptCoT-Mamba-Math-7B üî•üî•88.0 üî•üî•42.9 üî•üî•30.8 üî•üî•52.1 71.3 66.5 20.3 PromptCoT-Mamba-7B 84.6 35.2 24.6 50.7 81.7 75.0 29.9 Citation If you find PromptCoT or PromptCoT-Mamba useful in your research, please consider citing the respective papers:\nFor PromptCoT:\n@article{zhao2025promptcot, author = {Zhao, Xueliang and Wu, Wei and Guan, Jian and Kong, Lingpeng}, title = {PromptCoT: Synthesizing Olympiad-Level Problems for Mathematical Reasoning in Large Language Models}, year = {2025}, journal = {arXiv preprint arXiv:2503.02324}, url = {http://arxiv.org/abs/2503.02324} } For PromptCoT-Mamba:\n@article{zhao2025scaling, author = {Xueliang Zhao and Wei Wu and Lingpeng Kong}, title = {Scaling Reasoning without Attention}, journal = {arXiv preprint arXiv:2505.22425}, year = {2025}, url = {https://arxiv.org/abs/2505.22425} } ","wordCount":"823","inLanguage":"en","datePublished":"2025-04-01T00:00:03+08:00","dateModified":"2025-04-01T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/blog/promptcot/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>PromptCoT & PromptCoT-Mamba: Advancing the Frontiers of Reasoning</h1><div class=post-meta><span title='2025-04-01 00:00:03 +0800 +0800'>April 1, 2025</span>&nbsp;¬∑&nbsp;4 min&nbsp;¬∑&nbsp;823 words&nbsp;¬∑&nbsp;inclusionAI, Ant Group</div></div></div><main class=main><article class=post-single><div class=post-content><h2 id=news><strong>News</strong><a hidden class=anchor aria-hidden=true href=#news>#</a></h2><ul><li><strong>May 30, 2025</strong>: PromptCoT-Mamba released! Introducing an attention-free foundation model for reasoning tasks.</li><li><strong>Apr 11, 2025</strong>: PromptCoT-QwQ-32B model and its training data released, achieving new state-of-the-art results.</li><li><strong>Mar 7, 2025</strong>: PromptCoT project launched, including the problem generation model, distilled models (PromptCoT-DS series), and associated datasets.</li></ul><h2 id=overview><strong>Overview</strong><a hidden class=anchor aria-hidden=true href=#overview>#</a></h2><p>This repository unifies two synergistic projects aimed at advancing the frontiers of mathematical and code reasoning in Large Language Models (LLMs): <strong>PromptCoT</strong> and <strong>PromptCoT-Mamba</strong>.</p><p><strong>PromptCoT (Synthesizing Olympiad-Level Problems for Mathematical Reasoning in Large Language Models)</strong> addresses the critical challenge of acquiring high-quality, complex problems for training advanced LLMs. It introduces a novel methodology to systematically generate Olympiad-level mathematical problems by modeling the rationale behind expert problem design. This approach not only enhances problem diversity and difficulty but also ensures logical consistency in problem construction, providing a scalable solution for creating robust training datasets.</p><p><strong>PromptCoT-Mamba (Scaling Reasoning without Attention)</strong> leverages the problem generation capabilities of the PromptCoT pipeline to train <strong>PromptCoT-Mamba-7B</strong>, the first attention-free foundation model based on the Mamba-2 architecture. This model demonstrates that structured training curricula can enable attention-free models to surpass strong Transformer baselines on a wide array of competition-level math and code reasoning tasks, all while maintaining constant-memory inference without KV caching.</p><p>Together, these projects offer a powerful suite of tools, models, and datasets for researchers and developers working on the cutting edge of AI reasoning.</p><hr><h2 id=highlights--key-results><strong>Highlights & Key Results</strong><a hidden class=anchor aria-hidden=true href=#highlights--key-results>#</a></h2><h3 id=1-promptcot-problem-generation--distilled-models><strong>1. PromptCoT: Problem Generation & Distilled Models</strong><a hidden class=anchor aria-hidden=true href=#1-promptcot-problem-generation--distilled-models>#</a></h3><ul><li><strong>‚ú® The Missing Piece for Test-Time Scaling</strong>: A lightweight yet powerful problem generation model enabling the construction of prompt sets at any scale with sufficient quality, perfect for SFT or RL post-training.</li><li><strong>üìñ A Fully Open Project</strong>: All models (generation, distilled LLMs) and datasets (generation inputs, SFT data) are open-sourced.</li><li><strong>üèÜ Superior Performance of Distilled Models</strong>:<ul><li><strong>PromptCoT-DS-7B</strong> consistently surpasses its base model, DeepSeek-R1-Distill-Qwen-7B, with significant gains:<ul><li><strong>+0.9%</strong> on MATH-500 (<strong>93.7%</strong>)</li><li><strong>+3.2%</strong> on AIME2024 (<strong>58.7%</strong>)</li><li><strong>+9.2%</strong> on AIME2025 (<strong>49.2%</strong>)</li></ul></li><li><strong>PromptCoT-DS-7B</strong> (7B parameters) achieves results comparable to larger 32B models like S1-32B and LIMO-32B.</li><li><strong>PromptCoT-QwQ-32B</strong> sets a new standard, outperforming other 32B models by a significant margin:<ul><li>MATH-500: <strong>96.7% ¬± 0.5%</strong></li><li>AIME2024: <strong>83.8% ¬± 2.8%</strong></li><li>AIME2025: <strong>75.4% ¬± 4.7%</strong></li></ul></li><li><strong>PromptCoT-DS-1.5B</strong> demonstrates competitive performance against RL-based models purely through distillation.</li></ul></li><li><strong>‚ö° Efficiency Without Compromise</strong>: <strong>PromptCoT-DS-1.5B</strong> achieves <strong>40+% AIME scores</strong> using <strong>over 15√ó fewer A100 GPU hours</strong> compared to models like DeepScaleR-1.5B-Preview.</li></ul><h3 id=2-promptcot-mamba-attention-free-reasoning><strong>2. PromptCoT-Mamba: Attention-Free Reasoning</strong><a hidden class=anchor aria-hidden=true href=#2-promptcot-mamba-attention-free-reasoning>#</a></h3><ul><li>üöÄ <strong>First Attention-Free SOTA</strong>: PromptCoT-Mamba-7B is the first attention-free model (Mamba-2 architecture) to outperform strong Transformer baselines in math and code reasoning.</li><li>üß† <strong>Trained with PromptCoT Pipeline</strong>: Utilizes a structured, two-stage curriculum with data generated by PromptCoT.</li><li>üí™ <strong>Strong General Performance</strong>: PromptCoT-Mamba-7B consistently outperforms 7B-scale Transformer and hybrid Mamba-Transformer baselines.<ul><li>MATH-500: <strong>84.6%</strong></li><li>AIME 2024: <strong>35.2%</strong></li><li>AIME 2025: <strong>24.6%</strong></li><li>Livecodebench: <strong>29.9%</strong></li></ul></li><li>üéØ <strong>Math Specialization</strong>: The math-specialized variant, <strong>PromptCoT-Mamba-Math-7B</strong>, further boosts math performance:<ul><li>MATH-500: <strong>88.0%</strong></li><li>AIME 2024: <strong>42.9%</strong> (+7.7% over generalist)</li><li>AIME 2025: <strong>30.8%</strong> (+6.2% over generalist)</li></ul></li><li>‚ö° <strong>Inference Efficiency</strong>: Offers substantial speedups (e.g., <strong>3.66√ó faster</strong> on 24GB GPU for long sequences) and constant-memory inference, ideal for cost-sensitive or long-context workloads.</li></ul><hr><h2 id=performance-details><strong>Performance Details</strong><a hidden class=anchor aria-hidden=true href=#performance-details>#</a></h2><h3 id=promptcot-series-performance><strong>PromptCoT Series Performance</strong><a hidden class=anchor aria-hidden=true href=#promptcot-series-performance>#</a></h3><table><thead><tr><th><strong>Model</strong></th><th><strong>GSM8K</strong></th><th><strong>MATH-500</strong></th><th><strong>AIME2024</strong></th><th><strong>AIME2025</strong></th></tr></thead><tbody><tr><td><strong>üîπ 1.5B Models</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>DeepSeek-R1-Distill-Qwen-1.5B</td><td>-</td><td>83.9%</td><td>28.9%</td><td>28.1%</td></tr><tr><td>STILL-3-1.5B-preview</td><td>-</td><td>85.5%</td><td>39.3%</td><td>-</td></tr><tr><td>DeepScaleR-1.5B-Preview</td><td>-</td><td>üü¢ <strong>87.8%</strong></td><td>üü¢ <strong>43.1%</strong></td><td>üü¢ <strong>37.1%</strong></td></tr><tr><td><strong>PromptCoT-DS-1.5B</strong> (<strong>ours</strong>)</td><td>üü¢ <strong>87.6% ¬± 0.5%</strong></td><td><strong>85.3% ¬± 1.1%</strong></td><td><strong>41.2% ¬± 6.9%</strong></td><td><strong>36.7% ¬± 6.2%</strong></td></tr><tr><td><strong>üîπ 7B Models</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>DeepSeek-R1-Distill-Qwen-7B</td><td>-</td><td>92.8%</td><td>55.5%</td><td>40.0%</td></tr><tr><td>Qwen2.5-7B-SimpleRL</td><td>-</td><td>82.4%</td><td>26.7%</td><td>-</td></tr><tr><td>OpenThinker-7B</td><td>-</td><td>89.6%</td><td>30.0%</td><td>33.3%</td></tr><tr><td>OpenR1-Qwen-7B</td><td>-</td><td>90.6%</td><td>36.7%</td><td>40.0%</td></tr><tr><td><strong>PromptCoT-DS-7B</strong> (<strong>ours</strong>)</td><td>üî• <strong>92.8% ¬± 0.5%</strong></td><td>üî• <strong>93.7% ¬± 0.7%</strong></td><td>üî• <strong>58.7% ¬± 3.1%</strong></td><td>üî• <strong>49.2% ¬± 7.9%</strong></td></tr><tr><td><strong>üîπ 32B Models</strong></td><td></td><td></td><td></td><td></td></tr><tr><td>DeepSeek-R1-Distill-Qwen-32B</td><td>-</td><td>94.3%</td><td>72.6%</td><td>-</td></tr><tr><td>S1-32B</td><td>-</td><td>93.0%</td><td>56.7%</td><td>26.6%</td></tr><tr><td>LIMO-32B</td><td>-</td><td>94.8%</td><td>57.1%</td><td>46.6%</td></tr><tr><td>QwQ-32B</td><td>-</td><td>-</td><td>82.1%</td><td>70.8%</td></tr><tr><td><strong>PromptCoT-QwQ-32B</strong> (<strong>ours</strong>)</td><td>üî•üî• <strong>96.4% ¬± 0.2%</strong></td><td>üî•üî• <strong>96.7% ¬± 0.5%</strong></td><td>üî•üî• <strong>83.8% ¬± 2.8%</strong></td><td>üî•üî• <strong>75.4% ¬± 4.7%</strong></td></tr></tbody></table><h3 id=promptcot-mamba-performance><strong>PromptCoT-Mamba Performance</strong><a hidden class=anchor aria-hidden=true href=#promptcot-mamba-performance>#</a></h3><p><strong>General Performance:</strong></p><table><thead><tr><th>Model</th><th>MATH-500</th><th>AIME 24</th><th>AIME 25</th><th>OlympiadBench</th><th>HumanEval</th><th>HumanEval+</th><th>Livecodebench</th></tr></thead><tbody><tr><td><strong>PromptCoT-Mamba-7B</strong></td><td><strong>84.6</strong></td><td>üî•üî•<strong>35.2</strong></td><td>üî•üî•<strong>24.6</strong></td><td><strong>50.7</strong></td><td><strong>81.7</strong></td><td><strong>75.0</strong></td><td>üî•üî•<strong>29.9</strong></td></tr><tr><td>Gemma3-27B</td><td><strong>89.0</strong></td><td>32.6</td><td>24.0</td><td><strong>54.2</strong></td><td><strong>86.0</strong></td><td><strong>78.0</strong></td><td>26.9</td></tr><tr><td>Gemma3-12B</td><td>83.8</td><td>22.9</td><td>19.2</td><td>49.9</td><td>81.1</td><td>73.2</td><td>22.2</td></tr><tr><td>Sky-T1-7B</td><td>85.0</td><td>19.2</td><td>19.2</td><td>49.2</td><td>41.5</td><td>37.2</td><td>18.3</td></tr><tr><td>S1.1-7B</td><td>82.0</td><td>19.2</td><td>17.5</td><td>43.1</td><td>64.0</td><td>56.7</td><td>13.3</td></tr><tr><td>Bespoke-Stratos-7B</td><td>81.2</td><td>18.3</td><td>16.3</td><td>45.0</td><td>73.2</td><td>68.3</td><td>8.6</td></tr><tr><td>Nemotron-H-8B</td><td>77.6</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td><td>79.3</td><td>74.4</td><td>&ndash;</td></tr><tr><td>M1-3B</td><td>81.7</td><td>23.0</td><td>22.0</td><td>43.6</td><td>&ndash;</td><td>&ndash;</td><td>&ndash;</td></tr></tbody></table><p><strong>Math Specialization vs. Generalist:</strong></p><table><thead><tr><th>Model</th><th>MATH-500</th><th>AIME 24</th><th>AIME 25</th><th>OlympiadBench</th><th>HumanEval</th><th>HumanEval+</th><th>Livecodebench</th></tr></thead><tbody><tr><td><strong>PromptCoT-Mamba-Math-7B</strong></td><td>üî•üî•<strong>88.0</strong></td><td>üî•üî•<strong>42.9</strong></td><td>üî•üî•<strong>30.8</strong></td><td>üî•üî•<strong>52.1</strong></td><td>71.3</td><td>66.5</td><td>20.3</td></tr><tr><td>PromptCoT-Mamba-7B</td><td>84.6</td><td>35.2</td><td>24.6</td><td>50.7</td><td><strong>81.7</strong></td><td><strong>75.0</strong></td><td><strong>29.9</strong></td></tr></tbody></table><hr><h2 id=citation><strong>Citation</strong><a hidden class=anchor aria-hidden=true href=#citation>#</a></h2><p>If you find <strong>PromptCoT</strong> or <strong>PromptCoT-Mamba</strong> useful in your research, please consider citing the respective papers:</p><p><strong>For PromptCoT:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>zhao2025promptcot</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>    <span class=p>=</span> <span class=s>{Zhao, Xueliang and Wu, Wei and Guan, Jian and Kong, Lingpeng}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>     <span class=p>=</span> <span class=s>{PromptCoT: Synthesizing Olympiad-Level Problems for Mathematical Reasoning in Large Language Models}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>      <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span>   <span class=p>=</span> <span class=s>{arXiv preprint arXiv:2503.02324}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>       <span class=p>=</span> <span class=s>{http://arxiv.org/abs/2503.02324}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p><strong>For PromptCoT-Mamba:</strong></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>zhao2025scaling</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>author</span>    <span class=p>=</span> <span class=s>{Xueliang Zhao and Wei Wu and Lingpeng Kong}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>title</span>     <span class=p>=</span> <span class=s>{Scaling Reasoning without Attention}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>journal</span>   <span class=p>=</span> <span class=s>{arXiv preprint arXiv:2505.22425}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>year</span>      <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=na>url</span>       <span class=p>=</span> <span class=s>{https://arxiv.org/abs/2505.22425}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
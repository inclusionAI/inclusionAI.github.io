<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态 | INCLUSION AI</title><meta name=keywords content><meta name=description content="Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态 全模态Ming-omni系列更新！Ming-flash-omni是首个参数规模达到千亿的开源全模态大模型。基于Ling 2.0的稀疏MoE架构，Ming-flash-omni总参数103B， 激活9B。相比Ming-lite-omni，Ming-flash-omni 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别 等领域性能表现尤为突出。
能力一览 可控图像生成 针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。
流式视频理解 用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。
语音及方言理解 Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。
音色克隆 Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。
模型架构及能力简介 Ming-flash-omni-Preview 的模型结构图:
相比 Ming-lite-omni-1.5, Ming-flash-omni-Preview 主要有以下方面的技术优化:
基于稀疏专家架构的全模态训练 Ming-flash-omni-Preview 将 Ling-Flash-2.0 稀疏 MoE 架构拓展到全模态大模型，基于 Ming-lite-omni 提出的模态级路由实现对各模态分布和路由策略建模，实现各模态的 “大容量、小激活”。通过在 Attention 层引入 VideoRoPE，强化对长视频的时空建模，提升视频交互能力。 另外在训练策略上："><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/ming-flash-omni-preview/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/ming-flash-omni-preview/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/ming-flash-omni-preview/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态"><meta property="og:description" content="Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态 全模态Ming-omni系列更新！Ming-flash-omni是首个参数规模达到千亿的开源全模态大模型。基于Ling 2.0的稀疏MoE架构，Ming-flash-omni总参数103B， 激活9B。相比Ming-lite-omni，Ming-flash-omni 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别 等领域性能表现尤为突出。
能力一览 可控图像生成 针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。
流式视频理解 用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。
语音及方言理解 Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。
音色克隆 Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。
模型架构及能力简介 Ming-flash-omni-Preview 的模型结构图:
相比 Ming-lite-omni-1.5, Ming-flash-omni-Preview 主要有以下方面的技术优化:
基于稀疏专家架构的全模态训练 Ming-flash-omni-Preview 将 Ling-Flash-2.0 稀疏 MoE 架构拓展到全模态大模型，基于 Ming-lite-omni 提出的模态级路由实现对各模态分布和路由策略建模，实现各模态的 “大容量、小激活”。通过在 Attention 层引入 VideoRoPE，强化对长视频的时空建模，提升视频交互能力。 另外在训练策略上："><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/ming-flash-omni-preview/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content><meta property="article:published_time" content="2025-10-28T00:00:03+08:00"><meta property="article:modified_time" content="2025-10-28T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态"><meta name=twitter:description content="Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态 全模态Ming-omni系列更新！Ming-flash-omni是首个参数规模达到千亿的开源全模态大模型。基于Ling 2.0的稀疏MoE架构，Ming-flash-omni总参数103B， 激活9B。相比Ming-lite-omni，Ming-flash-omni 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别 等领域性能表现尤为突出。
能力一览 可控图像生成 针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。
流式视频理解 用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。
语音及方言理解 Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。
音色克隆 Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。
模型架构及能力简介 Ming-flash-omni-Preview 的模型结构图:
相比 Ming-lite-omni-1.5, Ming-flash-omni-Preview 主要有以下方面的技术优化:
基于稀疏专家架构的全模态训练 Ming-flash-omni-Preview 将 Ling-Flash-2.0 稀疏 MoE 架构拓展到全模态大模型，基于 Ming-lite-omni 提出的模态级路由实现对各模态分布和路由策略建模，实现各模态的 “大容量、小激活”。通过在 Attention 层引入 VideoRoPE，强化对长视频的时空建模，提升视频交互能力。 另外在训练策略上："><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态","item":"https://inclusionai.github.io/zh/ming-flash-omni-preview/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态","name":"Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态","description":"Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态 全模态Ming-omni系列更新！Ming-flash-omni是首个参数规模达到千亿的开源全模态大模型。基于Ling 2.0的稀疏MoE架构，Ming-flash-omni总参数103B， 激活9B。相比Ming-lite-omni，Ming-flash-omni 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别 等领域性能表现尤为突出。\n能力一览 可控图像生成 针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。\n流式视频理解 用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。\n语音及方言理解 Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。\n音色克隆 Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。\n模型架构及能力简介 Ming-flash-omni-Preview 的模型结构图:\n相比 Ming-lite-omni-1.5, Ming-flash-omni-Preview 主要有以下方面的技术优化:\n基于稀疏专家架构的全模态训练 Ming-flash-omni-Preview 将 Ling-Flash-2.0 稀疏 MoE 架构拓展到全模态大模型，基于 Ming-lite-omni 提出的模态级路由实现对各模态分布和路由策略建模，实现各模态的 “大容量、小激活”。通过在 Attention 层引入 VideoRoPE，强化对长视频的时空建模，提升视频交互能力。 另外在训练策略上：","keywords":[],"articleBody":"Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态 全模态Ming-omni系列更新！Ming-flash-omni是首个参数规模达到千亿的开源全模态大模型。基于Ling 2.0的稀疏MoE架构，Ming-flash-omni总参数103B， 激活9B。相比Ming-lite-omni，Ming-flash-omni 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别 等领域性能表现尤为突出。\n能力一览 可控图像生成 针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。\n流式视频理解 用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。\n语音及方言理解 Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。\n音色克隆 Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。\n模型架构及能力简介 Ming-flash-omni-Preview 的模型结构图:\n相比 Ming-lite-omni-1.5, Ming-flash-omni-Preview 主要有以下方面的技术优化:\n基于稀疏专家架构的全模态训练 Ming-flash-omni-Preview 将 Ling-Flash-2.0 稀疏 MoE 架构拓展到全模态大模型，基于 Ming-lite-omni 提出的模态级路由实现对各模态分布和路由策略建模，实现各模态的 “大容量、小激活”。通过在 Attention 层引入 VideoRoPE，强化对长视频的时空建模，提升视频交互能力。 另外在训练策略上：\n稳定稀疏训练：使用混合专家平衡方案（结合辅助负载均衡损失与路由器偏置更新），确保稀疏 MoE 架构下全模态训练的均匀激活和收敛性； 上下文感知的 ASR 训练范式：语音训练任务上以任务 / 领域信息输入作为解码条件，显著提高专有名词识别和转录一致性。同时引入高质量方言等训练语料，实现对湖南话、闽南话、粤语等 15 种中国方言的识别准确率显著提升。 生成式分割编辑一体化 在构建统一多模态模型时，核心挑战在于如何高效融合图像的理解与生成能力。我们的Ming-lite-omni-1.5 通过冻结语言通路，并借助多尺度QueryToken注入层级化语义，从而在保持理解性能的同时，使生成目标能更好地与理解任务融合。这一训练策略虽然提升了稳定性，但由于理解与生成的学习目标本质上存在差异，即使引入层级化语义，那些细粒度的视觉知识（如物体属性和空间关系）仍难以高效迁移到高精度的生成与编辑任务中，进而限制了模型在生成质量和可控性上的提升。 为克服这一瓶颈，Ming-flash-omni-Preview 提出了 “生成式分割即编辑” 的协同训练范式。该范式将图像分割重构为语义保持的编辑任务（例如：“将香蕉涂成紫色”）。相应的设计所提供的关键帮助是：强制统一了理解和生成目标 —— 成功的编辑必须依赖对对象轮廓的精确理解，编辑质量直接为理解提供监督信号。这一范式直接增强了模型的细粒度时空语义控制能力，间接解决了纯文本到图像生成中的组合性问题。 在 GenEval 基准测试中，Ming-flash-omni-Preview 取得了 0.90 分，超越所有领先的非强化学习（non-RL）方法；在 GEdit 基准测试中，在物体删除、物体替换等精准编辑任务上的均分从 6.9 提升至 7.9。这两项结果共同证明：通过“生成式分割即编辑”训练所获得的细粒度时空语义控制能力，不仅显著提升了精准编辑任务的表现，还能够有效泛化到纯文本驱动的图像生成任务中。\n高效全模态训练架构 训练全模态基础模型面临两大挑战：数据异构性（多模态输入形状不一）和模型异构性（模态专用编码器难以并行）。这些问题导致负载失衡、内存碎片化和流水线气泡，严重拖慢了训练速度。 为解决这些问题，我们在训练 Ming-flash-omni-Preview 模型时基于 Megatron-LM 框架进行了两项关键优化：\n序列打包 (Sequence Packing)：解决数据异构性。将变长序列密集打包成定长批次，大幅提升内存利用率和计算密度； 弹性编码器分片 (Flexible Encoder Sharding)：解决模型异构性。扩展 Megatron-LM 支持模态编码器在 DP/PP/TP 上的细粒度分片，消除流水线气泡，实现负载均衡。 这些优化措施使 Ming-flash-omni-Preview 的训练吞吐量比基线提升了一倍。 开始使用 Ming-flash-omni-Preview 我们的模型和代码均已开源，欢迎大家试用、反馈和交流： ● GitHub：https://github.com/inclusionAI/Ming\n● Hugging Face: https://huggingface.co/inclusionAI/Ming-flash-omni-Preview\n● ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview\n后续规划 这次开放的是 Ming-flash-omni-Preview 版， 当前版本有一些不完善的地方:\n视觉文本理解能力，虽然Ming-flash-omni-Preview在全模态模型中效果整体领先，但和SOTA的专用VL大模型仍存在一定差距，我们会继续探索全模态omni模型的效果上限。 语音能力：在语音识别和语音合成整体效果领先，语音多轮对话效果以及高质量的音色克隆是我们下一步的优化重点。 图片生成能力: 模型在 GenEval 基准上取得 0.9分，展现了不错的可控性，并已具备文字生成和编辑能力，但在复杂布局文字渲染与编辑，以及特定IP 角色的生成方面还有待提升。 我们仍在持续优化 Ming-flash-omni-Preview 的使用体验，欢迎通过社区 discussion 讨论或 issue 向我们反馈问题，正式版本会很快跟大家见面。\n","wordCount":"159","inLanguage":"zh","datePublished":"2025-10-28T00:00:03+08:00","dateModified":"2025-10-28T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/ming-flash-omni-preview/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态</h1><div class=post-meta><span title='2025-10-28 00:00:03 +0800 +0800'>2025年10月28日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;159 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/ming-flash-omni-preview/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=ming-flash-omni-preview千亿参数-moe洞察与创造一体的全模态>Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态<a hidden class=anchor aria-hidden=true href=#ming-flash-omni-preview千亿参数-moe洞察与创造一体的全模态>#</a></h1><p>全模态Ming-omni系列更新！Ming-flash-omni是首个参数规模达到千亿的开源全模态大模型。基于Ling 2.0的稀疏MoE架构，Ming-flash-omni总参数103B， 激活9B。相比Ming-lite-omni，Ming-flash-omni 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别 等领域性能表现尤为突出。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_drbxn1/afts/img/5hflRY595xwAAAAAgBAAAAgADkliAQFr/original alt=performance></p><h2 id=能力一览>能力一览<a hidden class=anchor aria-hidden=true href=#能力一览>#</a></h2><h3 id=可控图像生成>可控图像生成<a hidden class=anchor aria-hidden=true href=#可控图像生成>#</a></h3><p>针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。</p><p><video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/cb4mSp1jTwQAAAAAgIAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h3 id=流式视频理解>流式视频理解<a hidden class=anchor aria-hidden=true href=#流式视频理解>#</a></h3><p>用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。</p><p><video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/n6k6SqtCCqMAAAAAgJAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h3 id=语音及方言理解>语音及方言理解<a hidden class=anchor aria-hidden=true href=#语音及方言理解>#</a></h3><p>Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。</p><p><video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/iEf7QK3W3m4AAAAAgBAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h3 id=音色克隆>音色克隆<a hidden class=anchor aria-hidden=true href=#音色克隆>#</a></h3><p>Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。</p><p><video src=https://gw.alipayobjects.com/v/huamei_drbxn1/afts/video/Ru5dTrMPb30AAAAAgBAAAAgAfoeUAQBr width=704px height=740px controls></video></p><h2 id=模型架构及能力简介>模型架构及能力简介<a hidden class=anchor aria-hidden=true href=#模型架构及能力简介>#</a></h2><p>Ming-flash-omni-Preview 的模型结构图:</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_drbxn1/afts/img/MdHMSqYQCqAAAAAAVcAAAAgADkliAQFr/fmt.avif alt=architecture></p><p>相比 Ming-lite-omni-1.5, Ming-flash-omni-Preview 主要有以下方面的技术优化:</p><h3 id=基于稀疏专家架构的全模态训练>基于稀疏专家架构的全模态训练<a hidden class=anchor aria-hidden=true href=#基于稀疏专家架构的全模态训练>#</a></h3><p>Ming-flash-omni-Preview 将 Ling-Flash-2.0 稀疏 MoE 架构拓展到全模态大模型，基于 Ming-lite-omni 提出的模态级路由实现对各模态分布和路由策略建模，实现各模态的 “大容量、小激活”。通过在 Attention 层引入 VideoRoPE，强化对长视频的时空建模，提升视频交互能力。 另外在训练策略上：</p><ol><li>稳定稀疏训练：使用混合专家平衡方案（结合辅助负载均衡损失与路由器偏置更新），确保稀疏 MoE 架构下全模态训练的均匀激活和收敛性；</li><li>上下文感知的 ASR 训练范式：语音训练任务上以任务 / 领域信息输入作为解码条件，显著提高专有名词识别和转录一致性。同时引入高质量方言等训练语料，实现对湖南话、闽南话、粤语等 15 种中国方言的识别准确率显著提升。</li></ol><h3 id=生成式分割编辑一体化>生成式分割编辑一体化<a hidden class=anchor aria-hidden=true href=#生成式分割编辑一体化>#</a></h3><p>在构建统一多模态模型时，核心挑战在于如何高效融合图像的理解与生成能力。我们的Ming-lite-omni-1.5 通过冻结语言通路，并借助多尺度QueryToken注入层级化语义，从而在保持理解性能的同时，使生成目标能更好地与理解任务融合。这一训练策略虽然提升了稳定性，但由于理解与生成的学习目标本质上存在差异，即使引入层级化语义，那些细粒度的视觉知识（如物体属性和空间关系）仍难以高效迁移到高精度的生成与编辑任务中，进而限制了模型在生成质量和可控性上的提升。
为克服这一瓶颈，Ming-flash-omni-Preview 提出了 “生成式分割即编辑” 的协同训练范式。该范式将图像分割重构为语义保持的编辑任务（例如：“将香蕉涂成紫色”）。相应的设计所提供的关键帮助是：强制统一了理解和生成目标 —— 成功的编辑必须依赖对对象轮廓的精确理解，编辑质量直接为理解提供监督信号。这一范式直接增强了模型的细粒度时空语义控制能力，间接解决了纯文本到图像生成中的组合性问题。
在 GenEval 基准测试中，Ming-flash-omni-Preview 取得了 0.90 分，超越所有领先的非强化学习（non-RL）方法；在 GEdit 基准测试中，在物体删除、物体替换等精准编辑任务上的均分从 6.9 提升至 7.9。这两项结果共同证明：通过“生成式分割即编辑”训练所获得的细粒度时空语义控制能力，不仅显著提升了精准编辑任务的表现，还能够有效泛化到纯文本驱动的图像生成任务中。</p><h3 id=高效全模态训练架构>高效全模态训练架构<a hidden class=anchor aria-hidden=true href=#高效全模态训练架构>#</a></h3><p>训练全模态基础模型面临两大挑战：数据异构性（多模态输入形状不一）和模型异构性（模态专用编码器难以并行）。这些问题导致负载失衡、内存碎片化和流水线气泡，严重拖慢了训练速度。
为解决这些问题，我们在训练 Ming-flash-omni-Preview 模型时基于 Megatron-LM 框架进行了两项关键优化：</p><ol><li>序列打包 (Sequence Packing)：解决数据异构性。将变长序列密集打包成定长批次，大幅提升内存利用率和计算密度；</li><li>弹性编码器分片 (Flexible Encoder Sharding)：解决模型异构性。扩展 Megatron-LM 支持模态编码器在 DP/PP/TP 上的细粒度分片，消除流水线气泡，实现负载均衡。
这些优化措施使 Ming-flash-omni-Preview 的训练吞吐量比基线提升了一倍。</li></ol><h2 id=开始使用-ming-flash-omni-preview>开始使用 Ming-flash-omni-Preview<a hidden class=anchor aria-hidden=true href=#开始使用-ming-flash-omni-preview>#</a></h2><p>我们的模型和代码均已开源，欢迎大家试用、反馈和交流：
● GitHub：https://github.com/inclusionAI/Ming</p><p>● Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-flash-omni-Preview>https://huggingface.co/inclusionAI/Ming-flash-omni-Preview</a></p><p>● ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview>https://www.modelscope.cn/models/inclusionAI/Ming-flash-omni-Preview</a></p><h2 id=后续规划>后续规划<a hidden class=anchor aria-hidden=true href=#后续规划>#</a></h2><p>这次开放的是 Ming-flash-omni-Preview 版， 当前版本有一些不完善的地方:</p><ol><li>视觉文本理解能力，虽然Ming-flash-omni-Preview在全模态模型中效果整体领先，但和SOTA的专用VL大模型仍存在一定差距，我们会继续探索全模态omni模型的效果上限。</li><li>语音能力：在语音识别和语音合成整体效果领先，语音多轮对话效果以及高质量的音色克隆是我们下一步的优化重点。</li><li>图片生成能力: 模型在 GenEval 基准上取得 0.9分，展现了不错的可控性，并已具备文字生成和编辑能力，但在复杂布局文字渲染与编辑，以及特定IP 角色的生成方面还有待提升。</li></ol><p>我们仍在持续优化 Ming-flash-omni-Preview 的使用体验，欢迎通过社区 discussion 讨论或 issue 向我们反馈问题，正式版本会很快跟大家见面。</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni V1.5 介绍 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗Hugging Face) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 表A。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Lite-Omni V1.5 介绍"><meta property="og:description" content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗Hugging Face) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 表A。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-21T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-21T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni V1.5 介绍"><meta name=twitter:description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗Hugging Face) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 表A。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni V1.5 介绍","item":"https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni V1.5 介绍","name":"Ming-Lite-Omni V1.5 介绍","description":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\n前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗Hugging Face) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。\n性能对比图\n模型架构图\n详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。\n图像/语音生成\n图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID \u0026amp; Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 表A。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 图像/文本/视频/语音理解\nMRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 用户偏好对齐\n为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.","keywords":[],"articleBody":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\n前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗Hugging Face) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。\n性能对比图\n模型架构图\n详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。\n图像/语音生成\n图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID \u0026 Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 表A。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 图像/文本/视频/语音理解\nMRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 用户偏好对齐\n为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87.07%, 使用体验得到了明显优化。\n自建体验集评测维度 Qwen2.5-VL-7B Ming-Omni-Lite V1.5 相关性 4.308 4.5 流畅性 4.765 4.91 内容丰富性 3.828 3.69 格式合理性 4.727 4.8 正确性 3.741 3.92 均分 4.274 4.365 Demo展示 图像编辑 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID \u0026 Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。\n生成式图像分割 Next User Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. Ming-Lite-Omni V1.5 User Please segment different classes in this image. Ming-Lite-Omni V1.5 User Please segment different instances in this image. Ming-Lite-Omni V1.5 图像生成 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高GenEval评测指标中与位置、结构、数量相关的得分： 1-Obj 2-Obj Counting Colors Position Color Attr Avg. Ming-lite-omni 0.99 0.77 0.68 0.78 0.46 0.42 0.64 Ming-lite-omni V1.5 0.99 0.93 0.86 0.87 0.90 0.66 0.87 原图 生成的深度图 生成的检测框 生成的边缘轮廓 图文对话 Next User 这张图片看起来设计感很强烈，可以详细描述一下它的各个设计元素？ Ming-Lite-Omni 这张图片展示了一款高端游戏鼠标，具有现代感和科技感的设计。鼠标主体为黑色，表面光滑且具有金属质感。鼠标顶部有一个蓝色的发光标志，标志下方是鼠标滚轮和左右按键区域。鼠标侧面有蓝色的灯光装饰，增加了视觉吸引力。鼠标底部有防滑纹理设计，确保使用时的稳定性。整体设计简洁而富有未来感，适合游戏玩家使用。 Ming-Lite-Omni V1.5 这张图片展示了一款设计感强烈的游戏鼠标，具有以下设计元素： 1. 颜色和材质： - 鼠标主体为黑色，表面光滑且具有金属质感。 - 鼠标边缘和按钮部分有蓝色LED灯带，增加了科技感和视觉吸引力。 2. 形状和结构： - 鼠标整体呈流线型设计，顶部略微倾斜，便于手部自然放置。 - 鼠标侧面有明显的曲线设计，符合人体工程学，提供舒适的握持感。 3. 按钮和滚轮： - 鼠标左侧有一个滚轮，滚轮上方有蓝色LED灯带。 - 鼠标右侧有额外的按钮，这些按钮也有蓝色LED灯带，增加了功能性和美观性。 4. 品牌标识： - 鼠标底部中央有一个发光的“G”标志，这是该品牌的标志性设计元素。 5. 灯光效果： - 鼠标上的蓝色LED灯带在暗色背景下显得尤为突出，增强了科技感和未来感。 6. 纹理和细节： - 鼠标顶部有纹理设计，可能是为了增加防滑效果。 - 鼠标侧面有细长的纹理设计，增加了视觉上的层次感。 整体来看，这款鼠标的设计结合了现代科技感和人体工程学设计，既美观又实用，适合游戏玩家使用。 User 图中的书法为什么好？ Ming-Lite-Omni 这幅书法作品展示了高超的书法技巧和深厚的文化底蕴。笔法流畅自然，字形结构严谨，布局合理，体现了书法家的深厚功力和艺术修养。 Ming-Lite-Omni V1.5 这幅书法作品展示了几个显著的特点，使其成为一件优秀的书法作品： 1. 笔法精湛：书法的笔法流畅自然，线条粗细变化丰富，表现出书法家的深厚功力和熟练技巧。 2. 结构严谨：每个字的结构都经过精心设计，笔画之间的衔接自然，字与字之间保持了良好的平衡和协调。 3. 章法布局：整幅作品的章法布局合理，行距和字距均匀，给人一种整齐有序的感觉。 4. 墨色变化：墨色的浓淡变化自然，增添了作品的层次感和立体感。 5. 内容优美：书法内容是一篇优美的古文，文字内容富有哲理和文学价值，增加了作品的文化内涵。 6. 印章点缀：作品上有红色的印章，印章的使用不仅增加了作品的装饰性，还起到了点缀和平衡的作用。 这些特点共同构成了这幅书法作品的高超艺术价值，使其成为一件值得欣赏和学习的佳作。 User 图中这款手机适合给父母买吗？ Ming-Lite-Omni 这款手机外观简约时尚，适合追求简约风格的父母使用。相机配置也较为先进，适合喜欢拍照的用户。 Ming-Lite-Omni V1.5 这款手机适合给父母购买，原因如下： 1. 外观设计：手机采用简约时尚的设计，白色机身显得干净利落，符合现代审美。 2. 摄像头配置：后置摄像头设计独特，排列整齐，可能提供良好的拍照体验。 3. 品牌：荣耀（Honor）品牌在智能手机市场上有良好的口碑，提供稳定可靠的产品。 4. 功能：通常这类手机会配备大容量电池和长续航能力，适合日常使用。 5. 易用性：可能提供简洁的用户界面和易于操作的功能，适合不太熟悉复杂操作的用户。 6. 性价比：荣耀手机通常以高性价比著称，适合预算有限但希望获得良好体验的用户。 当然，具体是否适合还需考虑父母的实际需求和偏好。 User 如何使用图中软件购买机票？请列出具体操作步骤 Ming-Lite-Omni 使用图中软件购买机票的具体操作步骤如下： 1. 打开携程app。 2. 点击“机票”选项。 3. 在机票页面选择出发地、目的地、出发日期和返回日期。 4. 点击搜索按钮，查看可用的航班信息。 5. 选择合适的航班后，点击“预订”按钮。 6. 填写乘客信息并选择支付方式。 7. 确认订单信息并提交支付，完成购票。 Ming-Lite-Omni V1.5 使用携程软件购买机票的具体操作步骤如下： 1. 下载并安装携程App： - 如果您还没有安装携程App，可以在应用商店（如苹果App Store或安卓应用市场）搜索“携程旅行”并下载安装。 2. 打开携程App： - 安装完成后，打开携程App。 3. 选择“机票”选项： - 在首页的导航栏中，点击“机票”选项。 4. 输入出发地和目的地： - 在机票预订页面，输入您的出发地和目的地。 5. 选择出发日期和返回日期： - 选择您的出发日期和返回日期（如果是往返票）。 6. 选择乘客信息： - 输入乘客人数和座位类型（成人、儿童、婴儿）。 7. 选择航班： - 浏览并选择您喜欢的航班。 8. 填写乘客信息： - 输入乘客的姓名、身份证号码、联系方式等信息。 9. 选择支付方式： - 选择您喜欢的支付方式（如支付宝、微信支付、银行卡等）。 10. 确认订单并支付： - 确认订单信息无误后，点击“支付”按钮完成支付。 11. 接收电子机票： - 支付成功后，您将收到电子机票，可以通过App内的“我的订单”查看或下载。 通过以上步骤，您就可以使用携程App轻松购买机票了。 User 根据图表计算样本之间的 Δct 在哪一个引物下相差最小？ | 根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。（图片来源：互联网公开-舒桐科技公众号） Ming-Lite-Omni 根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。 Ming-Lite-Omni V1.5 在 EGFP-cGAS单克隆 引物下，样本之间的 Δct 相差最小。 计算过程： 1. 理解 Δct 的含义： Δct 是目标基因（cGAS-qF+qR 或 EGFP-cGAS）与内参基因（cTubulin）的相对表达量差值。Δct 越小，说明目标基因与内参基因的表达量差异越小。 2. 比较不同引物下的 Δct： - cGAS-qF+qR 引物： - 野生型：Δct = 8.06 - EGFP-cGAS单克隆：Δct = -0.15 - 差异：8.06 - (-0.15) = 8.21 - EGFP-cGAS单克隆 引物： - 野生型：Δct = 10.44 - EGFP-cGAS单克隆：Δct = -2.9 - 差异：10.44 - (-2.9) = 13.34 - EGFP-cGAS-qF+qR 引物： - 野生型：Δct = 21.43 - EGFP-cGAS单克隆：Δct = -13.45 - 差异：21.43 - (-13.45) = 34.88 3. 结论： 在 EGFP-cGAS单克隆 引物下，样本之间的 Δct 差异最小（Δct 差异为 13.34），说明该引物下样本之间的表达量差异最小。 总结： 在 EGFP-cGAS单克隆 引物下，样本之间的 Δct 相差最小。 语音合成 输入文本 Ming-lite-omni Ming-lite-omni V1.5 Tyler, Lucy, Michelle, we’re going to space! what is the forecast for California for rain Television can make you dumb, but it can also be good education. OCR文档理解 Next User 在判断题第95题中，考生答题的手写内容是什么？若考生的手写答案为√则输出对，若考生的手写答案为×则输出错，请输出对或错 Ming-Lite-Omni 对 Ming-Lite-Omni V1.5 错 User Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters. Ming-Lite-Omni 9743 Ming-Lite-Omni V1.5 9742 User What is the Latex tag for mathematical expression in images? Ming-Lite-Omni A C = C B \\therefore C D / / B E Ming-Lite-Omni V1.5 A C = C B \\because C D / / B E User 从图中提取: 日期 Ming-Lite-Omni 日期：2019年12月03日05时08分 从图中提取: 里程 里程：23.9公里 从图中提取: 金额 金额：52.6元 从图中提取: 状态 状态： 从图中提取: 客票印务 客票印务2019年12月印900000卷 从图中提取: 号码 号码从00000001至90000000止 Ming-Lite-Omni V1.5 日期: 2021年03月05日 User What is the difference in value between Lamb and Corn?’ You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \\boxed{}.' Ming-Lite-Omni 0.37 Ming-Lite-Omni V1.5 [think]The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.[/think]\\boxed{0.57} 视频理解 Users 视频里的小女孩在做什么？ Ming-Lite-Omni 视频中的场景发生在一家商店里，可能是超市或大型零售店，货架上摆满了各种商品，包括零食和家庭用品。视频里的小女孩正在拉动一个展示架，看起来她在尝试移动或者玩耍这个展示架。价格标签清晰可见，其中一个显著地标记着蓝色和白色的'18,690'字样。店内照明良好，荧光灯照亮了整个区域。如需更详细的分析或其他帮助，请告诉我！ Ming-Lite-Omni V1.5 小女孩坐在购物车里睡觉。 Users 描述下视频中主人公的穿着。 Ming-Lite-Omni 视频中，主人公穿着红色上衣，肩上背着一个彩色图案的包。 Ming-Lite-Omni V1.5 视频中的主人公穿着一件红色的高领毛衣。 Users 推测一下这个视频的拍摄时间和位置。 Ming-Lite-Omni 从视频中的画面来看，这应该是在纽约的中央公园拍摄的。画面中可以看到中央公园的标志性建筑和景观，比如中央公园的湖泊、桥梁和雕像。此外，视频中还有游客在公园内散步、拍照和享受户外活动，这进一步确认了拍摄地点为纽约的中央公园。 Ming-Lite-Omni V1.5 从视频中的场景来看，拍摄地点位于纽约中央公园。视频展示了公园内的多个景点，包括湖泊、桥梁、雕像和喷泉等。拍摄时间似乎是在白天，阳光明媚，天气晴朗，适合户外活动。 开始使用 Ming-lite-omni v1.5 Ming-lite-omni v1.5的模型和代码已开源，诚邀大家试用、反馈和交流。值得期待的是，我们即将发布量化加速版本的Ming-lite-omni，该版本将不仅进一步优化全模态效果，还使Ming-lite-omni更加轻量化，同时强化多模推理和生成能力。欢迎持续关注！\nGithub: https://github.com/inclusionAI/Ming Hugging Face: https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5 ModelScope: https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5 ","wordCount":"670","inLanguage":"zh","datePublished":"2025-07-21T00:00:03+08:00","dateModified":"2025-07-21T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Omni V1.5 介绍</h1><div class=post-meta><span title='2025-07-21 00:00:03 +0800 +0800'>2025年7月21日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;670 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> 🤗 <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>Hugging Face</a>｜ 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>ModelScope</a></p><h2 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h2><p>本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗<a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>Hugging Face</a>) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。</p><div style=text-align:center;margin:auto;width:70%><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/dv7TTL8MW5EAAAAAWtAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>性能对比图</p></div><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/A_LZTJcsc3EAAAAAXuAAAAgAeuUHAQFr/fmt.webp alt="Image description"><p style=font-size:14px;color:gray>模型架构图</p></div><h2 id=详细介绍>详细介绍<a hidden class=anchor aria-hidden=true href=#详细介绍>#</a></h2><p>为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。</p><p><strong>图像/语音生成</strong></p><ol><li>图像生成侧采用<u>双分支解耦策略</u>提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。</li><li>为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了<u>ID & Scene Consistency Loss</u>，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。</li><li>引入<u>感知增强策略</u>。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 <a href=#table1>表A</a>。</li><li>引入<u>多任务协同学习策略</u>。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。</li><li>语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。</li><li>语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。</li><li>全方位数据升级<ul><li>获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。</li><li>采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。</li><li>构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。</li></ul></li></ol><p><strong>图像/文本/视频/语音理解</strong></p><ol><li>MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。</li><li>高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需<u>分步冻结/解冻 LLM 的预训练流程</u>，升级为<u>高效全参数训练</u>，训练周期缩短 26.5%，保持性能无损。</li><li>针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。</li><li>针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。</li><li>全方位数据升级<ul><li>预训练阶段<ul><li>新增文本实体结构化数据，补全图谱盲区。</li><li>扩充高质量商品数据，提升通识能力。</li></ul></li><li>指令微调阶段<ul><li>提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。</li><li>提高垂类识别（动植物/车辆/食材等）数据深度。</li><li>从数据角度优化跨学科复杂图文推理能力。</li><li>针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。</li></ul></li></ul></li></ol><p><strong>用户偏好对齐</strong></p><p>为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了<u>体验评测集</u>，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1.5在内部对抗评测集上相比Ming-lite-omni v1 胜和率为 87.07%, 使用体验得到了明显优化。</p><div style=text-align:center;margin:auto;width:80%;line-height:1.1><table style="margin:0 auto"><thead><tr><th style=text-align:center;padding:5px>自建体验集评测维度</th><th style=text-align:center;padding:5px>Qwen2.5-VL-7B</th><th style=text-align:center;padding:5px>Ming-Omni-Lite V1.5</th></tr></thead><tbody><tr><td style=text-align:center;padding:2px>相关性</td><td style=text-align:center;padding:2px>4.308</td><td style=text-align:center;padding:2px>4.5</td></tr><tr><td style=text-align:center;padding:2px>流畅性</td><td style=text-align:center;padding:2px>4.765</td><td style=text-align:center;padding:2px>4.91</td></tr><tr><td style=text-align:center;padding:2px>内容丰富性</td><td style=text-align:center;padding:2px>3.828</td><td style=text-align:center;padding:2px>3.69</td></tr><tr><td style=text-align:center;padding:2px>格式合理性</td><td style=text-align:center;padding:2px>4.727</td><td style=text-align:center;padding:2px>4.8</td></tr><tr><td style=text-align:center;padding:2px>正确性</td><td style=text-align:center;padding:2px>3.741</td><td style=text-align:center;padding:2px>3.92</td></tr><tr><td style=text-align:center;padding:2px><strong>均分</strong></td><td style=text-align:center;padding:2px><strong>4.274</strong></td><td style=text-align:center;padding:2px><strong>4.365</strong></td></tr></tbody></table></div><br><h2 id=demo展示>Demo展示<a hidden class=anchor aria-hidden=true href=#demo展示>#</a></h2><h3 id=图像编辑>图像编辑<a hidden class=anchor aria-hidden=true href=#图像编辑>#</a></h3><p>为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了<u>ID & Scene Consistency Loss</u>，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。</p><div style=text-align:center><img src=https://mdn.alipayobjects.com/huamei_aukff7/afts/img/e-mDS5UyUogAAAAAgCAAAAgAeuUHAQFr/fmt.webp alt="Image description">
<video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/UoqbRYQnZYEAAAAAgCAAAAgAeuUHAQFr controls></video></div><div class="full-width-container example-container"><div class=example-content><div class=title><span>生成式图像分割</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp height=256px></figure>Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different classes in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp height=256px></figure></div><div class=role>User</div><div class=content>Please segment different instances in this image.</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp height=256px></figure></div></div></div></div><h3 id=图像生成>图像生成<a hidden class=anchor aria-hidden=true href=#图像生成>#</a></h3><p>引入<u>感知增强策略</u>。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高GenEval评测指标中与位置、结构、数量相关的得分：
<a id=table1></a></p><table><thead><tr><th></th><th style=text-align:center>1-Obj</th><th style=text-align:center>2-Obj</th><th style=text-align:center>Counting</th><th style=text-align:center>Colors</th><th style=text-align:center>Position</th><th style=text-align:center>Color Attr</th><th style=text-align:center>Avg.</th></tr></thead><tbody><tr><td>Ming-lite-omni</td><td style=text-align:center>0.99</td><td style=text-align:center>0.77</td><td style=text-align:center>0.68</td><td style=text-align:center>0.78</td><td style=text-align:center>0.46</td><td style=text-align:center>0.42</td><td style=text-align:center>0.64</td></tr><tr><td>Ming-lite-omni V1.5</td><td style=text-align:center>0.99</td><td style=text-align:center>0.93</td><td style=text-align:center>0.86</td><td style=text-align:center>0.87</td><td style=text-align:center>0.90</td><td style=text-align:center>0.66</td><td style=text-align:center>0.87</td></tr></tbody></table><table><thead><tr><th style=text-align:center>原图</th><th style=text-align:center>生成的深度图</th><th style=text-align:center>生成的检测框</th><th style=text-align:center>生成的边缘轮廓</th></tr></thead><tbody><tr><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td style=text-align:center><img loading=lazy src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>图文对话</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/2.jpg height=256px></figure>这张图片看起来设计感很强烈，可以详细描述一下它的各个设计元素？</div><div class=role>Ming-Lite-Omni</div><div class=content>这张图片展示了一款高端游戏鼠标，具有现代感和科技感的设计。鼠标主体为黑色，表面光滑且具有金属质感。鼠标顶部有一个蓝色的发光标志，标志下方是鼠标滚轮和左右按键区域。鼠标侧面有蓝色的灯光装饰，增加了视觉吸引力。鼠标底部有防滑纹理设计，确保使用时的稳定性。整体设计简洁而富有未来感，适合游戏玩家使用。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>这张图片展示了一款设计感强烈的游戏鼠标，具有以下设计元素：<br>1. <strong>颜色和材质</strong>： - 鼠标主体为黑色，表面光滑且具有金属质感。 - 鼠标边缘和按钮部分有蓝色LED灯带，增加了科技感和视觉吸引力。<br>2. <strong>形状和结构</strong>： - 鼠标整体呈流线型设计，顶部略微倾斜，便于手部自然放置。 - 鼠标侧面有明显的曲线设计，符合人体工程学，提供舒适的握持感。<br>3. <strong>按钮和滚轮</strong>： - 鼠标左侧有一个滚轮，滚轮上方有蓝色LED灯带。 - 鼠标右侧有额外的按钮，这些按钮也有蓝色LED灯带，增加了功能性和美观性。<br>4. <strong>品牌标识</strong>： - 鼠标底部中央有一个发光的“G”标志，这是该品牌的标志性设计元素。<br>5. <strong>灯光效果</strong>： - 鼠标上的蓝色LED灯带在暗色背景下显得尤为突出，增强了科技感和未来感。<br>6. <strong>纹理和细节</strong>： - 鼠标顶部有纹理设计，可能是为了增加防滑效果。 - 鼠标侧面有细长的纹理设计，增加了视觉上的层次感。<br>整体来看，这款鼠标的设计结合了现代科技感和人体工程学设计，既美观又实用，适合游戏玩家使用。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/3.webp height=256px></figure>图中的书法为什么好？</div><div class=role>Ming-Lite-Omni</div><div class=content>这幅书法作品展示了高超的书法技巧和深厚的文化底蕴。笔法流畅自然，字形结构严谨，布局合理，体现了书法家的深厚功力和艺术修养。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>这幅书法作品展示了几个显著的特点，使其成为一件优秀的书法作品：<br>1. <strong>笔法精湛</strong>：书法的笔法流畅自然，线条粗细变化丰富，表现出书法家的深厚功力和熟练技巧。<br>2. <strong>结构严谨</strong>：每个字的结构都经过精心设计，笔画之间的衔接自然，字与字之间保持了良好的平衡和协调。<br>3. <strong>章法布局</strong>：整幅作品的章法布局合理，行距和字距均匀，给人一种整齐有序的感觉。<br>4. <strong>墨色变化</strong>：墨色的浓淡变化自然，增添了作品的层次感和立体感。<br>5. <strong>内容优美</strong>：书法内容是一篇优美的古文，文字内容富有哲理和文学价值，增加了作品的文化内涵。<br>6. <strong>印章点缀</strong>：作品上有红色的印章，印章的使用不仅增加了作品的装饰性，还起到了点缀和平衡的作用。<br>这些特点共同构成了这幅书法作品的高超艺术价值，使其成为一件值得欣赏和学习的佳作。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/4.webp height=256px></figure>图中这款手机适合给父母买吗？</div><div class=role>Ming-Lite-Omni</div><div class=content>这款手机外观简约时尚，适合追求简约风格的父母使用。相机配置也较为先进，适合喜欢拍照的用户。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>这款手机适合给父母购买，原因如下：<br>1. <strong>外观设计</strong>：手机采用简约时尚的设计，白色机身显得干净利落，符合现代审美。<br>2. <strong>摄像头配置</strong>：后置摄像头设计独特，排列整齐，可能提供良好的拍照体验。<br>3. <strong>品牌</strong>：荣耀（Honor）品牌在智能手机市场上有良好的口碑，提供稳定可靠的产品。<br>4. <strong>功能</strong>：通常这类手机会配备大容量电池和长续航能力，适合日常使用。<br>5. <strong>易用性</strong>：可能提供简洁的用户界面和易于操作的功能，适合不太熟悉复杂操作的用户。<br>6. <strong>性价比</strong>：荣耀手机通常以高性价比著称，适合预算有限但希望获得良好体验的用户。<br>当然，具体是否适合还需考虑父母的实际需求和偏好。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/6.webp height=256px></figure>如何使用图中软件购买机票？请列出具体操作步骤</div><div class=role>Ming-Lite-Omni</div><div class=content>使用图中软件购买机票的具体操作步骤如下：<br>1. 打开携程app。 2. 点击“机票”选项。 3. 在机票页面选择出发地、目的地、出发日期和返回日期。 4. 点击搜索按钮，查看可用的航班信息。 5. 选择合适的航班后，点击“预订”按钮。 6. 填写乘客信息并选择支付方式。 7. 确认订单信息并提交支付，完成购票。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>使用携程软件购买机票的具体操作步骤如下：<br>1. <strong>下载并安装携程App</strong>： - 如果您还没有安装携程App，可以在应用商店（如苹果App Store或安卓应用市场）搜索“携程旅行”并下载安装。<br>2. <strong>打开携程App</strong>： - 安装完成后，打开携程App。<br>3. <strong>选择“机票”选项</strong>： - 在首页的导航栏中，点击“机票”选项。<br>4. <strong>输入出发地和目的地</strong>： - 在机票预订页面，输入您的出发地和目的地。<br>5. <strong>选择出发日期和返回日期</strong>： - 选择您的出发日期和返回日期（如果是往返票）。<br>6. <strong>选择乘客信息</strong>： - 输入乘客人数和座位类型（成人、儿童、婴儿）。<br>7. <strong>选择航班</strong>： - 浏览并选择您喜欢的航班。<br>8. <strong>填写乘客信息</strong>： - 输入乘客的姓名、身份证号码、联系方式等信息。<br>9. <strong>选择支付方式</strong>： - 选择您喜欢的支付方式（如支付宝、微信支付、银行卡等）。<br>10. <strong>确认订单并支付</strong>： - 确认订单信息无误后，点击“支付”按钮完成支付。<br>11. <strong>接收电子机票</strong>： - 支付成功后，您将收到电子机票，可以通过App内的“我的订单”查看或下载。<br>通过以上步骤，您就可以使用携程App轻松购买机票了。</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/7.webp height=128px></figure>根据图表计算样本之间的 Δct 在哪一个引物下相差最小？ | 根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。（图片来源：互联网公开-舒桐科技公众号）</div><div class=role>Ming-Lite-Omni</div><div class=content>根据图表计算样本之间的 Δct 在 cGAS-qF+qR 引物下相差最小。</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>在 <strong>EGFP-cGAS单克隆</strong> 引物下，样本之间的 Δct 相差最小。<br><br><strong>计算过程：</strong><br><strong>1. 理解 Δct 的含义：</strong> Δct 是目标基因（cGAS-qF+qR 或 EGFP-cGAS）与内参基因（cTubulin）的相对表达量差值。Δct 越小，说明目标基因与内参基因的表达量差异越小。<br><strong>2. 比较不同引物下的 Δct：</strong><br>- <strong>cGAS-qF+qR 引物：</strong> - 野生型：Δct = 8.06 - EGFP-cGAS单克隆：Δct = -0.15 - 差异：8.06 - (-0.15) = 8.21<br>- <strong>EGFP-cGAS单克隆 引物：</strong> - 野生型：Δct = 10.44 - EGFP-cGAS单克隆：Δct = -2.9 - 差异：10.44 - (-2.9) = 13.34<br>- <strong>EGFP-cGAS-qF+qR 引物：</strong> - 野生型：Δct = 21.43 - EGFP-cGAS单克隆：Δct = -13.45 - 差异：21.43 - (-13.45) = 34.88<br><strong>3. 结论：</strong> 在 <strong>EGFP-cGAS单克隆</strong> 引物下，样本之间的 Δct 差异最小（Δct 差异为 13.34），说明该引物下样本之间的表达量差异最小。<br><br><strong>总结：</strong> 在 <strong>EGFP-cGAS单克隆</strong> 引物下，样本之间的 Δct 相差最小。</div></div></div></div><h3 id=语音合成>语音合成<a hidden class=anchor aria-hidden=true href=#语音合成>#</a></h3><table><thead><tr><th>输入文本</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Ming-lite-omni V1.5</th></tr></thead><tbody><tr><td>Tyler, Lucy, Michelle, we&rsquo;re going to space!</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en1_v1.5.wav type=audio/wav></audio></td></tr><tr><td>what is the forecast for California for rain</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en2_v1.5.wav type=audio/wav></audio></td></tr><tr><td>Television can make you dumb, but it can also be good education.</td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.wav type=audio/wav></audio></td><td style=text-align:center><audio controls><source src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/tts_en3_v1.5.wav type=audio/wav></audio></td></tr></tbody></table><div class="full-width-container example-container"><div class=example-content><div class=title><span>OCR文档理解</span>
<a class=next-button>Next</a></div><div class="grid-layout scroll"><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/9.webp height=320px></figure>在判断题第95题中，考生答题的手写内容是什么？若考生的手写答案为√则输出对，若考生的手写答案为×则输出错，请输出对或错</div><div class=role>Ming-Lite-Omni</div><div class=content>对</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>错</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/10.webp height=64px></figure>Output the content of the CAPTCHA directly. Characters are only numbers and uppercase/lowercase letters.</div><div class=role>Ming-Lite-Omni</div><div class=content>9743</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>9742</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/11.webp height=64px></figure>What is the Latex tag for mathematical expression in images?</div><div class=role>Ming-Lite-Omni</div><div class=content>A C = C B \therefore C D / / B E</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>A C = C B \because C D / / B E</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/12.webp height=320px></figure>从图中提取: 日期</div><div class=role>Ming-Lite-Omni</div><div class=content>日期：2019年12月03日05时08分 从图中提取: 里程 里程：23.9公里 从图中提取: 金额 金额：52.6元 从图中提取: 状态 状态： 从图中提取: 客票印务 客票印务2019年12月印900000卷 从图中提取: 号码 号码从00000001至90000000止</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>日期: 2021年03月05日</div><div class=role>User</div><div class=content><figure><img src=https://gcore.jsdelivr.net/gh/biao-gong/static@main/0715/13.webp height=320px></figure>What is the difference in value between Lamb and Corn?&rsquo; You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within [think] [/think] tags. The final answer MUST BE put in \boxed{}.'</div><div class=role>Ming-Lite-Omni</div><div class=content>0.37</div><div class=role>Ming-Lite-Omni V1.5</div><div class=content>[think]<br>The value for Lamb is 103.7 and for Corn is 103.13. To find the difference, subtract the smaller value from the larger value: 103.7 - 103.13 = 0.57.<br>[/think]<br>\boxed{0.57}</div></div></div></div><h3 id=视频理解>视频理解<a hidden class=anchor aria-hidden=true href=#视频理解>#</a></h3><table style="border-collapse:collapse;width:100%;margin:20px auto;border:none;font-size:14.4px"><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Users</td><td style=padding:10px;text-align:left;vertical-align:top;border:none><video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/tMqiS4N2yPAAAAAAgEAAAAgAeuUHAQFr height=256px controls></video><br>视频里的小女孩在做什么？</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>视频中的场景发生在一家商店里，可能是超市或大型零售店，货架上摆满了各种商品，包括零食和家庭用品。视频里的小女孩正在拉动一个展示架，看起来她在尝试移动或者玩耍这个展示架。价格标签清晰可见，其中一个显著地标记着蓝色和白色的'18,690'字样。店内照明良好，荧光灯照亮了整个区域。如需更详细的分析或其他帮助，请告诉我！</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni V1.5</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>小女孩坐在购物车里睡觉。</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Users</td><td style=padding:10px;text-align:left;vertical-align:top;border:none><video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/NGX_T5E13IAAAAAAgIAAAAgAfoeUAQBr height=192px controls></video><br>描述下视频中主人公的穿着。</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>视频中，主人公穿着红色上衣，肩上背着一个彩色图案的包。</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni V1.5</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>视频中的主人公穿着一件红色的高领毛衣。</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Users</td><td style=padding:10px;text-align:left;vertical-align:top;border:none><video src=https://gw.alipayobjects.com/v/huamei_aukff7/afts/video/KD6LTqp9VTsAAAAAgMAAAAgAfoeUAQBr height=192px controls></video><br>推测一下这个视频的拍摄时间和位置。</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>从视频中的画面来看，这应该是在纽约的中央公园拍摄的。画面中可以看到中央公园的标志性建筑和景观，比如中央公园的湖泊、桥梁和雕像。此外，视频中还有游客在公园内散步、拍照和享受户外活动，这进一步确认了拍摄地点为纽约的中央公园。</td></tr><tr><td style=padding:10px;text-align:right;vertical-align:top;color:#777;border:none;white-space:nowrap>Ming-Lite-Omni V1.5</td><td style=padding:10px;text-align:left;vertical-align:top;border:none>从视频中的场景来看，拍摄地点位于纽约中央公园。视频展示了公园内的多个景点，包括湖泊、桥梁、雕像和喷泉等。拍摄时间似乎是在白天，阳光明媚，天气晴朗，适合户外活动。</td></tr></table><br><h2 id=开始使用-ming-lite-omni-v15>开始使用 Ming-lite-omni v1.5<a hidden class=anchor aria-hidden=true href=#开始使用-ming-lite-omni-v15>#</a></h2><p>Ming-lite-omni v1.5的模型和代码已开源，诚邀大家试用、反馈和交流。值得期待的是，我们即将发布量化加速版本的Ming-lite-omni，该版本将不仅进一步优化全模态效果，还使Ming-lite-omni更加轻量化，同时强化多模推理和生成能力。欢迎持续关注！</p><ul><li>Github: <a href=https://github.com/inclusionAI/Ming>https://github.com/inclusionAI/Ming</a></li><li>Hugging Face: <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5>https://huggingface.co/inclusionAI/Ming-Lite-Omni-1.5</a></li><li>ModelScope: <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5>https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni-1.5</a></li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni V1.5 介绍 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 前言 本次发布的 Ming-Lite-Omni V1.5 是对初版 Ming-Lite-Omni 的一次全面升级。在保留既有优势的前提下，我们扩展了更多的多模态任务，并对所有任务的性能表现做了大幅度的优化，实现了诸如更强的复杂文档理解、基于 MRoPE 的时空感知位置编码、生成式图像分割、更棒的图像生成效果和ID保持能力等。所有这一切都来自 一个 多模态大模型，凸显了多模态统一的魅力。下面我们对各部分的能力做详细介绍，感谢大家的关注，欢迎广泛尝试并沟通交流！
复杂文档理解 我们在多种文字识别、图表分析和文档理解基准上对Ming-Omni-Lite模型进行了系统评估。如下表所示：
<!DOCTYPE html> 任务类型 评测基准 Qwen2.5-VL-7B InternVL3-8B Ming-Omni-Lite OCR理解 ChartQA_test 87.24 86.60 88.84 DocVQA_test 95.57 92.70 93.68 TextVQA_val 85.06 80.20 82.27 OCRBench 87.8 88.00 88.90 average 88.91 86.87 88.42 OCR解析 OmniDocBench↓ en/zh 30.8/39.8 -- 34.9/34.9 OCR综合能力 OCRBenchV2 en/zh 56.3/57.2 -- 52.1/55.2 在OCR理解任务上，Ming-Omni-Lite平均得分88.42，其综合表现与当前领先的70亿参数多模态语言模型Qwen2.5VL-7B-Instruct（88.91）处于同一水平。特别是在具有挑战性的OCRBench基准（聚焦文本视觉理解任务）以及需深度图表视觉分析与逻辑推理的ChartQA任务中，Ming-Omni-Lite分别取得突破性进展，相较Qwen2.5-VL-7B等主流竞品模型实现显著性能跃升。这一优势源于其创新的训练策略——通过引入&#34;思维链&#34;（Chain-of-Thought）范式，使模型能够分步骤构建结构化推理路径，从而有效提升复杂问题的解决能力。 在面向多场景应用且覆盖手写体、表格、图表及数学公式等复杂版式文档的多模态OCR解析基准OmniDocBench中，Ming-Omni-Lite通过多源异构训练数据集的构建，在中英文双语环境下均展现出卓越的性能表现（英文34.9/中文34.9）。
图文及体验 图文 新版本Ming-lite-omni 1.5融合了3D位置编码，大幅提升对图像结构与动态信息的感知能力，配合更优的训练方法及全面升级的高质量数据，v1.5在通用图片理解、图像目标识别、垂类场景识别等多个关键任务上实现显著性能跃升，为广泛视觉应用提供更强大的基础能力。下面是本次版本迭代的改进清单及评测结果：
MRoPE 时空感知位置编码：引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略：优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，缩短周期/降低消耗，保持性能无损。 全方位数据升级： ○ 预训练阶段： 扩展知识广度与数据质量：新增文本实体结构化数据补全图谱盲区，生成任务引入原始描述约束抑制幻觉，扩充高质量垂类语料深化领域理解。 ○ 指令微调阶段： 强化核心任务与高阶理解：提升细粒度视觉感知（目标计数/颜色/场景识别）精度，深化垂类识别（动植物/车辆/食材等）深度，优化跨学科复杂图文推理能力。 任务类型 评测基准 Qwen2."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><link crossorigin=anonymous href=/assets/css/stylesheet.c89f196d8672b69cf34afeba27d213af1d6900c106ff9315c4a62af441df5c8a.css integrity="sha256-yJ8ZbYZytpzzSv66J9ITrx1pAMEG/5MVxKYq9EHfXIo=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Lite-Omni V1.5 介绍"><meta property="og:description" content="GITHUB 前言 本次发布的 Ming-Lite-Omni V1.5 是对初版 Ming-Lite-Omni 的一次全面升级。在保留既有优势的前提下，我们扩展了更多的多模态任务，并对所有任务的性能表现做了大幅度的优化，实现了诸如更强的复杂文档理解、基于 MRoPE 的时空感知位置编码、生成式图像分割、更棒的图像生成效果和ID保持能力等。所有这一切都来自 一个 多模态大模型，凸显了多模态统一的魅力。下面我们对各部分的能力做详细介绍，感谢大家的关注，欢迎广泛尝试并沟通交流！
复杂文档理解 我们在多种文字识别、图表分析和文档理解基准上对Ming-Omni-Lite模型进行了系统评估。如下表所示：
<!DOCTYPE html> 任务类型 评测基准 Qwen2.5-VL-7B InternVL3-8B Ming-Omni-Lite OCR理解 ChartQA_test 87.24 86.60 88.84 DocVQA_test 95.57 92.70 93.68 TextVQA_val 85.06 80.20 82.27 OCRBench 87.8 88.00 88.90 average 88.91 86.87 88.42 OCR解析 OmniDocBench↓ en/zh 30.8/39.8 -- 34.9/34.9 OCR综合能力 OCRBenchV2 en/zh 56.3/57.2 -- 52.1/55.2 在OCR理解任务上，Ming-Omni-Lite平均得分88.42，其综合表现与当前领先的70亿参数多模态语言模型Qwen2.5VL-7B-Instruct（88.91）处于同一水平。特别是在具有挑战性的OCRBench基准（聚焦文本视觉理解任务）以及需深度图表视觉分析与逻辑推理的ChartQA任务中，Ming-Omni-Lite分别取得突破性进展，相较Qwen2.5-VL-7B等主流竞品模型实现显著性能跃升。这一优势源于其创新的训练策略——通过引入&#34;思维链&#34;（Chain-of-Thought）范式，使模型能够分步骤构建结构化推理路径，从而有效提升复杂问题的解决能力。 在面向多场景应用且覆盖手写体、表格、图表及数学公式等复杂版式文档的多模态OCR解析基准OmniDocBench中，Ming-Omni-Lite通过多源异构训练数据集的构建，在中英文双语环境下均展现出卓越的性能表现（英文34.9/中文34.9）。
图文及体验 图文 新版本Ming-lite-omni 1.5融合了3D位置编码，大幅提升对图像结构与动态信息的感知能力，配合更优的训练方法及全面升级的高质量数据，v1.5在通用图片理解、图像目标识别、垂类场景识别等多个关键任务上实现显著性能跃升，为广泛视觉应用提供更强大的基础能力。下面是本次版本迭代的改进清单及评测结果：
MRoPE 时空感知位置编码：引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略：优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，缩短周期/降低消耗，保持性能无损。 全方位数据升级： ○ 预训练阶段： 扩展知识广度与数据质量：新增文本实体结构化数据补全图谱盲区，生成任务引入原始描述约束抑制幻觉，扩充高质量垂类语料深化领域理解。 ○ 指令微调阶段： 强化核心任务与高阶理解：提升细粒度视觉感知（目标计数/颜色/场景识别）精度，深化垂类识别（动植物/车辆/食材等）深度，优化跨学科复杂图文推理能力。 任务类型 评测基准 Qwen2."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-07-15T00:00:03+08:00"><meta property="article:modified_time" content="2025-07-15T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni V1.5 介绍"><meta name=twitter:description content="GITHUB 前言 本次发布的 Ming-Lite-Omni V1.5 是对初版 Ming-Lite-Omni 的一次全面升级。在保留既有优势的前提下，我们扩展了更多的多模态任务，并对所有任务的性能表现做了大幅度的优化，实现了诸如更强的复杂文档理解、基于 MRoPE 的时空感知位置编码、生成式图像分割、更棒的图像生成效果和ID保持能力等。所有这一切都来自 一个 多模态大模型，凸显了多模态统一的魅力。下面我们对各部分的能力做详细介绍，感谢大家的关注，欢迎广泛尝试并沟通交流！
复杂文档理解 我们在多种文字识别、图表分析和文档理解基准上对Ming-Omni-Lite模型进行了系统评估。如下表所示：
<!DOCTYPE html> 任务类型 评测基准 Qwen2.5-VL-7B InternVL3-8B Ming-Omni-Lite OCR理解 ChartQA_test 87.24 86.60 88.84 DocVQA_test 95.57 92.70 93.68 TextVQA_val 85.06 80.20 82.27 OCRBench 87.8 88.00 88.90 average 88.91 86.87 88.42 OCR解析 OmniDocBench↓ en/zh 30.8/39.8 -- 34.9/34.9 OCR综合能力 OCRBenchV2 en/zh 56.3/57.2 -- 52.1/55.2 在OCR理解任务上，Ming-Omni-Lite平均得分88.42，其综合表现与当前领先的70亿参数多模态语言模型Qwen2.5VL-7B-Instruct（88.91）处于同一水平。特别是在具有挑战性的OCRBench基准（聚焦文本视觉理解任务）以及需深度图表视觉分析与逻辑推理的ChartQA任务中，Ming-Omni-Lite分别取得突破性进展，相较Qwen2.5-VL-7B等主流竞品模型实现显著性能跃升。这一优势源于其创新的训练策略——通过引入&#34;思维链&#34;（Chain-of-Thought）范式，使模型能够分步骤构建结构化推理路径，从而有效提升复杂问题的解决能力。 在面向多场景应用且覆盖手写体、表格、图表及数学公式等复杂版式文档的多模态OCR解析基准OmniDocBench中，Ming-Omni-Lite通过多源异构训练数据集的构建，在中英文双语环境下均展现出卓越的性能表现（英文34.9/中文34.9）。
图文及体验 图文 新版本Ming-lite-omni 1.5融合了3D位置编码，大幅提升对图像结构与动态信息的感知能力，配合更优的训练方法及全面升级的高质量数据，v1.5在通用图片理解、图像目标识别、垂类场景识别等多个关键任务上实现显著性能跃升，为广泛视觉应用提供更强大的基础能力。下面是本次版本迭代的改进清单及评测结果：
MRoPE 时空感知位置编码：引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略：优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，缩短周期/降低消耗，保持性能无损。 全方位数据升级： ○ 预训练阶段： 扩展知识广度与数据质量：新增文本实体结构化数据补全图谱盲区，生成任务引入原始描述约束抑制幻觉，扩充高质量垂类语料深化领域理解。 ○ 指令微调阶段： 强化核心任务与高阶理解：提升细粒度视觉感知（目标计数/颜色/场景识别）精度，深化垂类识别（动植物/车辆/食材等）深度，优化跨学科复杂图文推理能力。 任务类型 评测基准 Qwen2."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni V1.5 介绍","item":"https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni V1.5 介绍","name":"Ming-Lite-Omni V1.5 介绍","description":"GITHUB 前言 本次发布的 Ming-Lite-Omni V1.5 是对初版 Ming-Lite-Omni 的一次全面升级。在保留既有优势的前提下，我们扩展了更多的多模态任务，并对所有任务的性能表现做了大幅度的优化，实现了诸如更强的复杂文档理解、基于 MRoPE 的时空感知位置编码、生成式图像分割、更棒的图像生成效果和ID保持能力等。所有这一切都来自 一个 多模态大模型，凸显了多模态统一的魅力。下面我们对各部分的能力做详细介绍，感谢大家的关注，欢迎广泛尝试并沟通交流！ 复杂文档理解 我们在多种文字识别、图表分析和文档理解基准上对Ming-Omni-Lite模型进行了系统评估。如下表所示： \u003c!DOCTYPE html\u003e 任务类型 评测基准 Qwen2.5-VL-7B InternVL3-8B Ming-Omni-Lite OCR理解 ChartQA_test 87.24 86.60 88.84 DocVQA_test 95.57 92.70 93.68 TextVQA_val 85.06 80.20 82.27 OCRBench 87.8 88.00 88.90 average 88.91 86.87 88.42 OCR解析 OmniDocBench↓ en/zh 30.8/39.8 -- 34.9/34.9 OCR综合能力 OCRBenchV2 en/zh 56.3/57.2 -- 52.1/55.2 在OCR理解任务上，Ming-Omni-Lite平均得分88.42，其综合表现与当前领先的70亿参数多模态语言模型Qwen2.5VL-7B-Instruct（88.91）处于同一水平。特别是在具有挑战性的OCRBench基准（聚焦文本视觉理解任务）以及需深度图表视觉分析与逻辑推理的ChartQA任务中，Ming-Omni-Lite分别取得突破性进展，相较Qwen2.5-VL-7B等主流竞品模型实现显著性能跃升。这一优势源于其创新的训练策略——通过引入\u0026quot;思维链\u0026quot;（Chain-of-Thought）范式，使模型能够分步骤构建结构化推理路径，从而有效提升复杂问题的解决能力。 在面向多场景应用且覆盖手写体、表格、图表及数学公式等复杂版式文档的多模态OCR解析基准OmniDocBench中，Ming-Omni-Lite通过多源异构训练数据集的构建，在中英文双语环境下均展现出卓越的性能表现（英文34.9/中文34.9）。 图文及体验 图文 新版本Ming-lite-omni 1.5融合了3D位置编码，大幅提升对图像结构与动态信息的感知能力，配合更优的训练方法及全面升级的高质量数据，v1.5在通用图片理解、图像目标识别、垂类场景识别等多个关键任务上实现显著性能跃升，为广泛视觉应用提供更强大的基础能力。下面是本次版本迭代的改进清单及评测结果： MRoPE 时空感知位置编码：引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略：优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，缩短周期/降低消耗，保持性能无损。 全方位数据升级： ○ 预训练阶段： 扩展知识广度与数据质量：新增文本实体结构化数据补全图谱盲区，生成任务引入原始描述约束抑制幻觉，扩充高质量垂类语料深化领域理解。 ○ 指令微调阶段： 强化核心任务与高阶理解：提升细粒度视觉感知（目标计数/颜色/场景识别）精度，深化垂类识别（动植物/车辆/食材等）深度，优化跨学科复杂图文推理能力。 任务类型 评测基准 Qwen2.","keywords":[],"articleBody":"GITHUB 前言 本次发布的 Ming-Lite-Omni V1.5 是对初版 Ming-Lite-Omni 的一次全面升级。在保留既有优势的前提下，我们扩展了更多的多模态任务，并对所有任务的性能表现做了大幅度的优化，实现了诸如更强的复杂文档理解、基于 MRoPE 的时空感知位置编码、生成式图像分割、更棒的图像生成效果和ID保持能力等。所有这一切都来自 一个 多模态大模型，凸显了多模态统一的魅力。下面我们对各部分的能力做详细介绍，感谢大家的关注，欢迎广泛尝试并沟通交流！\n复杂文档理解 我们在多种文字识别、图表分析和文档理解基准上对Ming-Omni-Lite模型进行了系统评估。如下表所示：\n\u003c!DOCTYPE html\u003e 任务类型 评测基准 Qwen2.5-VL-7B InternVL3-8B Ming-Omni-Lite OCR理解 ChartQA_test 87.24 86.60 88.84 DocVQA_test 95.57 92.70 93.68 TextVQA_val 85.06 80.20 82.27 OCRBench 87.8 88.00 88.90 average 88.91 86.87 88.42 OCR解析 OmniDocBench↓ en/zh 30.8/39.8 -- 34.9/34.9 OCR综合能力 OCRBenchV2 en/zh 56.3/57.2 -- 52.1/55.2 在OCR理解任务上，Ming-Omni-Lite平均得分88.42，其综合表现与当前领先的70亿参数多模态语言模型Qwen2.5VL-7B-Instruct（88.91）处于同一水平。特别是在具有挑战性的OCRBench基准（聚焦文本视觉理解任务）以及需深度图表视觉分析与逻辑推理的ChartQA任务中，Ming-Omni-Lite分别取得突破性进展，相较Qwen2.5-VL-7B等主流竞品模型实现显著性能跃升。这一优势源于其创新的训练策略——通过引入\"思维链\"（Chain-of-Thought）范式，使模型能够分步骤构建结构化推理路径，从而有效提升复杂问题的解决能力。 在面向多场景应用且覆盖手写体、表格、图表及数学公式等复杂版式文档的多模态OCR解析基准OmniDocBench中，Ming-Omni-Lite通过多源异构训练数据集的构建，在中英文双语环境下均展现出卓越的性能表现（英文34.9/中文34.9）。\n图文及体验 图文 新版本Ming-lite-omni 1.5融合了3D位置编码，大幅提升对图像结构与动态信息的感知能力，配合更优的训练方法及全面升级的高质量数据，v1.5在通用图片理解、图像目标识别、垂类场景识别等多个关键任务上实现显著性能跃升，为广泛视觉应用提供更强大的基础能力。下面是本次版本迭代的改进清单及评测结果：\nMRoPE 时空感知位置编码：引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略：优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，缩短周期/降低消耗，保持性能无损。 全方位数据升级： ○ 预训练阶段： 扩展知识广度与数据质量：新增文本实体结构化数据补全图谱盲区，生成任务引入原始描述约束抑制幻觉，扩充高质量垂类语料深化领域理解。 ○ 指令微调阶段： 强化核心任务与高阶理解：提升细粒度视觉感知（目标计数/颜色/场景识别）精度，深化垂类识别（动植物/车辆/食材等）深度，优化跨学科复杂图文推理能力。 任务类型 评测基准 Qwen2.5-VL-7B Ming-Omni-Lite 通用图文 AI2D 84.36 84.91 HallusionBench 55.77 54.59 MMBench_TEST_V11 82.75 80.73 MMMU 56.56 54.33 MMStar 65.27 65.07 MMVet 71.61 73.99 MathVista 68.10 72.00 OCRBench 87.80 88.90 目标检测 RefCOCO_val 90.00 91.40 RefCOCO+_val 84.20 86.30 自建评测集 通识 92.42 92.53 垂类 47.79 54.27 体验 得益于高质量的对齐偏好数据构建以及精细的DPO对齐训练超参数搜索/数据采样策略配置，与同等推理参数规模的开源模型相比，我们的模型在内部标注构建的人类体验偏好数据榜单中达到SOTA、均分超过Qwen2.5VL-7B-Instruct +0.09/5pt。我们的模型在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面有较大优势，即模型在问答响应过程中会给用户带来更佳的综合体验。\n评测基准 评测维度 Qwen2.5-VL-7B Ming-Omni-Lite 体验 均分 4.274 4.365 自建体验评测集_相关性 4.308 4.5 自建体验评测集_流畅性 4.765 4.91 自建体验评测集_内容丰富性 3.828 3.69 自建体验评测集_格式合理性 4.727 4.8 自建体验评测集_正确性 3.741 3.92 定义视频理解新标杆 在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 Ming-Omni-Lite 在多项核心视频理解基准测试中取得了突破性进展。\n性能 我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：\n评测基准 Qwen2.5-VL-7B Qwen2.5-Omni-7B InternVL3-8B Ming-Omni-Lite VideoMME(w/o subs) 65.10 64.30 66.30 67.07 VideoMME(w/ subs) 71.60 72.40 68.90 72.59 VideoMME(avg) 68.35 68.35 67.60 69.83 MVBench 69.60 70.30 75.40 69.43 LongVideoBench 56.00 54.82 58.80 59.54 OvOBench 51.10 50.46 51.91 52.17 技术背后 Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：\n高效的时空建模器： 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。 高质量、多样化的视频-文本对齐数据： 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。 创新的训练目标与课程学习： 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。 迈向更智能的视频交互 Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理复杂、长时间、信息密集视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。\n语音理解和生成 在语音理解方面，我们在训练中引入了丰富且多样的语音数据，并且将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现。我们的模型在语音理解方面支持中英文，粤语，四川话，上海话，闽南语等方言，在开源的中英文数据集上达到 SOTA 水平。\nModel aishell1 aishell2_android aishell2_ios cv15_zh fleurs_zh Ming-lite-omni 1.31 2.45 2.45 5.69 2.87 Qwen2.-Omni 1.18 2.75 2.63 5.20 3.00 Qwen2-Audio 1.53 2.92 2.92 6.90 7.50 Kimi-Audio 0.60 2.64 2.56 7.21 2.69 Model ws_meet ws_net lsc_test_cl lsc_test_ot multi_ls cv15_en flu_en vox_en Ming-lite-omni 6.18 5.22 1.24 2.61 4.13 6.95 3.28 6.82 Qwen2-Omni 5.90 7.70 1.80 3.40 7.56 7.60 4.10 5.80 Qwen2-Audio 7.16 8.42 1.60 3.60 5.40 8.60 6.90 6.84 Kimi-Audio 6.28 5.37 1.28 2.42 5.88 10.31 4.44 7.97 输入音频 方言 识别结果 粤语 你在干什么, 是不是不想聊天 上海话 我们考试还没定下来呢 闽南语 宝贝, 早点休息, 晚安 四川话 我难受的很, 别人都睡了 得益于模型卓越的语音理解能力，我们的模型在语音对话评估集上也取得优异的效果。\nModel AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-lite-omni 4.65 4.30 61.16 45.77 65.93 55.60 98.08 输入音频 输出音频 语音生成方面，Ming-lite-omni 集成了先进的音频解码器，该解码器接受来自LLM的输出隐藏状态，这使得模型能够处理上下文感知的多模态对话和标准的文本转语音（TTS）转换，从而生成自然流畅的语音 。为了提高韵律性能和实时生成能力，将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。\nModel seed-tts-eval-zh_wer seed-tts-eval-zh_sim seed-tts-eval-en_wer seed-tts-eval-en_sim Seed-TTS 1.11 0.796 2.24 0.762 MaskGCT 2.27 0.774 2.62 0.714 E2 TTS 1.97 0.730 2.19 0.710 F5-TTS 1.56 0.741 1.83 0.647 CosyVoice 2 1.45 0.748 2.57 0.652 Qwen2.5-Omni-7B 1.70 0.752 2.72 0.632 Ming-Lite-Omni 2.00 0.686 4.299 0.513 参考音频 输入文本 输出音频 我们的愿景是构建未来服务业的数字化基础设施，给世界带来更多微小而美好的改变。 The stained glass offered a hypnotic atmosphere 突破图像生成新极限 相较于早先发布的 Ming-Lite-Uni 以及 Ming-Omni，在此版本中，我们进一步提升了Ming对生成图像的 场景一致性（Scene Consistency）、ID 一致性（Character / Style Consistency）、以及 多感知扩展（Segmentation, Keypoints, Depth, …），让Ming从一个具备图像生成和图像编辑能力的多模态大模型（MLLM），变成一个能够处理更多图像生成任务以及效果更好的完整MLLM。下面是我们最近一段时间的工作进展的报告，欢迎大家交流讨论。\nGenEval指标 Overall score Single Ojbect Two Objects Counting Color Position Color Attr Ours 0.86 100.00% 96.72% 76.56% 89.89% 89.75% 68.69% 模型结构回顾及改进 子模块 对应技术点 作用 关键点 跨模态桥接方式 Channel concat / Token concat / Blend - Channel concat：参数少、显存低，但语义对齐弱 - Token concat：保语义结构，适合大分辨率 - Blend：编辑/重绘场景更鲁棒 根据任务需求选择桥接方式。目前采用Token Concat方案 双分支表示解耦 解耦参考图图像patch编码和refiner参数 - 提升参考图的独立控制参数容量，起到部分解耦的作用 双 patchfy 模块与双分支额外 refiner，提升了模型编辑与分割性能 双分支解耦指在将图像送入 DiT 的 transformer 之前，使用不同的网络权重将参考图像与噪声图像进行 patchfy，这样能够有效降低参考图像信息对于编辑时语义遵循的影响，refiner 是在 patchfy 之后额外的两层轻量级 transformer，能够进一步增强这一效果，在推理分割上的性能评估表明了新增模块的有效性 推理分割考验模型对于语义的正确理解，需要模型根据复杂的指令确定要分割的目标 实验结果如下方表格所示，可以看到解耦的patchfy显著增强了推理分割的指标，增加 refiner 模块后能够进一步提升性能 GEdit 子集：[“background_change”, “color_alter”, “material_alter”, “motion_change”] Mode ID double-patchfy add-refiner refcoc 分割指标 GEdit(subset-full) 0 ❌ ❌ 62.8 6.129 1 ✅ ❌ 64.2 6.391 2 ✅ ✅ 64.5 6.306 条件控制与引导策略 子模块 对应技术点 作用 关键点 多条件的CFG控制策略 语义CFG vs 图像CFG（Ref-Guided） 多条件的 Classifier-free Guidance 策略：语义二分差分+图像三分差分提升ID一致性 当纯语义控制时，编辑后的图像遵循了指令，但完全丧失与原图的一致性；图像分支指导强度较大时编辑结果几乎与原图一致 ID \u0026 Scene Consistency Loss Weight mask Loss + Scene Consistency Loss 增大目标图编辑区域的权重，同时增加参考图非编辑区域的强约束和编辑区域的弱约束 调整λ平衡身份保持下的编辑效果与场景一致性，避免过拟合 与 Qwen-VLo 对比 prompt ours Qwen-VLo Make the person in the image smile slightly without altering the original structure 感知能力扩展 生成式分割\n相比于生成式图像编辑任务，分割任务的预测mask和原图之间存在较少的细节一致性，因此在token concat方案下难以较快的学习到原图和分割mask 之间的一致性关系。因此，我们将图像的分割目标建模成彩色分割图像，即mask和图像的融合形式，从而使得预测目标和原图之间存在较多一致的细节，因此能够更好的学习到分割图和原图之间的一致关系。在推理时，将预测图像和原图做diff并进行噪声过滤获取最终的预测mask。\n输入图像 推理分割 语义分割 全景分割 prompt: Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image. prompt: Please segment different classes in this image prompt: Please segment different instances in this image. 边缘轮廓图生成 原图 深度图 检测框 边缘轮廓 数据优化 文档解析语料：为了提升基模在深度文本视觉分析与逻辑推理方面的性能，我们在训练过程中引入\"思维链\"（Chain-of-Thought）范式。具体实施策略如下：\n首先，在训练体系中集成开源数据集ChartQA-PoT（Program-of-Thought），重点提升模型的图表数值计算能力。该方法继承了思维链的递进式推理理念，但突破性地采用可执行Python代码作为中间推理载体。其次，针对传统文本相关问答数据集中普遍存在的\"答案直推\"模式局限，我们创新性利用强化学习模型对原始训练样本进行多步推理轨迹及最终答案生成；筛选通过形式化验证的\"推理步骤-最终答案\"对构建新型训练集。该方法显著提升了模型对复杂逻辑任务的适应性。\n人类偏好语料：多模态大模型的用户真实交互体验优化通过利用人类体验偏好数据对齐训练来提升性能。我们的体验偏好指令集主要由三个来源构成：应用端的真实用户对话数据、高质量开源指令数据集采样以及网站的搜索查询数据。对于网站的搜索查询数据，我们首先用纯文本查询通过搜索引擎检索相关的网络图像；随后，我们利用现有的MLLM改写生成多样的专业问题及其对应的答案。我们对上述的多种用户指令数据源生成出高质量的正样本以及负样本数据偏好对，并结合MLLM和专业人工标注员对这些问答对的质量进行标注校验，构建了覆盖9个主要领域的41个子类别人类体验偏好语料库。\n视频理解语料：高质量的视频数据提供了更为丰富且精准的语义信息，能够有效提升模型对复杂视频场景的理解深度与广度。我们构建的视频语料库主要来源于两个方面：首先，我们深度挖掘并整合了现有开源视觉基础任务数据集，如采用目标跟踪数据集GOT-10K增强模型的物体追踪能力，利用时序检索数据集DiDemo提升模型的事件感知能力；其次，我们组织专业标注团队精心构建了一批高难度的视频问答样本对，用于训练模型的复杂推理能力。\n百科语料：百科数据扩充了多模态大模型的可识别实体范围，为大模型注入专家级的细粒度实体识别能力。在这个版本中，我们共包含三大类共十小类的实体集合，分别为自然百科实体集（植物，动物）；人文百科实体集（名人，动漫角色，地标，LOGO，和艺术品）；和日常生活实体集（食材，菜品，汽车）。为了构建一个高质量的专家级细粒度百科实体数据集，我们首先从专业的学术网站或垂类站点上大规模的收集了一批实体集合，之后我们使用这些实体集合组成检索词，在公开的搜索引擎上获取这些实体数据。接着，我们设计了一个高效的逐级递进的实体数据清洗链路，包含CLIP图文相似度过滤，大模型是否可用二分判断过滤，和人工纠正过滤的流程。\nGUI语料：GUI（图形用户界面）数据使模型在Android环境下具有基本的GUI导航能力。我们的GUI语料库主要由四个公共数据集构建：AITW， guiccourse， AndroidControl和AMEX。此外，我们利用可用的MLLM来优化类人GUI交互操作中每个步骤的推理过程，随后由另一个MLLM审查以提高数据质量。思维过程约束模型观察当前状态，反思之前的动作，在操作之前仔细考虑。为了更好地感知当前情况，我们加入了历史记忆，由前序的动作组成。\n图像生产与编辑语料：我们的图像生成语料库主要来自两个来源：一是从公开的图像生成的高质量图像数据集（例如，text-to-image-2M、JourneyDB、BLip3o、Uniworld dataset、InstructPix2Pix-clip-filtered、SEED-Data-Edit-part2/3、Ultraedit等）；二是从 StyleBooth 和 WikiArt 中采样的图像风格迁移数据。另外我们构造了生成图像数据的管线, 生成了部分文生图和编辑数据。\n感知推理语料：感知推理数据能够提升模型的综合理解能力和细粒度感知能力。在这个版本，我们针对目标计数、颜色识别、场景主题识别等感知类细分任务，从Object365、RefCOCO等开源数据集中，根据检测框数量、位置关系和物体类别数量等信息，筛选出一批潜在难样本。随后，利用VLM进行指定任务的问答合成，并通过多模型打分和人工过滤纠正的方式获得一批高质量样本。此外，为提升模型在感知任务上的推理能力，我们将上述合成数据改写为CoT形式，通过生成中间推理步骤，帮助模型分解复杂问题，提升综合感知能力。\n","wordCount":"644","inLanguage":"zh","datePublished":"2025-07-15T00:00:03+08:00","dateModified":"2025-07-15T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Omni V1.5 介绍</h1><div class=post-meta><span title='2025-07-15 00:00:03 +0800 +0800'>2025年7月15日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;644 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ming-lite-omni-1_5/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify class="btn external" target=_blank>GITHUB</a><h1 id=前言>前言<a hidden class=anchor aria-hidden=true href=#前言>#</a></h1><p>本次发布的 Ming-Lite-Omni V1.5 是对初版 Ming-Lite-Omni 的一次全面升级。在保留既有优势的前提下，我们扩展了更多的多模态任务，并对所有任务的性能表现做了大幅度的优化，实现了诸如更强的复杂文档理解、基于 MRoPE 的时空感知位置编码、生成式图像分割、更棒的图像生成效果和ID保持能力等。所有这一切都来自 <strong>一个</strong> 多模态大模型，凸显了多模态统一的魅力。下面我们对各部分的能力做详细介绍，感谢大家的关注，欢迎广泛尝试并沟通交流！</p><hr><h1 id=复杂文档理解>复杂文档理解<a hidden class=anchor aria-hidden=true href=#复杂文档理解>#</a></h1><p>我们在多种文字识别、图表分析和文档理解基准上对Ming-Omni-Lite模型进行了系统评估。如下表所示：</p><!doctype html><html lang=zh-cn><table class=optimized-table><thead><tr><th>任务类型</th><th>评测基准</th><th>Qwen2.5-VL-7B</th><th>InternVL3-8B</th><th>Ming-Omni-Lite</th></tr></thead><tbody><tr><td rowspan=5 class=merged-cell>OCR理解</td><td>ChartQA_test</td><td>87.24</td><td>86.60</td><td>88.84</td></tr><tr><td>DocVQA_test</td><td>95.57</td><td>92.70</td><td>93.68</td></tr><tr><td>TextVQA_val</td><td>85.06</td><td>80.20</td><td>82.27</td></tr><tr><td>OCRBench</td><td>87.8</td><td>88.00</td><td>88.90</td></tr><tr><td>average</td><td>88.91</td><td>86.87</td><td>88.42</td></tr><tr><td>OCR解析</td><td>OmniDocBench↓ en/zh</td><td>30.8/39.8</td><td>--</td><td>34.9/34.9</td></tr><tr><td>OCR综合能力</td><td>OCRBenchV2 en/zh</td><td>56.3/57.2</td><td>--</td><td>52.1/55.2</td></tr></tbody></table></body></html><p>在OCR理解任务上，Ming-Omni-Lite平均得分88.42，其综合表现与当前领先的70亿参数多模态语言模型Qwen2.5VL-7B-Instruct（88.91）处于同一水平。特别是在具有挑战性的OCRBench基准（聚焦文本视觉理解任务）以及需深度图表视觉分析与逻辑推理的ChartQA任务中，Ming-Omni-Lite分别取得突破性进展，相较Qwen2.5-VL-7B等主流竞品模型实现显著性能跃升。这一优势源于其创新的训练策略——通过引入"思维链"（Chain-of-Thought）范式，使模型能够分步骤构建结构化推理路径，从而有效提升复杂问题的解决能力。
在面向多场景应用且覆盖手写体、表格、图表及数学公式等复杂版式文档的多模态OCR解析基准OmniDocBench中，Ming-Omni-Lite通过多源异构训练数据集的构建，在中英文双语环境下均展现出卓越的性能表现（英文34.9/中文34.9）。</p><hr><h2 id=图文及体验>图文及体验<a hidden class=anchor aria-hidden=true href=#图文及体验>#</a></h2><h3 id=图文><strong>图文</strong><a hidden class=anchor aria-hidden=true href=#图文>#</a></h3><p>新版本Ming-lite-omni 1.5融合了3D位置编码，大幅提升对图像结构与动态信息的感知能力，配合更优的训练方法及全面升级的高质量数据，v1.5在通用图片理解、图像目标识别、垂类场景识别等多个关键任务上实现显著性能跃升，为广泛视觉应用提供更强大的基础能力。下面是本次版本迭代的改进清单及评测结果：</p><ol><li>MRoPE 时空感知位置编码：引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。</li><li>高效全参数训练策略：优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，缩短周期/降低消耗，保持性能无损。</li><li>全方位数据升级：
○ 预训练阶段： 扩展知识广度与数据质量：新增文本实体结构化数据补全图谱盲区，生成任务引入原始描述约束抑制幻觉，扩充高质量垂类语料深化领域理解。
○ 指令微调阶段： 强化核心任务与高阶理解：提升细粒度视觉感知（目标计数/颜色/场景识别）精度，深化垂类识别（动植物/车辆/食材等）深度，优化跨学科复杂图文推理能力。</li></ol><table><thead><tr><th>任务类型</th><th>评测基准</th><th>Qwen2.5-VL-7B</th><th>Ming-Omni-Lite</th></tr></thead><tbody><tr><td><strong>通用图文</strong></td><td>AI2D</td><td>84.36</td><td>84.91</td></tr><tr><td></td><td>HallusionBench</td><td>55.77</td><td>54.59</td></tr><tr><td></td><td>MMBench_TEST_V11</td><td>82.75</td><td>80.73</td></tr><tr><td></td><td>MMMU</td><td>56.56</td><td>54.33</td></tr><tr><td></td><td>MMStar</td><td>65.27</td><td>65.07</td></tr><tr><td></td><td>MMVet</td><td>71.61</td><td>73.99</td></tr><tr><td></td><td>MathVista</td><td>68.10</td><td>72.00</td></tr><tr><td></td><td>OCRBench</td><td>87.80</td><td>88.90</td></tr><tr><td><strong>目标检测</strong></td><td>RefCOCO_val</td><td>90.00</td><td>91.40</td></tr><tr><td></td><td>RefCOCO+_val</td><td>84.20</td><td>86.30</td></tr><tr><td><strong>自建评测集</strong></td><td>通识</td><td>92.42</td><td>92.53</td></tr><tr><td></td><td>垂类</td><td>47.79</td><td>54.27</td></tr></tbody></table><h3 id=体验><strong>体验</strong><a hidden class=anchor aria-hidden=true href=#体验>#</a></h3><p>得益于高质量的对齐偏好数据构建以及精细的DPO对齐训练超参数搜索/数据采样策略配置，与同等推理参数规模的开源模型相比，我们的模型在内部标注构建的人类体验偏好数据榜单中达到SOTA、均分超过Qwen2.5VL-7B-Instruct +0.09/5pt。我们的模型在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面有较大优势，即模型在问答响应过程中会给用户带来更佳的综合体验。</p><table><thead><tr><th>评测基准</th><th>评测维度</th><th>Qwen2.5-VL-7B</th><th>Ming-Omni-Lite</th></tr></thead><tbody><tr><td>体验</td><td>均分</td><td>4.274</td><td>4.365</td></tr><tr><td></td><td>自建体验评测集_相关性</td><td>4.308</td><td>4.5</td></tr><tr><td></td><td>自建体验评测集_流畅性</td><td>4.765</td><td>4.91</td></tr><tr><td></td><td>自建体验评测集_内容丰富性</td><td>3.828</td><td>3.69</td></tr><tr><td></td><td>自建体验评测集_格式合理性</td><td>4.727</td><td>4.8</td></tr><tr><td></td><td>自建体验评测集_正确性</td><td>3.741</td><td>3.92</td></tr></tbody></table><hr><h2 id=定义视频理解新标杆>定义视频理解新标杆<a hidden class=anchor aria-hidden=true href=#定义视频理解新标杆>#</a></h2><p>在追求通用人工智能（AGI）的道路上，多模态大语言模型（MLLM）对视频内容的理解能力至关重要。现实世界的信息是动态、连续的，视频承载着远超静态图像的丰富时空语义。 <strong>Ming-Omni-Lite</strong> 在多项核心视频理解基准测试中取得了突破性进展。</p><h3 id=性能>性能<a hidden class=anchor aria-hidden=true href=#性能>#</a></h3><p>我们选取了当前最具代表性和挑战性的视频理解基准，将 Ming-Omni-Lite 与业界顶尖的同体量模型（Qwen2.5-VL-7B, Qwen2.5-Omni-7B, InternVL3-8B）进行了全面对比。结果评测结果展示了 Ming-Omni-Lite 的卓越性能：</p><table><thead><tr><th style=text-align:left>评测基准</th><th style=text-align:center>Qwen2.5-VL-7B</th><th style=text-align:center>Qwen2.5-Omni-7B</th><th style=text-align:center>InternVL3-8B</th><th style=text-align:center><strong>Ming-Omni-Lite</strong></th></tr></thead><tbody><tr><td style=text-align:left><strong>VideoMME(w/o subs)</strong></td><td style=text-align:center>65.10</td><td style=text-align:center>64.30</td><td style=text-align:center>66.30</td><td style=text-align:center><strong>67.07</strong></td></tr><tr><td style=text-align:left><strong>VideoMME(w/ subs)</strong></td><td style=text-align:center>71.60</td><td style=text-align:center>72.40</td><td style=text-align:center>68.90</td><td style=text-align:center><strong>72.59</strong></td></tr><tr><td style=text-align:left><strong>VideoMME(avg)</strong></td><td style=text-align:center>68.35</td><td style=text-align:center>68.35</td><td style=text-align:center>67.60</td><td style=text-align:center><strong>69.83</strong></td></tr><tr><td style=text-align:left><strong>MVBench</strong></td><td style=text-align:center>69.60</td><td style=text-align:center>70.30</td><td style=text-align:center><strong>75.40</strong></td><td style=text-align:center>69.43</td></tr><tr><td style=text-align:left><strong>LongVideoBench</strong></td><td style=text-align:center>56.00</td><td style=text-align:center>54.82</td><td style=text-align:center>58.80</td><td style=text-align:center><strong>59.54</strong></td></tr><tr><td style=text-align:left><strong>OvOBench</strong></td><td style=text-align:center>51.10</td><td style=text-align:center>50.46</td><td style=text-align:center>51.91</td><td style=text-align:center><strong>52.17</strong></td></tr></tbody></table><h3 id=技术背后>技术背后<a hidden class=anchor aria-hidden=true href=#技术背后>#</a></h3><p>Ming-Omni-Lite 在视频理解，尤其是长视频理解上的突破，源于我们在模型架构和训练策略上的多项创新：</p><ul><li><strong>高效的时空建模器：</strong> 加入3D RoPE，能更有效地捕捉视频帧内（空间）和帧间（时间）的依赖关系，提取关键的动态信息。</li><li><strong>高质量、多样化的视频-文本对齐数据：</strong> 构建了大规模、涵盖丰富场景和任务的长/短视频-文本对数据集以及TPO（task-perference optimization）数据，包括时间检索以及视频跟踪。我们进行了精细清洗，确保模型学习到精准的对齐能力。</li><li><strong>创新的训练目标与课程学习：</strong> 结合了视频特有的预训练和指令微调目标，并采用从短到长的课程学习策略，逐步提升模型处理长视频的复杂度。</li></ul><h3 id=迈向更智能的视频交互>迈向更智能的视频交互<a hidden class=anchor aria-hidden=true href=#迈向更智能的视频交互>#</a></h3><p>Ming-Omni-Lite 在和同尺寸SOTA模型的视频理解基准评测上保持领先，它证明了 Ming-Omni-Lite 具备处理<strong>复杂、长时间、信息密集</strong>视频内容的强大能力，为视频摘要、长视频问答、智能教学、视频内容审核、人机交互等广泛应用场景奠定了坚实的基础。我们将持续投入研发，进一步释放 Ming-Omni-Lite 在视频乃至多模态领域的潜力，致力于打造能够真正理解、推理和与现实世界交互的智能体。</p><hr><h2 id=语音理解和生成>语音理解和生成<a hidden class=anchor aria-hidden=true href=#语音理解和生成>#</a></h2><p>在语音理解方面，我们在训练中引入了丰富且多样的语音数据，并且将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现。我们的模型在语音理解方面支持中英文，粤语，四川话，上海话，闽南语等方言，在开源的中英文数据集上达到 <strong>SOTA</strong> 水平。</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>aishell1</th><th style=text-align:center>aishell2_android</th><th style=text-align:center>aishell2_ios</th><th style=text-align:center>cv15_zh</th><th style=text-align:center>fleurs_zh</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>1.31</td><td style=text-align:center><strong>2.45</strong></td><td style=text-align:center><strong>2.45</strong></td><td style=text-align:center>5.69</td><td style=text-align:center>2.87</td></tr><tr><td style=text-align:center>Qwen2.-Omni</td><td style=text-align:center>1.18</td><td style=text-align:center>2.75</td><td style=text-align:center>2.63</td><td style=text-align:center><strong>5.20</strong></td><td style=text-align:center>3.00</td></tr><tr><td style=text-align:center>Qwen2-Audio</td><td style=text-align:center>1.53</td><td style=text-align:center>2.92</td><td style=text-align:center>2.92</td><td style=text-align:center>6.90</td><td style=text-align:center>7.50</td></tr><tr><td style=text-align:center>Kimi-Audio</td><td style=text-align:center><strong>0.60</strong></td><td style=text-align:center>2.64</td><td style=text-align:center>2.56</td><td style=text-align:center>7.21</td><td style=text-align:center><strong>2.69</strong></td></tr></tbody></table><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>ws_meet</th><th style=text-align:center>ws_net</th><th style=text-align:center>lsc_test_cl</th><th style=text-align:center>lsc_test_ot</th><th style=text-align:center>multi_ls</th><th style=text-align:center>cv15_en</th><th style=text-align:center>flu_en</th><th style=text-align:center>vox_en</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>6.18</td><td style=text-align:center><strong>5.22</strong></td><td style=text-align:center><strong>1.24</strong></td><td style=text-align:center>2.61</td><td style=text-align:center><strong>4.13</strong></td><td style=text-align:center><strong>6.95</strong></td><td style=text-align:center><strong>3.28</strong></td><td style=text-align:center>6.82</td></tr><tr><td style=text-align:center>Qwen2-Omni</td><td style=text-align:center><strong>5.90</strong></td><td style=text-align:center>7.70</td><td style=text-align:center>1.80</td><td style=text-align:center>3.40</td><td style=text-align:center>7.56</td><td style=text-align:center>7.60</td><td style=text-align:center>4.10</td><td style=text-align:center><strong>5.80</strong></td></tr><tr><td style=text-align:center>Qwen2-Audio</td><td style=text-align:center>7.16</td><td style=text-align:center>8.42</td><td style=text-align:center>1.60</td><td style=text-align:center>3.60</td><td style=text-align:center>5.40</td><td style=text-align:center>8.60</td><td style=text-align:center>6.90</td><td style=text-align:center>6.84</td></tr><tr><td style=text-align:center>Kimi-Audio</td><td style=text-align:center>6.28</td><td style=text-align:center>5.37</td><td style=text-align:center>1.28</td><td style=text-align:center><strong>2.42</strong></td><td style=text-align:center>5.88</td><td style=text-align:center>10.31</td><td style=text-align:center>4.44</td><td style=text-align:center>7.97</td></tr></tbody></table><style type=text/css>.tg{border:none;border-collapse:collapse;border-spacing:0}.tg td{border-style:solid;border-width:0;font-family:Arial,sans-serif;font-size:14px;overflow:hidden;padding:10px 5px;word-break:normal}.tg th{border-style:solid;border-width:0;font-family:Arial,sans-serif;font-size:14px;font-weight:400;overflow:hidden;padding:10px 5px;word-break:normal}.tg .tg-x5q1{font-size:16px;text-align:left;vertical-align:top}.tg .tg-t0cb{background-color:#fff;color:#1f1f1f;font-size:16px;text-align:left;vertical-align:middle}.tg .tg-hxmt{background-color:#fff;color:#1f1f1f;font-size:16px;text-align:left;vertical-align:top}.tg .tg-19xi{background-color:#fff;color:#1f1f1f;font-size:16px;font-weight:700;text-align:center;vertical-align:middle}</style><table class=tg><thead><tr><th class=tg-19xi>输入音频</th><th class=tg-19xi>方言</th><th class=tg-19xi>识别结果</th></tr></thead><tbody><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr1.wav type=audio/wav></audio></td><td class=tg-t0cb>粤语</td><td class=tg-t0cb>你在干什么, 是不是不想聊天</td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr2.wav type=audio/wav></audio></td><td class=tg-t0cb>上海话</td><td class=tg-t0cb>我们考试还没定下来呢</td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr3.wav type=audio/wav></audio></td><td class=tg-t0cb>闽南语</td><td class=tg-t0cb>宝贝, 早点休息, 晚安</td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/asr4.wav type=audio/wav></audio></td><td class=tg-t0cb>四川话</td><td class=tg-t0cb>我难受的很, 别人都睡了</td></tr></tbody></table><p>得益于模型卓越的语音理解能力，我们的模型在语音对话评估集上也取得优异的效果。</p><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>AlpacaEval</th><th style=text-align:center>CommonEval</th><th style=text-align:center>SD-QA</th><th style=text-align:center>MMSU</th><th style=text-align:center>OpenBookQA</th><th style=text-align:center>IFEval</th><th style=text-align:center>AdvBench</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2-Audio-chat</td><td style=text-align:center>3.69</td><td style=text-align:center>3.40</td><td style=text-align:center>35.35</td><td style=text-align:center>35.43</td><td style=text-align:center>49.01</td><td style=text-align:center>22.57</td><td style=text-align:center>98.85</td></tr><tr><td style=text-align:left>Baichuan-Audio</td><td style=text-align:center>4.00</td><td style=text-align:center>3.39</td><td style=text-align:center>49.64</td><td style=text-align:center>48.80</td><td style=text-align:center>63.30</td><td style=text-align:center>41.32</td><td style=text-align:center>86.73</td></tr><tr><td style=text-align:left>GLM-4-Voice</td><td style=text-align:center>4.06</td><td style=text-align:center>3.48</td><td style=text-align:center>43.31</td><td style=text-align:center>40.11</td><td style=text-align:center>52.97</td><td style=text-align:center>24.91</td><td style=text-align:center>88.08</td></tr><tr><td style=text-align:left>Kimi-Audio</td><td style=text-align:center>4.46</td><td style=text-align:center>3.97</td><td style=text-align:center><b>63.12</b></td><td style=text-align:center><b>62.17</b></td><td style=text-align:center><b>83.52</b></td><td style=text-align:center><b>61.10</b></td><td style=text-align:center><b>100.00</b></td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center>4.49</td><td style=text-align:center>3.93</td><td style=text-align:center>55.71</td><td style=text-align:center>61.32</td><td style=text-align:center>81.10</td><td style=text-align:center>52.87</td><td style=text-align:center>99.42</td></tr><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center><b>4.65</b></td><td style=text-align:center><b>4.30</b></td><td style=text-align:center>61.16</td><td style=text-align:center>45.77</td><td style=text-align:center>65.93</td><td style=text-align:center>55.60</td><td style=text-align:center>98.08</td></tr></tbody></table><table class=tg><thead><tr><th class=tg-19xi>输入音频</th><th class=tg-19xi>输出音频</th></tr></thead><tbody><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa1.wav type=audio/wav></audio></td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa1_out.wav type=audio/wav></audio></td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa2.wav type=audio/wav></audio></td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/qa2_out.wav type=audio/wav></audio></td></tr></tbody></table><p>语音生成方面，Ming-lite-omni 集成了先进的音频解码器，该解码器接受来自LLM的输出隐藏状态，这使得模型能够处理上下文感知的多模态对话和标准的文本转语音（TTS）转换，从而生成自然流畅的语音 。为了提高韵律性能和实时生成能力，将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。</p><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>seed-tts-eval-zh_wer</th><th style=text-align:center>seed-tts-eval-zh_sim</th><th style=text-align:center>seed-tts-eval-en_wer</th><th style=text-align:center>seed-tts-eval-en_sim</th></tr></thead><tbody><tr><td style=text-align:center>Seed-TTS</td><td style=text-align:center>1.11</td><td style=text-align:center>0.796</td><td style=text-align:center>2.24</td><td style=text-align:center>0.762</td></tr><tr><td style=text-align:center>MaskGCT</td><td style=text-align:center>2.27</td><td style=text-align:center>0.774</td><td style=text-align:center>2.62</td><td style=text-align:center>0.714</td></tr><tr><td style=text-align:center>E2 TTS</td><td style=text-align:center>1.97</td><td style=text-align:center>0.730</td><td style=text-align:center>2.19</td><td style=text-align:center>0.710</td></tr><tr><td style=text-align:center>F5-TTS</td><td style=text-align:center>1.56</td><td style=text-align:center>0.741</td><td style=text-align:center>1.83</td><td style=text-align:center>0.647</td></tr><tr><td style=text-align:center>CosyVoice 2</td><td style=text-align:center>1.45</td><td style=text-align:center>0.748</td><td style=text-align:center>2.57</td><td style=text-align:center>0.652</td></tr><tr><td style=text-align:center>Qwen2.5-Omni-7B</td><td style=text-align:center>1.70</td><td style=text-align:center>0.752</td><td style=text-align:center>2.72</td><td style=text-align:center>0.632</td></tr><tr><td style=text-align:center>Ming-Lite-Omni</td><td style=text-align:center>2.00</td><td style=text-align:center>0.686</td><td style=text-align:center>4.299</td><td style=text-align:center>0.513</td></tr></tbody></table><table class=tg><thead><tr><th class=tg-19xi>参考音频</th><th class=tg-19xi>输入文本</th><th class=tg-19xi>输出音频</th></tr></thead><tbody><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts1_prompt.wav type=audio/wav></audio></td><td class=tg-t0cb>我们的愿景是构建未来服务业的数字化基础设施，给世界带来更多微小而美好的改变。</td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts1wav.wav type=audio/wav></audio></td></tr><tr><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts2_prompt.wav type=audio/wav></audio></td><td class=tg-t0cb>The stained glass offered a hypnotic atmosphere</td><td class=tg-hxmt><audio controls><source src=https://github.com/Biao-Gong/static/raw/refs/heads/main/aud/0715/tts2.wav type=audio/wav></audio></td></tr></tbody></table><hr><h2 id=突破图像生成新极限>突破图像生成新极限<a hidden class=anchor aria-hidden=true href=#突破图像生成新极限>#</a></h2><p>相较于早先发布的 Ming-Lite-Uni 以及 Ming-Omni，在此版本中，我们进一步提升了Ming对生成图像的 <strong>场景一致性</strong>（Scene Consistency）、<strong>ID 一致性</strong>（Character / Style Consistency）、以及 <strong>多感知扩展</strong>（Segmentation, Keypoints, Depth, …），让Ming从一个具备图像生成和图像编辑能力的多模态大模型（MLLM），变成一个能够处理更多图像生成任务以及效果更好的完整MLLM。下面是我们最近一段时间的工作进展的报告，欢迎大家交流讨论。</p><p><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752039359523-ef57c4ba-3f99-4a9a-9515-5728b6c46c1c.webp alt></p><table><thead><tr><th><strong>GenEval指标</strong></th><th><strong>Overall score</strong></th><th><strong>Single Ojbect</strong></th><th><strong>Two Objects</strong></th><th><strong>Counting</strong></th><th><strong>Color</strong></th><th><strong>Position</strong></th><th><strong>Color Attr</strong></th></tr></thead><tbody><tr><td>Ours</td><td>0.86</td><td>100.00%</td><td>96.72%</td><td>76.56%</td><td>89.89%</td><td>89.75%</td><td>68.69%</td></tr></tbody></table><h3 id=模型结构回顾及改进>模型结构回顾及改进<a hidden class=anchor aria-hidden=true href=#模型结构回顾及改进>#</a></h3><table><thead><tr><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">子模块</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">对应技术点</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">作用</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">关键点</font></strong></th></tr></thead><tbody><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">跨模态桥接方式</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Channel concat / Token concat / Blend</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Channel concat</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">：参数少、显存低，但语义对齐弱 </font><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Token concat</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">：保语义结构，适合大分辨率 </font><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- </font><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Blend</font></strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">：编辑/重绘场景更鲁棒</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">根据任务需求选择桥接方式。目前采用Token Concat方案</font></td></tr><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">双分支表示解耦</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">解耦参考图图像patch编码和refiner参数</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">- 提升参考图的独立控制参数容量，起到部分解耦的作用</font></td><td>双 patchfy 模块与双分支额外 refiner，提升了模型编辑与分割性能</td></tr></tbody></table><ul><li>双分支解耦指在将图像送入 DiT 的 transformer 之前，使用不同的网络权重将参考图像与噪声图像进行 patchfy，这样能够有效降低参考图像信息对于编辑时语义遵循的影响，refiner 是在 patchfy 之后额外的两层轻量级 transformer，能够进一步增强这一效果，在推理分割上的性能评估表明了新增模块的有效性<ul><li>推理分割考验模型对于语义的正确理解，需要模型根据复杂的指令确定要分割的目标</li><li>实验结果如下方表格所示，可以看到解耦的patchfy显著增强了推理分割的指标，增加 refiner 模块后能够进一步提升性能</li></ul></li><li>GEdit 子集：[&ldquo;background_change&rdquo;, &ldquo;color_alter&rdquo;, &ldquo;material_alter&rdquo;, &ldquo;motion_change&rdquo;]</li></ul><table><thead><tr><th>Mode ID</th><th>double-patchfy</th><th>add-refiner</th><th>refcoc 分割指标</th><th>GEdit(subset-full)</th></tr></thead><tbody><tr><td>0</td><td>❌</td><td>❌</td><td>62.8</td><td>6.129</td></tr><tr><td>1</td><td>✅</td><td>❌</td><td>64.2</td><td>6.391</td></tr><tr><td>2</td><td>✅</td><td>✅</td><td>64.5</td><td>6.306</td></tr></tbody></table><h3 id=条件控制与引导策略>条件控制与引导策略<a hidden class=anchor aria-hidden=true href=#条件控制与引导策略>#</a></h3><table><thead><tr><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">子模块</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">对应技术点</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">作用</font></strong></th><th><strong><font style="color:rgb(0, 0, 0);background-color:rgba(0, 0, 0, 0);">关键点</font></strong></th></tr></thead><tbody><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">多条件的CFG控制策略</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">语义CFG vs 图像CFG（Ref-Guided）</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">多条件的 Classifier-free Guidance 策略：语义二分差分+图像三分差分提升ID一致性</font></td><td>当纯语义控制时，编辑后的图像遵循了指令，但完全丧失与原图的一致性；图像分支指导强度较大时编辑结果几乎与原图一致</td></tr><tr><td><strong><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">ID & Scene Consistency Loss</font></strong></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">Weight mask Loss + Scene Consistency Loss</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">增大目标图编辑区域的权重，同时增加参考图非编辑区域的强约束和编辑区域的弱约束</font></td><td><font style="color:rgba(0, 0, 0, 0.9);background-color:rgba(0, 0, 0, 0);">调整λ平衡身份保持下的编辑效果与场景一致性，避免过拟合</font></td></tr></tbody></table><ul><li>与 Qwen-VLo 对比</li></ul><table><thead><tr><th>prompt</th><th>ours</th><th>Qwen-VLo</th></tr></thead><tbody><tr><td><font style="color:rgb(44, 44, 54);">Make the person in the image smile slightly without altering the original structure</font><br><img loading=lazy src="https://github.com/Biao-Gong/static/blob/main/gen/1752147843685-5b097f6b-b2aa-4baf-abe4-f1abd89265e8.png?raw=true" alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752147837185-62077f0c-e7ec-415f-bd34-1c8453253949.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752147953713-703c31c8-2fd1-4c2d-b4bc-6e0f52e70017.webp alt></td></tr></tbody></table><h3 id=感知能力扩展>感知能力扩展<a hidden class=anchor aria-hidden=true href=#感知能力扩展>#</a></h3><ul><li><p><strong>生成式分割</strong></p><p>相比于生成式图像编辑任务，分割任务的预测mask和原图之间存在较少的细节一致性，因此在token concat方案下难以较快的学习到原图和分割mask 之间的一致性关系。因此，我们将图像的分割目标建模成彩色分割图像，即mask和图像的融合形式，从而使得预测目标和原图之间存在较多一致的细节，因此能够更好的学习到分割图和原图之间的一致关系。在推理时，将预测图像和原图做diff并进行噪声过滤获取最终的预测mask。</p></li></ul><table><thead><tr><th>输入图像</th><th>推理分割</th><th>语义分割</th><th>全景分割</th></tr></thead><tbody><tr><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115158022-12254e69-e8c0-43fb-a725-f6730cda22d8.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115142775-3975827c-4110-445b-af53-e20201d1043a.webp alt><br>prompt: Given the following instructions: little girl, pink, your monitors colors off friend p pink shirt girl; please perform referring segmentation on this image.</td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752116495974-7708ba3a-5909-46df-82f5-a1bfa1519d4d.webp alt><br>prompt: Please segment different <strong>classes</strong> in this image</td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752115151406-c4780a97-5f1c-46cd-9a45-d4ef600d0897.webp alt><br>prompt: Please segment different <strong>instances</strong> in this image.</td></tr></tbody></table><ul><li><strong>边缘轮廓图生成</strong></li></ul><table><thead><tr><th>原图</th><th>深度图</th><th>检测框</th><th>边缘轮廓</th></tr></thead><tbody><tr><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466889319-bd19acce-c07d-4664-9890-41e4dff1ba8d.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466903529-996bcd35-a9a0-484b-98bf-2f2468f4df42.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752466895795-1955ead5-6d94-4142-8d7b-e265352d2bcb.webp alt></td><td><img loading=lazy src=https://raw.githubusercontent.com/Biao-Gong/static/refs/heads/main/gen/1752467020122-ad8b436c-bb33-4ef0-85b8-cf45ae8c9be1.webp alt></td></tr></tbody></table><hr><h2 id=数据优化>数据优化<a hidden class=anchor aria-hidden=true href=#数据优化>#</a></h2><p><strong>文档解析语料</strong>：为了提升基模在深度文本视觉分析与逻辑推理方面的性能，我们在训练过程中引入"思维链"（Chain-of-Thought）范式。具体实施策略如下：</p><p>首先，在训练体系中集成开源数据集ChartQA-PoT（Program-of-Thought），重点提升模型的图表数值计算能力。该方法继承了思维链的递进式推理理念，但突破性地采用可执行Python代码作为中间推理载体。其次，针对传统文本相关问答数据集中普遍存在的"答案直推"模式局限，我们创新性利用强化学习模型对原始训练样本进行多步推理轨迹及最终答案生成；筛选通过形式化验证的"推理步骤-最终答案"对构建新型训练集。该方法显著提升了模型对复杂逻辑任务的适应性。</p><p><strong>人类偏好语料</strong>：多模态大模型的用户真实交互体验优化通过利用人类体验偏好数据对齐训练来提升性能。我们的体验偏好指令集主要由三个来源构成：应用端的真实用户对话数据、高质量开源指令数据集采样以及网站的搜索查询数据。对于网站的搜索查询数据，我们首先用纯文本查询通过搜索引擎检索相关的网络图像；随后，我们利用现有的MLLM改写生成多样的专业问题及其对应的答案。我们对上述的多种用户指令数据源生成出高质量的正样本以及负样本数据偏好对，并结合MLLM和专业人工标注员对这些问答对的质量进行标注校验，构建了覆盖9个主要领域的41个子类别人类体验偏好语料库。</p><p><strong>视频理解语料</strong>：高质量的视频数据提供了更为丰富且精准的语义信息，能够有效提升模型对复杂视频场景的理解深度与广度。我们构建的视频语料库主要来源于两个方面：首先，我们深度挖掘并整合了现有开源视觉基础任务数据集，如采用目标跟踪数据集GOT-10K增强模型的物体追踪能力，利用时序检索数据集DiDemo提升模型的事件感知能力；其次，我们组织专业标注团队精心构建了一批高难度的视频问答样本对，用于训练模型的复杂推理能力。</p><p><strong>百科语料</strong>：百科数据扩充了多模态大模型的可识别实体范围，为大模型注入专家级的细粒度实体识别能力。在这个版本中，我们共包含三大类共十小类的实体集合，分别为自然百科实体集（植物，动物）；人文百科实体集（名人，动漫角色，地标，LOGO，和艺术品）；和日常生活实体集（食材，菜品，汽车）。为了构建一个高质量的专家级细粒度百科实体数据集，我们首先从专业的学术网站或垂类站点上大规模的收集了一批实体集合，之后我们使用这些实体集合组成检索词，在公开的搜索引擎上获取这些实体数据。接着，我们设计了一个高效的逐级递进的实体数据清洗链路，包含CLIP图文相似度过滤，大模型是否可用二分判断过滤，和人工纠正过滤的流程。</p><p><strong>GUI语料</strong>：GUI（图形用户界面）数据使模型在Android环境下具有基本的GUI导航能力。我们的GUI语料库主要由四个公共数据集构建：AITW， guiccourse， AndroidControl和AMEX。此外，我们利用可用的MLLM来优化类人GUI交互操作中每个步骤的推理过程，随后由另一个MLLM审查以提高数据质量。思维过程约束模型观察当前状态，反思之前的动作，在操作之前仔细考虑。为了更好地感知当前情况，我们加入了历史记忆，由前序的动作组成。</p><p><strong>图像生产与编辑语料</strong>：我们的图像生成语料库主要来自两个来源：一是从公开的图像生成的高质量图像数据集（例如，text-to-image-2M、JourneyDB、BLip3o、Uniworld dataset、InstructPix2Pix-clip-filtered、SEED-Data-Edit-part2/3、Ultraedit等）；二是从 StyleBooth 和 WikiArt 中采样的图像风格迁移数据。另外我们构造了生成图像数据的管线, 生成了部分文生图和编辑数据。</p><p><strong>感知推理语料</strong>：感知推理数据能够提升模型的综合理解能力和细粒度感知能力。在这个版本，我们针对目标计数、颜色识别、场景主题识别等感知类细分任务，从Object365、RefCOCO等开源数据集中，根据检测框数量、位置关系和物体类别数量等信息，筛选出一批潜在难样本。随后，利用VLM进行指定任务的问答合成，并通过多模型打分和人工过滤纠正的方式获得一批高质量样本。此外，为提升模型在感知任务上的推理能力，我们将上述合成数据改写为CoT形式，通过生成中间推理步骤，帮助模型分解复杂问题，提升综合感知能力。</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
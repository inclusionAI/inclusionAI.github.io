<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Omni-Preview: MOE架构的多模态大模型 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 🤗 Hugging Face | 🤖 ModelScope
简介 Ming-Lite-Omni-Preview 构建自 Ling-Lite，它是一个 MoE（专家混合）模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式生成文本和自然语音。 为了更自然地处理多模态输入，我们对 Ling-Lite 进行了增强，为每种模态引入了专用路由模块。 因此，Ming-Omni 在处理多模态信息方面表现优异，并具有很强的可扩展性。
主要特性 Omni and Novel MoE Architecture: 一种基于专家混合（MoE）的创新型 Omni 架构，在多个多模态评测中取得了领先性能。
Video understanding: 支持视觉 Token 的 KV-Cache 动态压缩机制，既能理解数小时的长视频，也能对几秒钟的短视频进行精细分析。
Natural Speech Generation and Fine-grained Voice Dialogue: 支持端到端对话中的方言理解与生成，具备一次性语音克隆能力，并通过音频分词器压缩提升语调表现力。
评测结果 Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-preview/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Lite-Omni-Preview: MOE架构的多模态大模型"><meta property="og:description" content="GITHUB 🤗 Hugging Face | 🤖 ModelScope
简介 Ming-Lite-Omni-Preview 构建自 Ling-Lite，它是一个 MoE（专家混合）模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式生成文本和自然语音。 为了更自然地处理多模态输入，我们对 Ling-Lite 进行了增强，为每种模态引入了专用路由模块。 因此，Ming-Omni 在处理多模态信息方面表现优异，并具有很强的可扩展性。
主要特性 Omni and Novel MoE Architecture: 一种基于专家混合（MoE）的创新型 Omni 架构，在多个多模态评测中取得了领先性能。
Video understanding: 支持视觉 Token 的 KV-Cache 动态压缩机制，既能理解数小时的长视频，也能对几秒钟的短视频进行精细分析。
Natural Speech Generation and Fine-grained Voice Dialogue: 支持端到端对话中的方言理解与生成，具备一次性语音克隆能力，并通过音频分词器压缩提升语调表现力。
评测结果 Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-05T00:00:03+08:00"><meta property="article:modified_time" content="2025-05-05T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Omni-Preview: MOE架构的多模态大模型"><meta name=twitter:description content="GITHUB 🤗 Hugging Face | 🤖 ModelScope
简介 Ming-Lite-Omni-Preview 构建自 Ling-Lite，它是一个 MoE（专家混合）模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式生成文本和自然语音。 为了更自然地处理多模态输入，我们对 Ling-Lite 进行了增强，为每种模态引入了专用路由模块。 因此，Ming-Omni 在处理多模态信息方面表现优异，并具有很强的可扩展性。
主要特性 Omni and Novel MoE Architecture: 一种基于专家混合（MoE）的创新型 Omni 架构，在多个多模态评测中取得了领先性能。
Video understanding: 支持视觉 Token 的 KV-Cache 动态压缩机制，既能理解数小时的长视频，也能对几秒钟的短视频进行精细分析。
Natural Speech Generation and Fine-grained Voice Dialogue: 支持端到端对话中的方言理解与生成，具备一次性语音克隆能力，并通过音频分词器压缩提升语调表现力。
评测结果 Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Omni-Preview: MOE架构的多模态大模型","item":"https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Omni-Preview: MOE架构的多模态大模型","name":"Ming-Lite-Omni-Preview: MOE架构的多模态大模型","description":"GITHUB 🤗 Hugging Face | 🤖 ModelScope\n简介 Ming-Lite-Omni-Preview 构建自 Ling-Lite，它是一个 MoE（专家混合）模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式生成文本和自然语音。 为了更自然地处理多模态输入，我们对 Ling-Lite 进行了增强，为每种模态引入了专用路由模块。 因此，Ming-Omni 在处理多模态信息方面表现优异，并具有很强的可扩展性。\n主要特性 Omni and Novel MoE Architecture: 一种基于专家混合（MoE）的创新型 Omni 架构，在多个多模态评测中取得了领先性能。\nVideo understanding: 支持视觉 Token 的 KV-Cache 动态压缩机制，既能理解数小时的长视频，也能对几秒钟的短视频进行精细分析。\nNatural Speech Generation and Fine-grained Voice Dialogue: 支持端到端对话中的方言理解与生成，具备一次性语音克隆能力，并通过音频分词器压缩提升语调表现力。\n评测结果 Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67.","keywords":[],"articleBody":"GITHUB 🤗 Hugging Face | 🤖 ModelScope\n简介 Ming-Lite-Omni-Preview 构建自 Ling-Lite，它是一个 MoE（专家混合）模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式生成文本和自然语音。 为了更自然地处理多模态输入，我们对 Ling-Lite 进行了增强，为每种模态引入了专用路由模块。 因此，Ming-Omni 在处理多模态信息方面表现优异，并具有很强的可扩展性。\n主要特性 Omni and Novel MoE Architecture: 一种基于专家混合（MoE）的创新型 Omni 架构，在多个多模态评测中取得了领先性能。\nVideo understanding: 支持视觉 Token 的 KV-Cache 动态压缩机制，既能理解数小时的长视频，也能对几秒钟的短视频进行精细分析。\nNatural Speech Generation and Fine-grained Voice Dialogue: 支持端到端对话中的方言理解与生成，具备一次性语音克隆能力，并通过音频分词器压缩提升语调表现力。\n评测结果 Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67.1 68.1 MathVista 69.0 68.2 67.9 OCRBench 87.9 86.4 88.2 Average 70.96 70.5 70.3 Object Recognition Object Recognition Ming-Lite-Omni-Preview Qwen2.5-VL-7B InternVL-2.5-8B Plants 52.1 55.3 32.8 Animals 52.6 54.8 36.5 Home appliances \u0026 furniture 93.5 97.4 90.9 Personal Electronics 96.1 95.1 93.2 Food \u0026 Ingredients 57.5 60.0 48.7 Tableware 96.6 94.9 88.1 Vehicles 31.9 40.9 31.9 Average 68.6 71.2 60.3 Video benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5VL-7B VideoMME wo/w sub. 63.9/67.6 65.1/71.6 MVBench 67.0 72.0 Video-MMMU 45.4 47.44 LongVideoBench 53.7 60.0 Audio benchmark SpeechQA Model AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-Lite-Omni-Preview 4.25 3.88 58.95 46.06 60.00 46.71 96.53 ASR Model Aishell-1 Aishell-2 ios Wenetspeech test-net Wenet test-meeting Librispeech test-clean Librispeech test-other Whisper Large-v3 5.14 4.76 9.68 18.54 1.9 3.65 Qwen2-Audio 1.53 3.06 7.72 8.4 1.6 3.6 GLM-4-voice Base 2.46 - - - 2.82 7.66 Baichuan-Omni-1.5 - - 6.9 8.4 - - Qwen2.5-Omni 1.18 2.36 5.9 7.7 1.8 3.4 Ming-Lite-Omni-Preview 1.62 2.82 6.23 6.9 2.34 5.74 Knowledge Model InfoSeek_H-mean InfoSeek_unseen_question InfoSeek_unseen_entity GPT-4o 36.05 - - PaLI-X 22.06 23.5 20.8 Qwen2.5-vl-32B 19.35 20.55 18.28 Ming-Lite-Omni-Preview 27.3 28.9 25.9 OCR\u0026GUI Model Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct ChartQA_TEST 85.2 87.3 DocVQA_TEST 93.2 95.7 OCRBenchV2_en/zh 52.2/51.6 56.3/57.2 OmniDocBench↓ 34.7/34.5 30.8/39.8 TextVQA_VAL 82.36 84.9 ScreenSpot 79.3 84.7 模型下载 你可以从 Huggingface 和 ModelScope 两个平台下载本模型。\nModel Input modality Oput modality Download Ming-Lite-Omni-Preview Image,text,viedio,audio Image,text,audio 🤗 HuggingFace 🤖 ModelScope 如果你在中国大陆，强烈建议你通过以下平台下载模型： 🤖 ModelScope. 使用案例 视频音频问答 MultiModal Input QA Q: (audio content: 请描述视频内容。)\nA: The video features a woman performing a series of yoga poses on a rooftop with a scenic view of mountains and a clear blue sky. Q: Is there any food in front of me? A: Yes, there’s candy on the table. 语音转语音（支持方言） 快速上手 Please download our model following Model Downloads, then you can refer to the following codes to run Ming-Lite-Omni-Preview model.\nimport os from transformers import AutoProcessor from modeling_bailingmm import BailingMMNativeForConditionalGeneration # build model model = BailingMMNativeForConditionalGeneration.from_pretrained( \"inclusionAI/Ming-Lite-Omni\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).to(\"cuda\") assets_path = YOUR_ASSETS_PATH # build processor processor = AutoProcessor.from_pretrained(\"inclusionAI/Ming-Lite-Omni\", trust_remote_code=True) # qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"请详细介绍鹦鹉的生活习性。\"} ], }, ] # Output: # 鹦鹉是一种非常聪明和社交性强的鸟类，它们的生活习性非常丰富和有趣。以下是一些关于鹦鹉生活习性的详细介绍： # ### 1. **栖息地** # 鹦鹉主要分布在热带和亚热带地区，包括非洲、亚洲、澳大利亚和南美洲。它们通常生活在森林、草原、沙漠和城市环境中。不同种类的鹦鹉对栖息地的要求有所不同，但大多数鹦鹉喜欢有丰富植被和水源的地方。 # ### 2. **饮食** # 鹦鹉是杂食性动物，它们的饮食非常多样化。它们的食物包括种子、坚果、水果、蔬菜、花蜜和昆虫。鹦鹉的喙非常强壮，能够轻松地打开坚硬的果壳和坚果。一些鹦鹉还会吃泥土或沙子，以帮助消化和补充矿物质。 # ...... # image qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"image\", \"image\": os.path.join(assets_path, \"flowers.jpg\")}, {\"type\": \"text\", \"text\": \"What kind of flower is this?\"}, ], }, ] # Output: # The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white. To enable thinking before response, adding the following system prompt before your question:\ncot_prompt = \"SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in ... tags, then the final answer enclosed in ... tags. The critical answer or key result should be placed within \\\\boxed{}.\\n\" # And your input message should be like this: messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"image\", \"image\": os.path.join(assets_path, \"reasoning.png\")}, {\"type\": \"text\", \"text\": cot_prompt + \"In the rectangle $A B C D$ pictured, $M_{1}$ is the midpoint of $D C, M_{2}$ the midpoint of $A M_{1}, M_{3}$ the midpoint of $B M_{2}$ and $M_{4}$ the midpoint of $C M_{3}$. Determine the ratio of the area of the quadrilateral $M_{1} M_{2} M_{3} M_{4}$ to the area of the rectangle $A B C D$.\\nChoices:\\n(A) $\\frac{7}{16}$\\n(B) $\\frac{3}{16}$\\n(C) $\\frac{7}{32}$\\n(D) $\\frac{9}{32}$\\n(E) $\\frac{1}{5}$\"}, ], }, ] # Output: # \\\\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\\n\\\\n\\\\\\boxed{C}\\\\n\\n # video qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"video\", \"video\": os.path.join(assets_path, \"yoga.mp4\")}, {\"type\": \"text\", \"text\": \"What is the woman doing?\"}, ], }, ] # Output: # The image shows a woman performing a yoga pose on a rooftop. She's in a dynamic yoga pose, with her arms and legs extended in various positions. # multi-turn chat messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"中国的首都是哪里？\"}, ], }, { \"role\": \"ASSISTANT\", \"content\": [ {\"type\": \"text\", \"text\": \"北京\"}, ], }, { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"它的占地面积是多少？有多少常住人口？\"}, ], }, ] # Output: # 北京市的总面积约为16,410.54平方公里，常住人口约为21,542,000人。 # Preparation for inference text = processor.apply_chat_template(messages, add_generation_prompt=True) image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages) inputs = processor( text=[text], images=image_inputs, videos=video_inputs, audios=audio_inputs, return_tensors=\"pt\", ) inputs = inputs.to(model.device) for k in inputs.keys(): if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\": inputs[k] = inputs[k].to(dtype=torch.bfloat16) # call generate generated_ids = model.generate( **inputs, max_new_tokens=512, use_cache=False, eos_token_id=processor.gen_terminator, ) generated_ids_trimmed = [ out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] output_text = processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False )[0] print(output_text) # ASR messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"Please recognize the language of this speech and transcribe it. Format: oral.\"}, {\"type\": \"audio\", \"audio\": 'data/wavs/BAC009S0915W0292.wav'}, ], }, ] outputs = model.generate(messages, max_new_tokens=512) print(outputs) # speech2speech messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"audio\", \"audio\": 'data/wavs/BAC009S0915W0292.wav'}, ], }, ] outputs = model.generate(messages, max_new_tokens=512, speaker='luna', output_audio_path='out.wav', output_audio=True) print(outputs) 许可证与法律声明 本代码库遵循 MIT 协议，法律免责声明见项目根目录下的 LEGAL.md 文件。\n","wordCount":"944","inLanguage":"zh","datePublished":"2025-05-05T00:00:03+08:00","dateModified":"2025-05-05T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Omni-Preview: MOE架构的多模态大模型</h1><div class=post-meta><span title='2025-05-05 00:00:03 +0800 +0800'>2025年5月5日</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;944 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ming-lite-omni-preview/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> 🤗 <a href=https://huggingface.co/inclusionAI>Hugging Face</a> | 🤖 <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a></p><h2 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h2><p>Ming-Lite-Omni-Preview 构建自 <a href=https://github.com/inclusionAI/Ling>Ling-Lite</a>，它是一个 MoE（专家混合）模型，能够感知文本、图像、音频和视频等多种模态，并以流式方式生成文本和自然语音。 为了更自然地处理多模态输入，我们对 Ling-Lite 进行了增强，为每种模态引入了专用路由模块。 因此，Ming-Omni 在处理多模态信息方面表现优异，并具有很强的可扩展性。</p><h2 id=主要特性>主要特性<a hidden class=anchor aria-hidden=true href=#主要特性>#</a></h2><ul><li><p><strong>Omni and Novel MoE Architecture</strong>: 一种基于专家混合（MoE）的创新型 Omni 架构，在多个多模态评测中取得了领先性能。</p></li><li><p><strong>Video understanding</strong>: 支持视觉 Token 的 KV-Cache 动态压缩机制，既能理解数小时的长视频，也能对几秒钟的短视频进行精细分析。</p></li><li><p><strong>Natural Speech Generation and Fine-grained Voice Dialogue</strong>: 支持端到端对话中的方言理解与生成，具备一次性语音克隆能力，并通过音频分词器压缩提升语调表现力。</p></li></ul><h2 id=评测结果>评测结果<a hidden class=anchor aria-hidden=true href=#评测结果>#</a></h2><h3 id=image-benchmark>Image benchmark<a hidden class=anchor aria-hidden=true href=#image-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th><th style=text-align:center>InternVL2.5-8B-MPO</th></tr></thead><tbody><tr><td style=text-align:left>AI2D</td><td style=text-align:center>83.84</td><td style=text-align:center>83.9</td><td style=text-align:center><b>84.5</b></td></tr><tr><td style=text-align:left>HallusionBench</td><td style=text-align:center><b>54.68</b></td><td style=text-align:center>51.9</td><td style=text-align:center>51.7</td></tr><tr><td style=text-align:left>MMBench_TEST_V11</td><td style=text-align:center>79.63</td><td style=text-align:center><b>84.3</b></td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>MMMU</td><td style=text-align:center>57.0</td><td style=text-align:center><b>58.6</b></td><td style=text-align:center>54.8</td></tr><tr><td style=text-align:left>MMStar</td><td style=text-align:center>62.0</td><td style=text-align:center>63.9</td><td style=text-align:center><b>65.2</b></td></tr><tr><td style=text-align:left>MMVet</td><td style=text-align:center><b>73.6</b></td><td style=text-align:center>67.1</td><td style=text-align:center>68.1</td></tr><tr><td style=text-align:left>MathVista</td><td style=text-align:center><b>69.0</b></td><td style=text-align:center>68.2</td><td style=text-align:center>67.9</td></tr><tr><td style=text-align:left>OCRBench</td><td style=text-align:center>87.9</td><td style=text-align:center>86.4</td><td style=text-align:center><b>88.2</b></td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center><b>70.96</b></td><td style=text-align:center>70.5</td><td style=text-align:center>70.3</td></tr></tbody></table></div><h4 id=object-recognition>Object Recognition<a hidden class=anchor aria-hidden=true href=#object-recognition>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Object Recognition</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5-VL-7B</th><th style=text-align:center>InternVL-2.5-8B</th></tr></thead><tbody><tr><td style=text-align:left>Plants</td><td style=text-align:center>52.1</td><td style=text-align:center><b>55.3</b></td><td style=text-align:center>32.8</td></tr><tr><td style=text-align:left>Animals</td><td style=text-align:center>52.6</td><td style=text-align:center><b>54.8</b></td><td style=text-align:center>36.5</td></tr><tr><td style=text-align:left>Home appliances & furniture</td><td style=text-align:center>93.5</td><td style=text-align:center><b>97.4</b></td><td style=text-align:center>90.9</td></tr><tr><td style=text-align:left>Personal Electronics</td><td style=text-align:center><b>96.1</b></td><td style=text-align:center>95.1</td><td style=text-align:center>93.2</td></tr><tr><td style=text-align:left>Food & Ingredients</td><td style=text-align:center>57.5</td><td style=text-align:center><b>60.0</b></td><td style=text-align:center>48.7</td></tr><tr><td style=text-align:left>Tableware</td><td style=text-align:center><b>96.6</td><td style=text-align:center>94.9</td><td style=text-align:center>88.1</td></tr><tr><td style=text-align:left>Vehicles</td><td style=text-align:center>31.9</td><td style=text-align:center><b>40.9</b></td><td style=text-align:center>31.9</td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center>68.6</td><td style=text-align:center><b>71.2</b></td><td style=text-align:center>60.3</td></tr></tbody></table></div><h3 id=video-benchmark>Video benchmark<a hidden class=anchor aria-hidden=true href=#video-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5VL-7B</th></tr></thead><tbody><tr><td style=text-align:left>VideoMME wo/w sub.</td><td style=text-align:center>63.9/67.6</td><td style=text-align:center><b>65.1/71.6</b></td></tr><tr><td style=text-align:left>MVBench</td><td style=text-align:center>67.0</td><td style=text-align:center><b>72.0</b></td></tr><tr><td style=text-align:left>Video-MMMU</td><td style=text-align:center>45.4</td><td style=text-align:center><b>47.44</b></td></tr><tr><td style=text-align:left>LongVideoBench</td><td style=text-align:center>53.7</td><td style=text-align:center><b>60.0</b></td></tr></tbody></table></div><h3 id=audio-benchmark>Audio benchmark<a hidden class=anchor aria-hidden=true href=#audio-benchmark>#</a></h3><h4 id=speechqa>SpeechQA<a hidden class=anchor aria-hidden=true href=#speechqa>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>AlpacaEval</th><th style=text-align:center>CommonEval</th><th style=text-align:center>SD-QA</th><th style=text-align:center>MMSU</th><th style=text-align:center>OpenBookQA</th><th style=text-align:center>IFEval</th><th style=text-align:center>AdvBench</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2-Audio-chat</td><td style=text-align:center>3.69</td><td style=text-align:center>3.40</td><td style=text-align:center>35.35</td><td style=text-align:center>35.43</td><td style=text-align:center>49.01</td><td style=text-align:center>22.57</td><td style=text-align:center>98.85</td></tr><tr><td style=text-align:left>Baichuan-Audio</td><td style=text-align:center>4.00</td><td style=text-align:center>3.39</td><td style=text-align:center>49.64</td><td style=text-align:center>48.80</td><td style=text-align:center>63.30</td><td style=text-align:center>41.32</td><td style=text-align:center>86.73</td></tr><tr><td style=text-align:left>GLM-4-Voice</td><td style=text-align:center>4.06</td><td style=text-align:center>3.48</td><td style=text-align:center>43.31</td><td style=text-align:center>40.11</td><td style=text-align:center>52.97</td><td style=text-align:center>24.91</td><td style=text-align:center>88.08</td></tr><tr><td style=text-align:left>Kimi-Audio</td><td style=text-align:center>4.46</td><td style=text-align:center><b>3.97</b></td><td style=text-align:center><b>63.12</b></td><td style=text-align:center>62.17</td><td style=text-align:center><b>83.52</b></td><td style=text-align:center><b>61.10</b></td><td style=text-align:center><b>100.00</b></td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center><b>4.49</b></td><td style=text-align:center>3.93</td><td style=text-align:center>55.71</td><td style=text-align:center><b>61.32</b></td><td style=text-align:center>81.10</td><td style=text-align:center>52.87</td><td style=text-align:center>99.42</td></tr><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>4.25</td><td style=text-align:center>3.88</td><td style=text-align:center>58.95</td><td style=text-align:center>46.06</td><td style=text-align:center>60.00</td><td style=text-align:center>46.71</td><td style=text-align:center>96.53</td></tr></tbody></table></div><h4 id=asr>ASR<a hidden class=anchor aria-hidden=true href=#asr>#</a></h4><div align=center><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:center><strong>Aishell-1</strong></th><th style=text-align:center><strong>Aishell-2 ios</strong></th><th style=text-align:center><strong>Wenetspeech test-net</strong></th><th style=text-align:center><strong>Wenet test-meeting</strong></th><th style=text-align:center><strong>Librispeech test-clean</strong></th><th style=text-align:center><strong>Librispeech test-other</strong></th></tr></thead><tbody><tr><td style=text-align:left>Whisper Large-v3</td><td style=text-align:center>5.14</td><td style=text-align:center>4.76</td><td style=text-align:center>9.68</td><td style=text-align:center>18.54</td><td style=text-align:center>1.9</td><td style=text-align:center>3.65</td></tr><tr><td style=text-align:left>Qwen2-Audio</td><td style=text-align:center>1.53</td><td style=text-align:center>3.06</td><td style=text-align:center>7.72</td><td style=text-align:center>8.4</td><td style=text-align:center><b>1.6</b></td><td style=text-align:center>3.6</td></tr><tr><td style=text-align:left>GLM-4-voice Base</td><td style=text-align:center>2.46</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>2.82</td><td style=text-align:center>7.66</td></tr><tr><td style=text-align:left>Baichuan-Omni-1.5</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>6.9</td><td style=text-align:center>8.4</td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center><b>1.18</b></td><td style=text-align:center><b>2.36</b></td><td style=text-align:center><b>5.9</b></td><td style=text-align:center>7.7</td><td style=text-align:center>1.8</td><td style=text-align:center><b>3.4</b></td></tr><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>1.62</td><td style=text-align:center>2.82</td><td style=text-align:center>6.23</td><td style=text-align:center><b>6.9</b></td><td style=text-align:center>2.34</td><td style=text-align:center>5.74</td></tr></tbody></table></div><h3 id=knowledge>Knowledge<a hidden class=anchor aria-hidden=true href=#knowledge>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>InfoSeek_H-mean</th><th style=text-align:center>InfoSeek_unseen_question</th><th style=text-align:center>InfoSeek_unseen_entity</th></tr></thead><tbody><tr><td style=text-align:left>GPT-4o</td><td style=text-align:center><b>36.05</b></td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>PaLI-X</td><td style=text-align:center>22.06</td><td style=text-align:center>23.5</td><td style=text-align:center>20.8</td></tr><tr><td style=text-align:left>Qwen2.5-vl-32B</td><td style=text-align:center>19.35</td><td style=text-align:center>20.55</td><td style=text-align:center>18.28</td></tr><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>27.3</td><td style=text-align:center>28.9</td><td style=text-align:center>25.9</td></tr></tbody></table></div><h3 id=ocrgui>OCR&GUI<a hidden class=anchor aria-hidden=true href=#ocrgui>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Ming-Lite-Omni-Preview</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>ChartQA_TEST</td><td style=text-align:center>85.2</td><td style=text-align:center><b>87.3</b></td></tr><tr><td style=text-align:left>DocVQA_TEST</td><td style=text-align:center>93.2</td><td style=text-align:center><b>95.7</b></td></tr><tr><td style=text-align:left>OCRBenchV2_en/zh</td><td style=text-align:center>52.2/51.6</td><td style=text-align:center><b>56.3/57.2</b></td></tr><tr><td style=text-align:left>OmniDocBench↓</td><td style=text-align:center>34.7/34.5</td><td style=text-align:center><b>30.8/39.8</b></td></tr><tr><td style=text-align:left>TextVQA_VAL</td><td style=text-align:center>82.36</td><td style=text-align:center><b>84.9</b></td></tr><tr><td style=text-align:left>ScreenSpot</td><td style=text-align:center>79.3</td><td style=text-align:center><b>84.7</b></td></tr></tbody></table></div><h2 id=模型下载>模型下载<a hidden class=anchor aria-hidden=true href=#模型下载>#</a></h2><p>你可以从 Huggingface 和 ModelScope 两个平台下载本模型。</p><div align=center><table><thead><tr><th style=text-align:left><strong>Model</strong></th><th style=text-align:center><strong>Input modality</strong></th><th style=text-align:center><strong>Oput modality</strong></th><th style=text-align:center><strong>Download</strong></th></tr></thead><tbody><tr><td style=text-align:left>Ming-Lite-Omni-Preview</td><td style=text-align:center>Image,text,viedio,audio</td><td style=text-align:center>Image,text,audio</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni/tree/250504>🤗 HuggingFace</a><br><a href="https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni/files?version=250504">🤖 ModelScope</a></td></tr></tbody></table></div>如果你在中国大陆，强烈建议你通过以下平台下载模型： 🤖 <a href=https://modelscope.cn/organization/inclusionAI>ModelScope</a>.<h2 id=使用案例>使用案例<a hidden class=anchor aria-hidden=true href=#使用案例>#</a></h2><h3 id=视频音频问答>视频音频问答<a hidden class=anchor aria-hidden=true href=#视频音频问答>#</a></h3><table><thead><tr><th>MultiModal Input</th><th>QA</th></tr></thead><tbody><tr><td><video src=https://github.com/user-attachments/assets/a1327779-030a-44d0-a073-bbc1abe04efc controls width=70% height=auto></video></td><td>Q: <a href=./figures/cases/audioqa_audio.wav>&lt;audio></a> (audio content: 请描述视频内容。)<br>A: The video features a woman performing a series of yoga poses on a rooftop with a scenic view of mountains and a clear blue sky.</td></tr><tr><td><video src=https://github.com/user-attachments/assets/bdeb43ce-9048-4dc1-897c-aa1d7b6f3836 controls width=70% height=auto></video></td><td>Q: Is there any food in front of me?<br>A: Yes, there&rsquo;s candy on the table.</td></tr></tbody></table><h3 id=语音转语音支持方言>语音转语音（支持方言）<a hidden class=anchor aria-hidden=true href=#语音转语音支持方言>#</a></h3><p><video src=https://github.com/user-attachments/assets/842e3e18-ee4a-47ea-ba92-a009be5cf2a3 controls width=70% height=auto></video></p><h2 id=快速上手>快速上手<a hidden class=anchor aria-hidden=true href=#快速上手>#</a></h2><p>Please download our model following <a href=#model-downloads>Model Downloads</a>, then you can refer to the following codes to run Ming-Lite-Omni-Preview model.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoProcessor</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>modeling_bailingmm</span> <span class=kn>import</span> <span class=n>BailingMMNativeForConditionalGeneration</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BailingMMNativeForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>low_cpu_mem_usage</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>assets_path</span> <span class=o>=</span> <span class=n>YOUR_ASSETS_PATH</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build processor</span>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;请详细介绍鹦鹉的生活习性。&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 鹦鹉是一种非常聪明和社交性强的鸟类，它们的生活习性非常丰富和有趣。以下是一些关于鹦鹉生活习性的详细介绍：</span>
</span></span><span class=line><span class=cl><span class=c1># ### 1. **栖息地**</span>
</span></span><span class=line><span class=cl><span class=c1># 鹦鹉主要分布在热带和亚热带地区，包括非洲、亚洲、澳大利亚和南美洲。它们通常生活在森林、草原、沙漠和城市环境中。不同种类的鹦鹉对栖息地的要求有所不同，但大多数鹦鹉喜欢有丰富植被和水源的地方。</span>
</span></span><span class=line><span class=cl><span class=c1># ### 2. **饮食**</span>
</span></span><span class=line><span class=cl><span class=c1># 鹦鹉是杂食性动物，它们的饮食非常多样化。它们的食物包括种子、坚果、水果、蔬菜、花蜜和昆虫。鹦鹉的喙非常强壮，能够轻松地打开坚硬的果壳和坚果。一些鹦鹉还会吃泥土或沙子，以帮助消化和补充矿物质。</span>
</span></span><span class=line><span class=cl><span class=c1># ......</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># image qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span> <span class=s2>&#34;image&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>assets_path</span><span class=p>,</span> <span class=s2>&#34;flowers.jpg&#34;</span><span class=p>)},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What kind of flower is this?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The flowers in this image are forget-me-nots. These delicate blooms are known for their small, five-petaled flowers that come in various shades of blue, pink, and white.</span>
</span></span></code></pre></div><p>To enable thinking before response, adding the following system prompt before your question:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=n>cot_prompt</span> <span class=o>=</span> <span class=s2>&#34;SYSTEM: You are a helpful assistant. When the user asks a question, your response must include two parts: first, the reasoning process enclosed in &lt;thinking&gt;...&lt;/thinking&gt; tags, then the final answer enclosed in &lt;answer&gt;...&lt;/answer&gt; tags. The critical answer or key result should be placed within </span><span class=se>\\</span><span class=s2>boxed</span><span class=si>{}</span><span class=s2>.</span><span class=se>\n</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl><span class=c1># And your input message should be like this:</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;image&#34;</span><span class=p>,</span> <span class=s2>&#34;image&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>assets_path</span><span class=p>,</span> <span class=s2>&#34;reasoning.png&#34;</span><span class=p>)},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=n>cot_prompt</span> <span class=o>+</span> <span class=s2>&#34;In the rectangle $A B C D$ pictured, $M_</span><span class=si>{1}</span><span class=s2>$ is the midpoint of $D C, M_</span><span class=si>{2}</span><span class=s2>$ the midpoint of $A M_</span><span class=si>{1}</span><span class=s2>, M_</span><span class=si>{3}</span><span class=s2>$ the midpoint of $B M_</span><span class=si>{2}</span><span class=s2>$ and $M_</span><span class=si>{4}</span><span class=s2>$ the midpoint of $C M_</span><span class=si>{3}</span><span class=s2>$. Determine the ratio of the area of the quadrilateral $M_</span><span class=si>{1}</span><span class=s2> M_</span><span class=si>{2}</span><span class=s2> M_</span><span class=si>{3}</span><span class=s2> M_</span><span class=si>{4}</span><span class=s2>$ to the area of the rectangle $A B C D$.</span><span class=se>\n</span><span class=s2>Choices:</span><span class=se>\n</span><span class=s2>(A) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{7}{16}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(B) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{3}{16}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(C) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{7}{32}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(D) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{9}{32}</span><span class=s2>$</span><span class=se>\n</span><span class=s2>(E) $</span><span class=se>\f</span><span class=s2>rac</span><span class=si>{1}{5}</span><span class=s2>$&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl><span class=c1># \&lt;think\&gt;\nOkay, so I have this problem about a rectangle ABCD ... (thinking process omitted) ... So, the correct answer is C.\n\&lt;/think\&gt;\n\&lt;answer\&gt;\\boxed{C}\&lt;/answer\&gt;\n\n</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># video qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;video&#34;</span><span class=p>,</span> <span class=s2>&#34;video&#34;</span><span class=p>:</span> <span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>assets_path</span><span class=p>,</span> <span class=s2>&#34;yoga.mp4&#34;</span><span class=p>)},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;What is the woman doing?&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># The image shows a woman performing a yoga pose on a rooftop. She&#39;s in a dynamic yoga pose, with her arms and legs extended in various positions.</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># multi-turn chat</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;中国的首都是哪里？&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;ASSISTANT&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;北京&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;它的占地面积是多少？有多少常住人口？&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 北京市的总面积约为16,410.54平方公里，常住人口约为21,542,000人。</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># Preparation for inference</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>image_inputs</span><span class=p>,</span> <span class=n>video_inputs</span><span class=p>,</span> <span class=n>audio_inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>process_vision_info</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>images</span><span class=o>=</span><span class=n>image_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>videos</span><span class=o>=</span><span class=n>video_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>audios</span><span class=o>=</span><span class=n>audio_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>inputs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values_videos&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;audio_feats&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># call generate</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>use_cache</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eos_token_id</span><span class=o>=</span><span class=n>processor</span><span class=o>.</span><span class=n>gen_terminator</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids_trimmed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>out_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>in_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>in_ids</span><span class=p>,</span> <span class=n>out_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>output_text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_ids_trimmed</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output_text</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># ASR</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;Please recognize the language of this speech and transcribe it. Format: oral.&#34;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio&#34;</span><span class=p>:</span> <span class=s1>&#39;data/wavs/BAC009S0915W0292.wav&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>)</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=c1># speech2speech</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;audio&#34;</span><span class=p>,</span> <span class=s2>&#34;audio&#34;</span><span class=p>:</span> <span class=s1>&#39;data/wavs/BAC009S0915W0292.wav&#39;</span><span class=p>},</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>outputs</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>speaker</span><span class=o>=</span><span class=s1>&#39;luna&#39;</span><span class=p>,</span> <span class=n>output_audio_path</span><span class=o>=</span><span class=s1>&#39;out.wav&#39;</span><span class=p>,</span> <span class=n>output_audio</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>outputs</span><span class=p>)</span>
</span></span></code></pre></div><h2 id=许可证与法律声明>许可证与法律声明<a hidden class=anchor aria-hidden=true href=#许可证与法律声明>#</a></h2><p>本代码库遵循 <a href=../LICENSE>MIT 协议</a>，法律免责声明见项目根目录下的 <a href=../LEGAL.md>LEGAL.md 文件</a>。</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
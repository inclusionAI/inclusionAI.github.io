<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Omni：一个用于感知与生成的统一多模态模型 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope
介绍 Ming-lite-omni 是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。
📌 更新 [2025.06.12] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。 [2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：Ming-lite-omni-Preview。 主要特性 统一全模态感知：Ming-lite-omni 基于 Ling（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。
统一感知与生成：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。
创新的生成能力：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。
评测 Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ming-omni/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-omni/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-omni/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Ming-Omni：一个用于感知与生成的统一多模态模型"><meta property="og:description" content="GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope
介绍 Ming-lite-omni 是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。
📌 更新 [2025.06.12] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。 [2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：Ming-lite-omni-Preview。 主要特性 统一全模态感知：Ming-lite-omni 基于 Ling（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。
统一感知与生成：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。
创新的生成能力：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。
评测 Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ming-omni/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-06-11T00:00:03+08:00"><meta property="article:modified_time" content="2025-06-11T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Omni：一个用于感知与生成的统一多模态模型"><meta name=twitter:description content="GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope
介绍 Ming-lite-omni 是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。
📌 更新 [2025.06.12] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。 [2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：Ming-lite-omni-Preview。 主要特性 统一全模态感知：Ming-lite-omni 基于 Ling（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。
统一感知与生成：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。
创新的生成能力：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。
评测 Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Omni：一个用于感知与生成的统一多模态模型","item":"https://inclusionai.github.io/zh/blog/ming-omni/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Omni：一个用于感知与生成的统一多模态模型","name":"Ming-Omni：一个用于感知与生成的统一多模态模型","description":"GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope\n介绍 Ming-lite-omni 是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。\n📌 更新 [2025.06.12] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。 [2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：Ming-lite-omni-Preview。 主要特性 统一全模态感知：Ming-lite-omni 基于 Ling（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。\n统一感知与生成：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。\n创新的生成能力：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。\n评测 Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0.","keywords":[],"articleBody":" GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope\n介绍 Ming-lite-omni 是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。\n📌 更新 [2025.06.12] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。 [2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：Ming-lite-omni-Preview。 主要特性 统一全模态感知：Ming-lite-omni 基于 Ling（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。\n统一感知与生成：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。\n创新的生成能力：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。\n评测 Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0.64，优于主流模型如 SDXL。在 FID 指标上，Ming-lite-omni 达到 4.85，刷新了现有方法的最佳水平。\nImage benchmark Benchmarks Ming-lite-omni Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.1 84.4 84.5 HallusionBench 55.0 55.8 51.7 MMBench_TEST_V11 80.8 82.8 82.0 MMMU 56.3 56.6 54.8 MMStar 64.7 65.3 65.2 MMVet 71.3 71.6 68.1 MathVista 71.6 68.1 67.9 OCRBench 88.4 87.8 88.2 Average 71.4 71.5 70.3 Encyclopedia Benchmarks Object Recognition Ming-lite-omni Qwen2.5-VL-7B-Instruct Plants 54.96 47.8 Animals 56.7 50.85 Vehicles 41.91 42.29 Food \u0026 Ingredients 62.28 54.09 Dishes 44.3 39.07 General 91.08 92.42 Average 58.54 54.43 Video benchmark Benchmarks Ming-lite-omni Qwen2.5VL-7B-Instruct VideoMME 67.0 67.3 MVBench 67.7 67.4 Video-MMMU 46.3 47.4 LongVideoBench 56.6 54.7 Average 59.4 59.2 Note: All models are evaluated based on 128 uniformly sampled frames. Audio benchmark SpeechQA Model Average AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.545 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 3.695 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 3.77 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.215 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.21 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-lite-omni 4.34 4.63 4.06 58.84 47.53 61.98 58.36 99.04 ASR Model aishell1 aishell2_android aishell2_ios cv15_zh fleurs_zh wenetspeech_meeting wenetspeech_net librispeech_test_clean librispeech_test_other multilingual_librispeech cv15_en fleurs_en voxpopuli_v1.0_en Ming-lite-omni 1.47 2.55 2.52 6.31 2.96 5.95 5.46 1.44 2.80 4.15 6.89 3.39 5.80 Qwen2.-Omni 1.18 2.75 2.63 5.20 3.00 5.90 7.70 1.80 3.40 7.56 7.60 4.10 5.80 Qwen2-Audio 1.53 2.92 2.92 6.90 7.50 7.16 8.42 1.60 3.60 5.40 8.60 6.90 6.84 Kimi-Audio 0.60 2.64 2.56 7.21 2.69 6.28 5.37 1.28 2.42 5.88 10.31 4.44 7.97 Information-Seeking Benchmark Model InfoSeek_H-mean InfoSeek_unseen_question InfoSeek_unseen_entity GPT-4o 36.05 - - PaLI-X 22.06 23.5 20.8 Qwen2.5-vl-32B 19.35 20.55 18.28 Ming-lite-omni 27.7 30.4 25.4 OCR Model Ming-lite-omni Qwen2.5-VL-7B-Instruct ChartQA_TEST 85.1 87.3 DocVQA_TEST 93 95.7 OCRBenchV2_en/zh 53.3/52 56.3/57.2 OmniDocBench↓ 34/34.4 30.8/39.8 TextVQA_VAL 82.8 84.9 GUI Model Ming-lite-omni InternVL3 8B Qwen2.5-VL-7B-Instruct ScreenSpot 82.1 79.5 78.9* ScreenSpot-V2 84.1 81.4 - AITZ(EM) 66.6 - 57.6* Note: * denotes the reproduced results. Unified Generation Benchmark Model single_object two_object counting colors position color_attr GENEVAL DPGBench FID↓ Ming-lite-omni 0.9875 0.7727 0.6812 0.7872 0.31 0.29 0.64 81.72 4.85 Metaquery-XL - - - - - - 0.61 82.05 6.02 SDv2.1 0.98 0.51 0.44 0.85 0.07 0.17 0.50 68.09 26.96 Emu3-Gen 0.98 0.71 0.34 0.81 0.17 0.21 0.54 80.60 - SDXL 0.98 0.74 0.39 0.85 0.15 0.23 0.55 74.65 8.76 Janus 0.97 0.68 0.30 0.84 0.46 0.42 0.61 79.68 10.10 JanusFlow - - - - - - 0.63 80.09 9.51 Please refer to our technical report for more comprehensive evaluation results.\n模型下载 您可以从 Huggingface 和 ModelScope 两个平台下载模型。\n模型 输入模态 输出模态 下载地址 Ming-Lite-Omni 图像、文本、视频、音频 图像、文本、音频 🤗 HuggingFace 🤖 ModelScope 如果您位于中国大陆，我们强烈建议您从 🤖 ModelScope 下载模型。\n环境准备 Installation with pip pip install -r requirements.txt # for python 3.10 pip install data/matcha_tts-0.0.5.1-cp310-cp310-linux_x86_64.whl # for python 3.8 # pip install data/matcha_tts-0.0.5.1-cp38-cp38-linux_x86_64.whl pip install diffusers==0.33.0 pip install nvidia-cublas-cu12==12.4.5.8 # for H20 GPU Installation with docker You can also initialize the environment by building the docker image. First clone this repository:\ngit clone --depth 1 https://github.com/inclusionAI/Ming.git cd Ming Then build the docker image with the provided Dockerfile in docker/docker-py310-cu121. This step might take a while:\ndocker build -t ming:py310-cu121 docker/docker-py310-cu121 At last, start the container with the current repo directory mounted:\ndocker run -it --gpus all -v \"$(pwd)\":/workspace/Ming ming:py310-cu121 ming:py310-cu121 /bin/bash You can run the model with python interface. You may download the huggingface model in the repo directory first (.../Ming/) or mount the downloaded model path when starting the container.\n使用样例 We provide a step-by-step running example:\nStep 1 - Download the source code\ngit clone https://github.com/inclusionAI/Ming.git cd Ming Step 2 - Download the model weights and create a soft link to the source code directory\nDownload our model following Model Downloads\nmkdir inclusionAI ln -s /path/to/inclusionAI/Ming-Lite-Omni inclusionAI/Ming-Lite-Omni Step 3 - Enter the code directory, you can refer to the following codes to run the Ming-Lite-Omni model.\njupyter notebook cookbook.ipynb We also provide a simple example on the usage of this repo. For detailed usage, please refer to cookbook.ipynb.\nimport torch from transformers import AutoProcessor, GenerationConfig from modeling_bailingmm import BailingMMNativeForConditionalGeneration # load model model = BailingMMNativeForConditionalGeneration.from_pretrained( \"inclusionAI/Ming-Lite-Omni\", torch_dtype=torch.bfloat16, low_cpu_mem_usage=True ).to(\"cuda\") # build processor processor = AutoProcessor.from_pretrained(\"inclusionAI/Ming-Lite-Omni\", trust_remote_code=True) # qa messages = [ { \"role\": \"HUMAN\", \"content\": [ {\"type\": \"text\", \"text\": \"请详细介绍鹦鹉的生活习性。\"} ], }, ] # 1. Format inputs using chat template text = processor.apply_chat_template(messages, add_generation_prompt=True) # 2. Extract vision/audio data image_inputs, video_inputs, audio_inputs = processor.process_vision_info(messages) # 3. Prepare tensor inputs inputs = processor( text=[text], images=image_inputs, videos=video_inputs, audios=audio_inputs, return_tensors=\"pt\", ) inputs = inputs.to(model.device) for k in inputs.keys(): if k == \"pixel_values\" or k == \"pixel_values_videos\" or k == \"audio_feats\": inputs[k] = inputs[k].to(dtype=torch.bfloat16) # 4. Configure generation generation_config = GenerationConfig.from_dict({'no_repeat_ngram_size': 10}) generated_ids = model.generate( **inputs, max_new_tokens=512, use_cache=True, eos_token_id=processor.gen_terminator, generation_config=generation_config, ) generated_ids_trimmed = [ out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids) ] # 5. Decode output output_text = processor.batch_decode( generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False )[0] print(output_text) # Output: # 鹦鹉是一种非常聪明和社交性强的鸟类，它们的生活习性非常丰富和有趣。以下是一些关于鹦鹉生活习性的详细介绍： # ### 1. **栖息地** # 鹦鹉主要分布在热带和亚热带地区，包括非洲、亚洲、澳大利亚和南美洲。它们通常生活在森林、草原、沙漠和城市环境中。不同种类的鹦鹉对栖息地的要求有所不同，但大多数鹦鹉喜欢有丰富植被和水源的地方。 # ### 2. **饮食** # 鹦鹉是杂食性动物，它们的饮食非常多样化。它们的食物包括种子、坚果、水果、蔬菜、花蜜和昆虫。鹦鹉的喙非常强壮，能够轻松地打开坚硬的果壳和坚果。一些鹦鹉还会吃泥土或沙子，以帮助消化和补充矿物质。 # ...... Note: We test the examples on hardware of NVIDIA H800-80GB/H20-96G with CUDA 12.4. Loading inclusionAI/Ming-Lite-Omni in bfloat16 takes about 62G GPU memory.\n许可与法律声明 本代码仓库遵循 MIT 许可证，法律声明见项目根目录下的 LEGAL.md 文件。\n引用 如果您觉得我们的工作对您有帮助，欢迎引用。\n@misc{Mingomni2025, title = {Ming-Omni: A Unified Multimodal Model for Perception and Generation}, author = {Inclusion AI}, year = {2025}, eprint = {2506.09344}, archivePrefix = {arXiv}, url = {https://arxiv.org/abs/2506.09344} } ","wordCount":"936","inLanguage":"zh","datePublished":"2025-06-11T00:00:03+08:00","dateModified":"2025-06-11T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ming-omni/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Omni：一个用于感知与生成的统一多模态模型</h1><div class=post-meta><span title='2025-06-11 00:00:03 +0800 +0800'>2025年6月11日</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;936 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ming-omni/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify class="btn external" target=_blank>GITHUB</a> 📑 <a href=https://arxiv.org/abs/2506.09344>Technical Report</a>｜📖<a href=https://lucaria-academy.github.io/Ming-Omni/>Project Page</a> ｜🤗 <a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>Hugging Face</a>｜ 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni>ModelScope</a></p><h2 id=介绍>介绍<a hidden class=anchor aria-hidden=true href=#介绍>#</a></h2><p>Ming-lite-omni 是 Ming-omni 的轻量版，源自 <a href=https://github.com/inclusionAI/Ling>Ling-lite</a>，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。</p><h2 id=-更新>📌 更新<a hidden class=anchor aria-hidden=true href=#-更新>#</a></h2><ul><li>[2025.06.12] 🔥 我们的<a href=https://arxiv.org/abs/2506.09344>技术报告</a>已公开发布于 arxiv。</li><li>[2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。</li><li>[2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：<a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview>Ming-lite-omni-Preview</a>。</li></ul><h2 id=主要特性>主要特性<a hidden class=anchor aria-hidden=true href=#主要特性>#</a></h2><ul><li><p><strong>统一全模态感知</strong>：Ming-lite-omni 基于 <a href=https://github.com/inclusionAI/Ling>Ling</a>（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。</p></li><li><p><strong>统一感知与生成</strong>：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。</p></li><li><p><strong>创新的生成能力</strong>：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。</p></li></ul><h2 id=评测>评测<a hidden class=anchor aria-hidden=true href=#评测>#</a></h2><p>Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0.64，优于主流模型如 SDXL。在 FID 指标上，Ming-lite-omni 达到 4.85，刷新了现有方法的最佳水平。</p><h3 id=image-benchmark>Image benchmark<a hidden class=anchor aria-hidden=true href=#image-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th><th style=text-align:center>InternVL2.5-8B-MPO</th></tr></thead><tbody><tr><td style=text-align:left>AI2D</td><td style=text-align:center>83.1</td><td style=text-align:center>84.4</td><td style=text-align:center><b>84.5</b></td></tr><tr><td style=text-align:left>HallusionBench</td><td style=text-align:center><b>55.0</b></td><td style=text-align:center>55.8</td><td style=text-align:center>51.7</td></tr><tr><td style=text-align:left>MMBench_TEST_V11</td><td style=text-align:center>80.8</td><td style=text-align:center><b>82.8</b></td><td style=text-align:center>82.0</td></tr><tr><td style=text-align:left>MMMU</td><td style=text-align:center>56.3</td><td style=text-align:center><b>56.6</b></td><td style=text-align:center>54.8</td></tr><tr><td style=text-align:left>MMStar</td><td style=text-align:center>64.7</td><td style=text-align:center>65.3</td><td style=text-align:center><b>65.2</b></td></tr><tr><td style=text-align:left>MMVet</td><td style=text-align:center>71.3</td><td style=text-align:center>71.6</td><td style=text-align:center>68.1</td></tr><tr><td style=text-align:left>MathVista</td><td style=text-align:center><b>71.6</b></td><td style=text-align:center>68.1</td><td style=text-align:center>67.9</td></tr><tr><td style=text-align:left>OCRBench</td><td style=text-align:center><b>88.4</b></td><td style=text-align:center>87.8</td><td style=text-align:center>88.2</td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center>71.4</td><td style=text-align:center><b>71.5</b></td><td style=text-align:center>70.3</td></tr></tbody></table></div><h4 id=encyclopedia-benchmarks>Encyclopedia Benchmarks<a hidden class=anchor aria-hidden=true href=#encyclopedia-benchmarks>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Object Recognition</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>Plants</td><td style=text-align:center><strong>54.96</strong></td><td style=text-align:center>47.8</td></tr><tr><td style=text-align:left>Animals</td><td style=text-align:center><strong>56.7</strong></td><td style=text-align:center>50.85</td></tr><tr><td style=text-align:left>Vehicles</td><td style=text-align:center>41.91</td><td style=text-align:center><strong>42.29</strong></td></tr><tr><td style=text-align:left>Food & Ingredients</td><td style=text-align:center><strong>62.28</strong></td><td style=text-align:center>54.09</td></tr><tr><td style=text-align:left>Dishes</td><td style=text-align:center><strong>44.3</strong></td><td style=text-align:center>39.07</td></tr><tr><td style=text-align:left>General</td><td style=text-align:center>91.08</td><td style=text-align:center><strong>92.42</strong></td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center><strong>58.54</strong></td><td style=text-align:center>54.43</td></tr></tbody></table></div><h3 id=video-benchmark>Video benchmark<a hidden class=anchor aria-hidden=true href=#video-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Benchmarks</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>VideoMME</td><td style=text-align:center>67.0</td><td style=text-align:center><b>67.3</b></td></tr><tr><td style=text-align:left>MVBench</td><td style=text-align:center>67.7</td><td style=text-align:center><b>67.4</b></td></tr><tr><td style=text-align:left>Video-MMMU</td><td style=text-align:center>46.3</td><td style=text-align:center><b>47.4</b></td></tr><tr><td style=text-align:left>LongVideoBench</td><td style=text-align:center>56.6</td><td style=text-align:center>54.7</td></tr><tr><td style=text-align:left>Average</td><td style=text-align:center><b>59.4</b></td><td style=text-align:center>59.2</td></tr></tbody></table></div>Note: All models are evaluated based on 128 uniformly sampled frames.<h3 id=audio-benchmark>Audio benchmark<a hidden class=anchor aria-hidden=true href=#audio-benchmark>#</a></h3><h4 id=speechqa>SpeechQA<a hidden class=anchor aria-hidden=true href=#speechqa>#</a></h4><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Average</th><th style=text-align:center>AlpacaEval</th><th style=text-align:center>CommonEval</th><th style=text-align:center>SD-QA</th><th style=text-align:center>MMSU</th><th style=text-align:center>OpenBookQA</th><th style=text-align:center>IFEval</th><th style=text-align:center>AdvBench</th></tr></thead><tbody><tr><td style=text-align:left>Qwen2-Audio-chat</td><td style=text-align:center>3.545</td><td style=text-align:center>3.69</td><td style=text-align:center>3.40</td><td style=text-align:center>35.35</td><td style=text-align:center>35.43</td><td style=text-align:center>49.01</td><td style=text-align:center>22.57</td><td style=text-align:center>98.85</td></tr><tr><td style=text-align:left>Baichuan-Audio</td><td style=text-align:center>3.695</td><td style=text-align:center>4.00</td><td style=text-align:center>3.39</td><td style=text-align:center>49.64</td><td style=text-align:center>48.80</td><td style=text-align:center>63.30</td><td style=text-align:center>41.32</td><td style=text-align:center>86.73</td></tr><tr><td style=text-align:left>GLM-4-Voice</td><td style=text-align:center>3.77</td><td style=text-align:center>4.06</td><td style=text-align:center>3.48</td><td style=text-align:center>43.31</td><td style=text-align:center>40.11</td><td style=text-align:center>52.97</td><td style=text-align:center>24.91</td><td style=text-align:center>88.08</td></tr><tr><td style=text-align:left>Kimi-Audio</td><td style=text-align:center>4.215</td><td style=text-align:center>4.46</td><td style=text-align:center>3.97</td><td style=text-align:center><b>63.12</b></td><td style=text-align:center><b>62.17</b></td><td style=text-align:center><b>83.52</b></td><td style=text-align:center><b>61.10</b></td><td style=text-align:center><b>100.00</b></td></tr><tr><td style=text-align:left>Qwen2.5-Omni</td><td style=text-align:center>4.21</td><td style=text-align:center>4.49</td><td style=text-align:center>3.93</td><td style=text-align:center>55.71</td><td style=text-align:center>61.32</td><td style=text-align:center>81.10</td><td style=text-align:center>52.87</td><td style=text-align:center>99.42</td></tr><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center><b>4.34</b></td><td style=text-align:center><b>4.63</b></td><td style=text-align:center><b>4.06</b></td><td style=text-align:center>58.84</td><td style=text-align:center>47.53</td><td style=text-align:center>61.98</td><td style=text-align:center>58.36</td><td style=text-align:center>99.04</td></tr></tbody></table></div><h4 id=asr>ASR<a hidden class=anchor aria-hidden=true href=#asr>#</a></h4><div align=center><table><thead><tr><th style=text-align:center>Model</th><th style=text-align:center>aishell1</th><th style=text-align:center>aishell2_android</th><th style=text-align:center>aishell2_ios</th><th style=text-align:center>cv15_zh</th><th style=text-align:center>fleurs_zh</th><th style=text-align:center>wenetspeech_meeting</th><th style=text-align:center>wenetspeech_net</th><th style=text-align:center>librispeech_test_clean</th><th style=text-align:center>librispeech_test_other</th><th style=text-align:center>multilingual_librispeech</th><th style=text-align:center>cv15_en</th><th style=text-align:center>fleurs_en</th><th style=text-align:center>voxpopuli_v1.0_en</th></tr></thead><tbody><tr><td style=text-align:center>Ming-lite-omni</td><td style=text-align:center>1.47</td><td style=text-align:center><strong>2.55</strong></td><td style=text-align:center><strong>2.52</strong></td><td style=text-align:center>6.31</td><td style=text-align:center>2.96</td><td style=text-align:center>5.95</td><td style=text-align:center>5.46</td><td style=text-align:center>1.44</td><td style=text-align:center>2.80</td><td style=text-align:center><strong>4.15</strong></td><td style=text-align:center><strong>6.89</strong></td><td style=text-align:center><strong>3.39</strong></td><td style=text-align:center><strong>5.80</strong></td></tr><tr><td style=text-align:center>Qwen2.-Omni</td><td style=text-align:center>1.18</td><td style=text-align:center>2.75</td><td style=text-align:center>2.63</td><td style=text-align:center><strong>5.20</strong></td><td style=text-align:center>3.00</td><td style=text-align:center><strong>5.90</strong></td><td style=text-align:center>7.70</td><td style=text-align:center>1.80</td><td style=text-align:center>3.40</td><td style=text-align:center>7.56</td><td style=text-align:center>7.60</td><td style=text-align:center>4.10</td><td style=text-align:center><strong>5.80</strong></td></tr><tr><td style=text-align:center>Qwen2-Audio</td><td style=text-align:center>1.53</td><td style=text-align:center>2.92</td><td style=text-align:center>2.92</td><td style=text-align:center>6.90</td><td style=text-align:center>7.50</td><td style=text-align:center>7.16</td><td style=text-align:center>8.42</td><td style=text-align:center>1.60</td><td style=text-align:center>3.60</td><td style=text-align:center>5.40</td><td style=text-align:center>8.60</td><td style=text-align:center>6.90</td><td style=text-align:center>6.84</td></tr><tr><td style=text-align:center>Kimi-Audio</td><td style=text-align:center><strong>0.60</strong></td><td style=text-align:center>2.64</td><td style=text-align:center>2.56</td><td style=text-align:center>7.21</td><td style=text-align:center><strong>2.69</strong></td><td style=text-align:center>6.28</td><td style=text-align:center><strong>5.37</strong></td><td style=text-align:center><strong>1.28</strong></td><td style=text-align:center><strong>2.42</strong></td><td style=text-align:center>5.88</td><td style=text-align:center>10.31</td><td style=text-align:center>4.44</td><td style=text-align:center>7.97</td></tr></tbody></table></div><h3 id=information-seeking-benchmark>Information-Seeking Benchmark<a hidden class=anchor aria-hidden=true href=#information-seeking-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>InfoSeek_H-mean</th><th style=text-align:center>InfoSeek_unseen_question</th><th style=text-align:center>InfoSeek_unseen_entity</th></tr></thead><tbody><tr><td style=text-align:left>GPT-4o</td><td style=text-align:center><b>36.05</b></td><td style=text-align:center>-</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>PaLI-X</td><td style=text-align:center>22.06</td><td style=text-align:center>23.5</td><td style=text-align:center>20.8</td></tr><tr><td style=text-align:left>Qwen2.5-vl-32B</td><td style=text-align:center>19.35</td><td style=text-align:center>20.55</td><td style=text-align:center>18.28</td></tr><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center>27.7</td><td style=text-align:center><strong>30.4</strong></td><td style=text-align:center><strong>25.4</strong></td></tr></tbody></table></div><h3 id=ocr>OCR<a hidden class=anchor aria-hidden=true href=#ocr>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>ChartQA_TEST</td><td style=text-align:center>85.1</td><td style=text-align:center><b>87.3</b></td></tr><tr><td style=text-align:left>DocVQA_TEST</td><td style=text-align:center>93</td><td style=text-align:center><b>95.7</b></td></tr><tr><td style=text-align:left>OCRBenchV2_en/zh</td><td style=text-align:center>53.3/52</td><td style=text-align:center><b>56.3/57.2</b></td></tr><tr><td style=text-align:left>OmniDocBench↓</td><td style=text-align:center>34/<b>34.4</b></td><td style=text-align:center><b>30.8</b>/39.8</td></tr><tr><td style=text-align:left>TextVQA_VAL</td><td style=text-align:center>82.8</td><td style=text-align:center><b>84.9</b></td></tr></tbody></table></div><h3 id=gui>GUI<a hidden class=anchor aria-hidden=true href=#gui>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>Ming-lite-omni</th><th style=text-align:center>InternVL3 8B</th><th style=text-align:center>Qwen2.5-VL-7B-Instruct</th></tr></thead><tbody><tr><td style=text-align:left>ScreenSpot</td><td style=text-align:center><b>82.1</b></td><td style=text-align:center>79.5</td><td style=text-align:center>78.9*</td></tr><tr><td style=text-align:left>ScreenSpot-V2</td><td style=text-align:center><b>84.1</b></td><td style=text-align:center>81.4</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>AITZ(EM)</td><td style=text-align:center><b>66.6</b></td><td style=text-align:center>-</td><td style=text-align:center>57.6*</td></tr></tbody></table></div>Note: * denotes the reproduced results.<h3 id=unified-generation-benchmark>Unified Generation Benchmark<a hidden class=anchor aria-hidden=true href=#unified-generation-benchmark>#</a></h3><div align=center><table><thead><tr><th style=text-align:left>Model</th><th style=text-align:center>single_object</th><th style=text-align:center>two_object</th><th style=text-align:center>counting</th><th style=text-align:center>colors</th><th style=text-align:center>position</th><th style=text-align:center>color_attr</th><th style=text-align:center>GENEVAL</th><th style=text-align:center>DPGBench</th><th style=text-align:center>FID↓</th></tr></thead><tbody><tr><td style=text-align:left>Ming-lite-omni</td><td style=text-align:center><strong>0.9875</strong></td><td style=text-align:center><strong>0.7727</strong></td><td style=text-align:center><strong>0.6812</strong></td><td style=text-align:center>0.7872</td><td style=text-align:center>0.31</td><td style=text-align:center>0.29</td><td style=text-align:center><strong>0.64</strong></td><td style=text-align:center>81.72</td><td style=text-align:center><strong>4.85</strong></td></tr><tr><td style=text-align:left>Metaquery-XL</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>0.61</td><td style=text-align:center><strong>82.05</strong></td><td style=text-align:center>6.02</td></tr><tr><td style=text-align:left>SDv2.1</td><td style=text-align:center>0.98</td><td style=text-align:center>0.51</td><td style=text-align:center>0.44</td><td style=text-align:center><strong>0.85</strong></td><td style=text-align:center>0.07</td><td style=text-align:center>0.17</td><td style=text-align:center>0.50</td><td style=text-align:center>68.09</td><td style=text-align:center>26.96</td></tr><tr><td style=text-align:left>Emu3-Gen</td><td style=text-align:center>0.98</td><td style=text-align:center>0.71</td><td style=text-align:center>0.34</td><td style=text-align:center>0.81</td><td style=text-align:center>0.17</td><td style=text-align:center>0.21</td><td style=text-align:center>0.54</td><td style=text-align:center>80.60</td><td style=text-align:center>-</td></tr><tr><td style=text-align:left>SDXL</td><td style=text-align:center>0.98</td><td style=text-align:center>0.74</td><td style=text-align:center>0.39</td><td style=text-align:center><strong>0.85</strong></td><td style=text-align:center>0.15</td><td style=text-align:center>0.23</td><td style=text-align:center>0.55</td><td style=text-align:center>74.65</td><td style=text-align:center>8.76</td></tr><tr><td style=text-align:left>Janus</td><td style=text-align:center>0.97</td><td style=text-align:center>0.68</td><td style=text-align:center>0.30</td><td style=text-align:center>0.84</td><td style=text-align:center><strong>0.46</strong></td><td style=text-align:center><strong>0.42</strong></td><td style=text-align:center>0.61</td><td style=text-align:center>79.68</td><td style=text-align:center>10.10</td></tr><tr><td style=text-align:left>JanusFlow</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>-</td><td style=text-align:center>0.63</td><td style=text-align:center>80.09</td><td style=text-align:center>9.51</td></tr></tbody></table></div><p>Please refer to our technical report for more comprehensive evaluation results.</p><h2 id=模型下载>模型下载<a hidden class=anchor aria-hidden=true href=#模型下载>#</a></h2><p>您可以从 Huggingface 和 ModelScope 两个平台下载模型。</p><div align=center><table><thead><tr><th style=text-align:left><strong>模型</strong></th><th style=text-align:center><strong>输入模态</strong></th><th style=text-align:center><strong>输出模态</strong></th><th style=text-align:center><strong>下载地址</strong></th></tr></thead><tbody><tr><td style=text-align:left>Ming-Lite-Omni</td><td style=text-align:center>图像、文本、视频、音频</td><td style=text-align:center>图像、文本、音频</td><td style=text-align:center><a href=https://huggingface.co/inclusionAI/Ming-Lite-Omni>🤗 HuggingFace</a><br><a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni>🤖 ModelScope</a></td></tr></tbody></table></div><p>如果您位于中国大陆，我们强烈建议您从 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-Lite-Omni>ModelScope</a> 下载模型。</p><h2 id=环境准备>环境准备<a hidden class=anchor aria-hidden=true href=#环境准备>#</a></h2><h3 id=installation-with-pip>Installation with pip<a hidden class=anchor aria-hidden=true href=#installation-with-pip>#</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>pip install -r requirements.txt
</span></span><span class=line><span class=cl><span class=c1># for python 3.10</span>
</span></span><span class=line><span class=cl>pip install data/matcha_tts-0.0.5.1-cp310-cp310-linux_x86_64.whl 
</span></span><span class=line><span class=cl><span class=c1># for python 3.8 </span>
</span></span><span class=line><span class=cl><span class=c1># pip install data/matcha_tts-0.0.5.1-cp38-cp38-linux_x86_64.whl</span>
</span></span><span class=line><span class=cl>pip install <span class=nv>diffusers</span><span class=o>==</span>0.33.0
</span></span><span class=line><span class=cl>pip install nvidia-cublas-cu12<span class=o>==</span>12.4.5.8  <span class=c1># for H20 GPU</span>
</span></span></code></pre></div><h3 id=installation-with-docker>Installation with docker<a hidden class=anchor aria-hidden=true href=#installation-with-docker>#</a></h3><p>You can also initialize the environment by building the docker image. First clone this repository:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>git clone --depth <span class=m>1</span> https://github.com/inclusionAI/Ming.git
</span></span><span class=line><span class=cl><span class=nb>cd</span> Ming
</span></span></code></pre></div><p>Then build the docker image with the provided Dockerfile in <code>docker/docker-py310-cu121</code>. This step might take a while:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker build -t ming:py310-cu121 docker/docker-py310-cu121
</span></span></code></pre></div><p>At last, start the container with the current repo directory mounted:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>docker run -it --gpus all -v <span class=s2>&#34;</span><span class=k>$(</span><span class=nb>pwd</span><span class=k>)</span><span class=s2>&#34;</span>:/workspace/Ming ming:py310-cu121 ming:py310-cu121 /bin/bash
</span></span></code></pre></div><p>You can run the model with python interface. You may download the huggingface model in the repo directory first (<code>.../Ming/</code>) or mount the downloaded model path when starting the container.</p><h2 id=使用样例>使用样例<a hidden class=anchor aria-hidden=true href=#使用样例>#</a></h2><p>We provide a step-by-step running example:</p><p>Step 1 - Download the source code</p><pre tabindex=0><code>git clone https://github.com/inclusionAI/Ming.git 
cd Ming
</code></pre><p>Step 2 - Download the model weights and create a soft link to the source code directory</p><p>Download our model following <a href=#model-downloads>Model Downloads</a></p><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>mkdir inclusionAI 
</span></span><span class=line><span class=cl>ln -s /path/to/inclusionAI/Ming-Lite-Omni inclusionAI/Ming-Lite-Omni
</span></span></code></pre></div><p>Step 3 - Enter the code directory, you can refer to the following codes to run the Ming-Lite-Omni model.</p><pre tabindex=0><code>jupyter notebook cookbook.ipynb
</code></pre><p>We also provide a simple example on the usage of this repo. For detailed usage, please refer to <a href=cookbook.ipynb>cookbook.ipynb</a>.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>transformers</span> <span class=kn>import</span> <span class=n>AutoProcessor</span><span class=p>,</span> <span class=n>GenerationConfig</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>modeling_bailingmm</span> <span class=kn>import</span> <span class=n>BailingMMNativeForConditionalGeneration</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># load model</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>BailingMMNativeForConditionalGeneration</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>torch_dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>low_cpu_mem_usage</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=s2>&#34;cuda&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># build processor</span>
</span></span><span class=line><span class=cl><span class=n>processor</span> <span class=o>=</span> <span class=n>AutoProcessor</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&#34;inclusionAI/Ming-Lite-Omni&#34;</span><span class=p>,</span> <span class=n>trust_remote_code</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># qa</span>
</span></span><span class=line><span class=cl><span class=n>messages</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>    <span class=p>{</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;role&#34;</span><span class=p>:</span> <span class=s2>&#34;HUMAN&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=s2>&#34;content&#34;</span><span class=p>:</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>            <span class=p>{</span><span class=s2>&#34;type&#34;</span><span class=p>:</span> <span class=s2>&#34;text&#34;</span><span class=p>,</span> <span class=s2>&#34;text&#34;</span><span class=p>:</span> <span class=s2>&#34;请详细介绍鹦鹉的生活习性。&#34;</span><span class=p>}</span>
</span></span><span class=line><span class=cl>        <span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=p>},</span>
</span></span><span class=line><span class=cl><span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 1. Format inputs using chat template</span>
</span></span><span class=line><span class=cl><span class=n>text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>apply_chat_template</span><span class=p>(</span><span class=n>messages</span><span class=p>,</span> <span class=n>add_generation_prompt</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 2. Extract vision/audio data</span>
</span></span><span class=line><span class=cl><span class=n>image_inputs</span><span class=p>,</span> <span class=n>video_inputs</span><span class=p>,</span> <span class=n>audio_inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>process_vision_info</span><span class=p>(</span><span class=n>messages</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 3. Prepare tensor inputs</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>processor</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>text</span><span class=o>=</span><span class=p>[</span><span class=n>text</span><span class=p>],</span>
</span></span><span class=line><span class=cl>    <span class=n>images</span><span class=o>=</span><span class=n>image_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>videos</span><span class=o>=</span><span class=n>video_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>audios</span><span class=o>=</span><span class=n>audio_inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>return_tensors</span><span class=o>=</span><span class=s2>&#34;pt&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>inputs</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=k>for</span> <span class=n>k</span> <span class=ow>in</span> <span class=n>inputs</span><span class=o>.</span><span class=n>keys</span><span class=p>():</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;pixel_values_videos&#34;</span> <span class=ow>or</span> <span class=n>k</span> <span class=o>==</span> <span class=s2>&#34;audio_feats&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span> <span class=o>=</span> <span class=n>inputs</span><span class=p>[</span><span class=n>k</span><span class=p>]</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 4. Configure generation</span>
</span></span><span class=line><span class=cl><span class=n>generation_config</span> <span class=o>=</span> <span class=n>GenerationConfig</span><span class=o>.</span><span class=n>from_dict</span><span class=p>({</span><span class=s1>&#39;no_repeat_ngram_size&#39;</span><span class=p>:</span> <span class=mi>10</span><span class=p>})</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>generate</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=o>**</span><span class=n>inputs</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>max_new_tokens</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>use_cache</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>eos_token_id</span><span class=o>=</span><span class=n>processor</span><span class=o>.</span><span class=n>gen_terminator</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>generation_config</span><span class=o>=</span><span class=n>generation_config</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>generated_ids_trimmed</span> <span class=o>=</span> <span class=p>[</span>
</span></span><span class=line><span class=cl>        <span class=n>out_ids</span><span class=p>[</span><span class=nb>len</span><span class=p>(</span><span class=n>in_ids</span><span class=p>):]</span> <span class=k>for</span> <span class=n>in_ids</span><span class=p>,</span> <span class=n>out_ids</span> <span class=ow>in</span> <span class=nb>zip</span><span class=p>(</span><span class=n>inputs</span><span class=o>.</span><span class=n>input_ids</span><span class=p>,</span> <span class=n>generated_ids</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=p>]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 5. Decode output</span>
</span></span><span class=line><span class=cl><span class=n>output_text</span> <span class=o>=</span> <span class=n>processor</span><span class=o>.</span><span class=n>batch_decode</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>generated_ids_trimmed</span><span class=p>,</span> <span class=n>skip_special_tokens</span><span class=o>=</span><span class=kc>True</span><span class=p>,</span> <span class=n>clean_up_tokenization_spaces</span><span class=o>=</span><span class=kc>False</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>output_text</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=c1># Output:</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1># 鹦鹉是一种非常聪明和社交性强的鸟类，它们的生活习性非常丰富和有趣。以下是一些关于鹦鹉生活习性的详细介绍：</span>
</span></span><span class=line><span class=cl><span class=c1># ### 1. **栖息地**</span>
</span></span><span class=line><span class=cl><span class=c1># 鹦鹉主要分布在热带和亚热带地区，包括非洲、亚洲、澳大利亚和南美洲。它们通常生活在森林、草原、沙漠和城市环境中。不同种类的鹦鹉对栖息地的要求有所不同，但大多数鹦鹉喜欢有丰富植被和水源的地方。</span>
</span></span><span class=line><span class=cl><span class=c1># ### 2. **饮食**</span>
</span></span><span class=line><span class=cl><span class=c1># 鹦鹉是杂食性动物，它们的饮食非常多样化。它们的食物包括种子、坚果、水果、蔬菜、花蜜和昆虫。鹦鹉的喙非常强壮，能够轻松地打开坚硬的果壳和坚果。一些鹦鹉还会吃泥土或沙子，以帮助消化和补充矿物质。</span>
</span></span><span class=line><span class=cl><span class=c1># ......</span>
</span></span></code></pre></div><p>Note: We test the examples on hardware of NVIDIA H800-80GB/H20-96G with CUDA 12.4. Loading inclusionAI/Ming-Lite-Omni in bfloat16 takes about 62G GPU memory.</p><h2 id=许可与法律声明>许可与法律声明<a hidden class=anchor aria-hidden=true href=#许可与法律声明>#</a></h2><p>本代码仓库遵循 <a href=./LICENSE>MIT 许可证</a>，法律声明见项目根目录下的 <a href=./LEGAL.md>LEGAL.md 文件</a>。</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><p>如果您觉得我们的工作对您有帮助，欢迎引用。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>Mingomni2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>title</span>  <span class=p>=</span> <span class=s>{Ming-Omni: A Unified Multimodal Model for Perception and Generation}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>      <span class=na>author</span> <span class=p>=</span> <span class=s>{Inclusion AI}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>year</span> <span class=p>=</span> <span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>eprint</span> <span class=p>=</span> <span class=s>{2506.09344}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>archivePrefix</span> <span class=p>=</span> <span class=s>{arXiv}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>url</span> <span class=p>=</span> <span class=s>{https://arxiv.org/abs/2506.09344}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
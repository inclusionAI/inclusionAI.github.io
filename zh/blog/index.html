<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态</h2></header><div class=entry-content><p>GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
全模态 Ming-omni 系列更新！Ming-flash-omni-Preview 是首个参数规模达到千亿的开源全模态大模型。基于 Ling 2.0 的稀疏 MoE 架构，Ming-flash-omni-Preview 总参数 103B， 激活 9B。相比之前很受欢迎的 Ming-lite-omni-1.5，Ming-flash-omni-Preview 在全模态理解和生成能力上均有提升，各模态总体效果达到开源全模态模型的领先水平, 尤其在可控图像生成、流式视频理解、以及语音识别等领域性能表现尤为突出。
能力一览 可控图像生成 针对图像生成这个常见的场景，Ming-flash-omni-Preview 首创生成式分割范式 ，将 “图像分割” 重构为语义保持的编辑任务 (Generative Segmentation-as-Editing)，实现了细粒度的空间语义控制。Ming-flash-omni-Preview 在 GenEval 基准上评测达到 0.90 分，超越所有非强化学习的生成方法，展现出卓越的可控性。 流式视频理解 用户常有一种想跟 AI 基于现实场景持续对话，并通过 AI 来理解现实场景的需求。Ming-flash-omni-Preview 可以有效实现相关需求。如下图视频所示，Ming-flash-omni-Preview 可实现对流式视频的细粒度理解，看懂视频中的物体和交互，并实时提供相关理解和说明，帮助用户在实际场景中获得支持。 语音及方言理解 Ming-flash-omni-Preview 可实现上下文感知语音理解 (ContextASR) 和方言识别，在所有 12 个 ContextASR 子任务上全面 SOTA，对湖南话、闽南话、粤语等 15 种中国方言的理解能力大幅增强，对于在听不懂的方言中迷失的用户，能有效的提供翻译和实时理解支持。 音色克隆 Ming-flash-omni-Preview 的语音生成从离散 tokenizer 升级为连续 tokenizer，显著提升了音色克隆能力，中英文混合发音能力稳定性高，能够有效克隆原本对话的音色到新产生的对话中，seed-tts-zh WER 指标为 0.99，超过 qwen3 omni 和 seed-tts。 模型架构及能力简介 Ming-flash-omni-Preview 的模型结构图:...</p></div><footer class=entry-footer><span title='2025-10-28 00:00:03 +0800 +0800'>2025年10月28日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;170 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-flash-omni-Preview，千亿参数 MoE，洞察与创造一体的全模态" href=https://inclusionai.github.io/zh/blog/ming-flash-omni-preview/></a></article><article class=post-entry><header class=entry-header><h2>Ming-UniAudio: 用于统一表征的联合理解、生成和编辑的语音语言大模型</h2></header><div class=entry-content><p>GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-UniAudio的视频介绍 语音编辑的视频展示 编辑任务的视频demo展示 🚀 技术亮点 首个面向理解与生成任务的统一连续语音分词器: MingTok-Audio 是一种基于 VAE 框架与因果 Transformer 架构的统一连续语音分词器，首个有效融合语义与声学特征的连续语音分词器，通过层次化特征表示与 LLM 形成闭环系统，同时适用于理解与生成任务。 首个采用统一连续分词器、同时支持理解与生成的语音大模型: Ming-UniAudio 是端到端的统一语音语言模型，仅用一个 LLM 主干即可同时完成理解与生成，并配备扩散头以保证高保真语音合成。 首个无需时序约束、支持语义与声学任务的通用自由形式语音编辑模型: 我们提出了首个指令引导的自由形式语音编辑框架，无需显式指定编辑区域即可实现全面的语义与声学编辑；同时发布 Ming-Freeform-Audio-Edit，首个面向该任务的开源评测集。 首个自由形式语音编辑基准: 我们推出 Audio-Edit-Benchmark，首个开源自由形式评测集，涵盖四类语义与五类声学编辑任务，用于系统评估模型的编辑能力。 多项指令引导的自由形式语音编辑的任务展示 语义编辑 - 插入 Instruction Transcription Target Transcription Before Edit Speechedit Result insert ‘简直’ after the character or word at index 8. 真是个浪漫的邂逅可以说是英雄救美了 真是个浪漫的邂逅简直可以说是英雄救美了 insert ‘真正’ before the character or word ‘好’. 就有道而正焉可谓好学也已 就有道而正焉可谓真正好学也已 insert ‘clearly’ before the character or word at index 8....</p></div><footer class=entry-footer><span title='2025-10-01 00:00:03 +0800 +0800'>2025年10月1日</span>&nbsp;·&nbsp;6 分钟&nbsp;·&nbsp;1237 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-UniAudio: 用于统一表征的联合理解、生成和编辑的语音语言大模型" href=https://inclusionai.github.io/zh/blog/ming-uniaudio/></a></article><article class=post-entry><header class=entry-header><h2>Ming-UniVision：在连续的视觉世界里，统一理解与生成</h2></header><div class=entry-content><p>GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-UniVision：在连续的视觉世界里，统一理解与生成 🚀 技术亮点 业界首个连续统一的视觉令牌化器： MingTok 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。 首个采用连续视觉 Token 的 NTP 式自回归 MLLM： 基于 MingTok，Ming-UniVision 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。 缓解表征竞争 → 实现 3.5 倍收敛加速： 统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。 单一特征空间内的多轮上下文学习： 所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。 挑战：‘看’与‘画’的逆向天性 自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉理解（看懂图像）与视觉生成（画出图像）也纳入这个统一的序列预测框架。
然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。
理解： 像素 → 高维、抽象的语义概念 生成： 概念 → 精细、高保真的像素细节 这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。
为何现有方案存在不足 现有模型尝试通过两种有限的策略来统一它们：
非对称设计： 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。 共享离散令牌： 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。 我们的解决方案：Ming-UniVision 与 MingTok 为了打破这一僵局，我们推出了 Ming-UniVision，一个构建于颠覆性创新 MingTok 之上的新一代自回归视觉语言模型。
MingTok 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。
核心设计：三段式架构，调和表征竞争 Ming-UniVision 的核心是 MingTok 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。
图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3....</p></div><footer class=entry-footer><span title='2025-10-01 00:00:03 +0800 +0800'>2025年10月1日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;177 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-UniVision：在连续的视觉世界里，统一理解与生成" href=https://inclusionai.github.io/zh/blog/mingtok/></a></article><article class=post-entry><header class=entry-header><h2>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂</h2></header><div class=entry-content><p>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。
长久以来，我们追求着一个宏大目标：构建一个统一的多模态模型，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。
但现实却不尽人意。理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。
这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。
今天，我们想分享一个令人兴奋的发现。我们找到了这样一种催化剂，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，将经典的分割任务，重新定义为一次图像编辑，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。
困局：16%的分割得分与失控的生成 在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：生成式分割。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。
结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 16% 上下。
我们分析，根本原因在于数据分布的巨大鸿沟。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。
我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。
灵感迸发：让分割“穿上色彩的外衣” 我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？
答案显然是后者。
我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是将分割任务转换成一个色彩编辑任务。
例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。
这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。
对“理解”的促进：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。 对“创造”的释放：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。 “左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。
效果惊人：从16%到72.4%，以及更可控的编辑能力 当我们用这种新方法重新训练模型后，结果超出了所有人的预期。
1. SoTA级别的分割能力 首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 72.4%！这是一个超过 350% 的相对提升。
指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。
在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。
在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：
模型类别 模型名称 RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist
(专用视觉分割模型) VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67....</p></div><footer class=entry-footer><span title='2025-09-13 00:00:03 +0800 +0800'>2025年9月13日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;145 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to 编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂" href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/></a></article><article class=post-entry><header class=entry-header><h2>欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡</h2></header><div class=entry-content><p>📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope
概述 我们推出了Ring-lite-2507，该模型是在我们之前发布的轻量级推理模型Ring-lite-2506上的一次全面升级！Ring-lite-2507是一个激活参数为2.75B，总参数为16.8B的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，Ring-lite-2507相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。
我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。
亮点
🚀 多项任务中的卓越表现: Ring-lite-2507在推理和通用任务上均表现出卓越的性能； 🔥 仅激活2.75B模型参数: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数； ⛓️‍💥 算法-系统协同设计: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性； 🔍 公开可用: 我们的训练数据和模型权重均已公开。 模型评测 我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 & 智能体，以及对齐任务。
知识理解 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MMLU-Pro (EM) 72.50 63.44 72.56 GPQA-Diamond (Pass@1) 69.35 63.51 62.00 SuperGPQA (EM) 40.05 13.97 40.36 Phybench (Pass@1) 28.51 29.19 22.14 数学 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MATH-500 (Pass@1) 97.95 96.80 97.30 CNMO 2024 (Pass@1) 75....</p></div><footer class=entry-footer><span title='2025-08-05 00:00:03 +0800 +0800'>2025年8月5日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;359 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to 欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡" href=https://inclusionai.github.io/zh/blog/ring-lite-2507/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://inclusionai.github.io/zh/blog/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
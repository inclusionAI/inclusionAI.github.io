<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero-background style="background:linear-gradient(-225deg,#2CD8D5 0%,#6B8DD6 48%,#8E37D7 100%)"></div><div class="hero text-light"><h1 class=post-title>Blog<sup class=title-translation>&nbsp;&nbsp;[<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/>English</a></li></ul>]</sup></h1></div></div><main class=main><article class=post-entry><header class=entry-header><h2>Ming-Lite-Omni V1.5 介绍</h2></header><div class=entry-content><p>GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
前言 本次发布的 Ming-lite-omni V1.5 是对 Ming-lite-omni (🤗Hugging Face) 全模态能力的一次全面升级, 在包括图文理解、文档理解、视频理解、语音理解和合成、图像生成和编辑等任务上均有明显提升。Ming-lite-omni V1.5 基于Ling-lite-1.5 构建，总参数20.3B, MoE部分激活参数为3B，在各模态基准测试中取得较好的成绩。下面是我们本次更新在部分重要指标和模型架构上的提升的展示。
性能对比图
模型架构图
详细介绍 为了实现这样的提升，我们将自研方案与学术界/开源社区的最新进展相结合，在以下几个部分做了有效尝试，并取得多个重要结论。
图像/语音生成
图像生成侧采用双分支解耦策略提升模型对参考图的可学习参数量。具体来说，在图像进入 DiT 之前，我们使用不同的权重对参考图像与噪声图像分别进行处理，并增加额外两层transformer layers作为refiner进一步增强这一效果。 为了解决图像编辑时的人物ID及场景ID一致性问题，我们新增了ID & Scene Consistency Loss，增大目标图编辑区域的权重、增大参考图非编辑区域的参考强度、降低参考图编辑区域的参考强度。 引入感知增强策略。通过优化结构感知能力，如分割和关键点检测，提升模型对画面细节和空间关系的理解，增强编辑和生成过程的结构可控性，显著提高评测指标中与位置、结构、数量相关的得分，详见 表A。 引入多任务协同学习策略。通过联合训练链路实现生成与编辑的相互促进，将分割任务转化为彩色上色编辑任务，显著提升分割指标和图像局部编辑的精度与可控性，使编辑区域边缘更光滑。 语音生成解码器方面，我们实现了全新的音频解码器，直接接受来自LLM的输出特征实现上下文感知。 语音生成效率方面，为了提高韵律性能和实时生成能力，我们将离散的Audio codec token进行BPE编码，使得音频帧率降低了35%。 全方位数据升级 获取高质量人物图像数据，标准包括：图像分辨率/质量、人脸细粒度、人脸大小等。 采集名人数据，并做质量筛选和人脸裁剪获取名人图像数据。 构建边缘图、分割图、文字图、人物表情图等训练子集，扩充模型能力边界。 图像/文本/视频/语音理解
MRoPE 时空感知位置编码。引入了MRoPE，通过时间、高度、宽度三维分块位置编码，赋予模型时空感知能力，实现高效跨模态联合建模，提升对视频、复杂图像场景的理解精度。 高效全参数训练策略。优化学习率与多模态数据配比，将理解阶段需分步冻结/解冻 LLM 的预训练流程，升级为高效全参数训练，训练周期缩短 26.5%，保持性能无损。 针对视频理解任务，采用从短视频到长视频的课程学习策略，逐步提升模型处理长视频的复杂度。 针对复杂文档理解任务，引入 Chain-of-Thought 策略分步骤构建结构化推理路径，有效提升模型对复杂问题的解决能力。 全方位数据升级 预训练阶段 新增文本实体结构化数据，补全图谱盲区。 扩充高质量商品数据，提升通识能力。 指令微调阶段 提升细粒度视觉感知（目标计数/颜色/场景识别）数据精准性。 提高垂类识别（动植物/车辆/食材等）数据深度。 从数据角度优化跨学科复杂图文推理能力。 针对语音理解任务，将领域、主题、语种（包括方言）等信息引入到语音理解任务的指令文本中，增强模型的理解表现，实现对中英文，粤语，四川话，上海话，闽南语等方言的全面支持。 用户偏好对齐
为了保证我们模型的真实使用体验与常用Benchmark上的提升一致，我们自建了体验评测集，在内部进行多模型的人工对抗评分。得益于高质量的对齐偏好数据构建， Ming-lite-omni v1.5 在图文问答的内容准确性（低幻觉率）、相关性、格式美观性以及表述流畅性方面相比领先模型展现出一定优势， Ming-lite-omni v1....</p></div><footer class=entry-footer><span title='2025-07-21 00:00:03 +0800 +0800'>2025年7月21日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;670 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Lite-Omni V1.5 介绍" href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5/></a></article><article class=post-entry><header class=entry-header><h2>M2-Reasoning: 赋予多模态大语言模型统一的通用与空间推理能力</h2></header><div class=entry-content><p>📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope
介绍 我们推出了 M2-Reasoning-7B，一个在通用与空间推理方面都表现卓越的模型。我们的方法融合了两项关键创新：(1) 一个全新的数据管道，生成了29.42万个高质量数据样本（其中16.8万用于冷启动微调，12.62万用于RLVR）。这些数据具有逻辑连贯的推理轨迹，并经过了全面评估。(2) 一种动态多任务训练策略，通过逐步优化来缓解数据间的冲突，并利用针对特定任务的奖励机制来提供定制化的激励信号。通过这种精心筛选的数据与先进训练方法的结合，M2-Reasoning-7B 在8个基准测试中创造了新的业界最佳水平（SOTA），在通用和空间推理领域均展现出卓越的性能。 📌 更新 [2025.07.14] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.07.11] 🔥 M2-Reasoning模型开源: 🤗 Hugging Face、🤖 ModelScope。 主要特性 高质量的数据构建流程：我们设计并实现了一个多阶段的数据合成与筛选流程，能够生成大量的推理数据。 动态多任务训练策略：我们提出了一种高效的训练策略，能够有效应对数据异构性问题。该策略包括逐步动态优化，以缓解不同数据源之间的冲突，以及任务特定的奖励机制，提供定制化的激励信号。 统一的通用与空间推理模型：我们提出了 M2-Reasoning-7B，这是一款专为通用推理与空间推理任务而设计的多模态大语言模型（MLLM）。在8个不同的基准测试中进行的广泛评估表明，借助我们定制的数据和训练流程，M2-Reasoning在通用推理和空间推理领域均取得了新的SOTA成果。 评测 我们在通用推理和空间推理对模型进行了全面评估。我们的评估使用了一组多样化的公开基准测试，这些测试根据它们主要衡量的能力进行分类：
通用推理（数学与逻辑）：为了评估这一能力，我们采用了六项基准测试：MathVista、MathVision、MathVerse、DynaMath、WeMath 和 LogicVista。 Models MathVista MathVision MathVerse DynaMath WeMath LogicVista Avg. (Δ) 基础规模通用模型 InternVL3-8B 70.5 30.0 38.5 25.7 39.5 44.5 41.4 InternVL3-9B 69.0 29.3 37.9 25.1 34.8 49.0 40.8 Qwen2.5-VL-7B 68.1 25.4 41.1 21.8 36.2 47....</p></div><footer class=entry-footer><span title='2025-07-11 00:00:03 +0800 +0800'>2025年7月11日</span>&nbsp;·&nbsp;4 分钟&nbsp;·&nbsp;736 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to M2-Reasoning: 赋予多模态大语言模型统一的通用与空间推理能力" href=https://inclusionai.github.io/zh/blog/m2-reasoning/></a></article><article class=post-entry><header class=entry-header><h2>AWorld：为智能体自我演进提供运行环境</h2></header><div class=entry-content><p>“自我认知：最难的问题不是在有局限的情况下解决问题，而是发现自己的局限性” 目录 最新动态 — 项目最新更新与公告。 简介 — 项目概述与目标。 安装 — 步骤详尽的安装指南。 快速开始 — 使用示例，快速上手。 架构 — 多智能体系统设计解析。 演示 — 项目实际运行演示。 贡献 — 如何参与和贡献代码。 许可证 — 项目授权信息。 最新动态 🦤 [2025/07/07] AWorld 作为运行时现已准备好进行智能体训练。详情请参见自我改进部分。我们在 GAIA 测试中的得分已更新至 77.08。在演示部分了解如何构建 GAIA 运行时。 🦩 [2025/06/19] GAIA 测试分数提升至 72.43，新增本地运行模式，详见 ./README-local.md。 🐳 [2025/05/22] GAIA 评测、MCP 工具、AWorld 及模型现已集成于单一 Docker 镜像，详见 ./README-docker.md，演示视频。 🥳 [2025/05/13] 浏览器场景状态管理升级，视频处理 MCP server 增强，GAIA 验证分数 77.58（Pass@1 = 61.8），继续保持开源框架第一。详见 GAIA 排行榜。 ✨ [2025/04/23] GAIA 基准测试排名第三（69.7 分），Pass@1 = 58.8，开源框架第一。可用 python examples/gaia/run....</p></div><footer class=entry-footer><span title='2025-07-07 00:00:03 +0800 +0800'>2025年7月7日</span>&nbsp;·&nbsp;2 分钟&nbsp;·&nbsp;363 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to AWorld：为智能体自我演进提供运行环境" href=https://inclusionai.github.io/zh/blog/aworld/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Omni：一个用于感知与生成的统一多模态模型</h2></header><div class=entry-content><p>GITHUB 📑 Technical Report｜📖Project Page ｜🤗 Hugging Face｜ 🤖 ModelScope
介绍 Ming-lite-omni 是 Ming-omni 的轻量版，源自 Ling-lite，拥有 28 亿激活参数。Ming-lite-omni 是一个统一的多模态模型，能够处理图像、文本、音频和视频，并在语音和图像生成方面表现出较强能力。Ming-lite-omni 使用专用编码器从不同模态提取 token，然后由 Ling 处理，Ling 是一个 MoE 架构，配备了新提出的模态专用路由器。该设计使单一模型能在统一框架内高效处理和融合多模态输入，从而支持多样化任务，无需使用多个模型、任务专用微调或结构改动。重要的是，Ming-lite-omni 超越传统多模态模型，支持音频和图像生成。通过集成先进的音频解码器实现自然语音，以及利用 Ming-Lite-Uni 实现高质量图像生成，模型还能进行上下文感知聊天、文本转语音及多功能图像编辑。我们的实验结果表明，Ming-lite-omni 在所有模态上的统一感知与生成方面提供了强大解决方案。值得注意的是，Ming-lite-omni 是我们所知首个模态支持与 GPT-4o 匹配的开源模型，且我们发布了全部代码和模型权重，以促进社区进一步研究和发展。
📌 更新 [2025.06.12] 🔥 我们的技术报告已公开发布于 arxiv。 [2025.05.28] 🔥 Ming-lite-omni 官方版本发布，性能更佳并支持图像生成。 [2025.05.04] 🔥 发布 Ming-lite-omni 测试版本：Ming-lite-omni-Preview。 主要特性 统一全模态感知：Ming-lite-omni 基于 Ling（一个 MoE 架构的大语言模型），通过模态专用路由器解决任务冲突，确保来自不同模态的 token 的连贯融合。
统一感知与生成：Ming-lite-omni 实现统一的理解与生成，使模型在生成过程中能解读多模态指令和用户意图，从而提升生成质量并增强多任务使用便利性。
创新的生成能力：Ming-lite-omni 能感知所有模态，同时生成高质量文本、实时语音和生动图像，展现出卓越的跨模态表现，涵盖图像感知、视听交互和图像生成等多样任务。
评测 Ming-lite-omni 在图像感知、视听交互及图像生成任务中均展现出优异的跨模态性能。具体来说，在图像感知任务中，Ming-lite-omni 仅激活 28 亿参数，性能已可与 Qwen2.5-VL-7B 相媲美。它在端到端语音理解和指令执行上表现优于 Qwen2.5-Omni 和 Kimi-Audio。同时支持原生分辨率的图像生成、编辑及风格迁移，GenEval 得分达 0....</p></div><footer class=entry-footer><span title='2025-06-11 00:00:03 +0800 +0800'>2025年6月11日</span>&nbsp;·&nbsp;5 分钟&nbsp;·&nbsp;936 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Omni：一个用于感知与生成的统一多模态模型" href=https://inclusionai.github.io/zh/blog/ming-omni/></a></article><article class=post-entry><header class=entry-header><h2>Ming-Lite-Uni：自然多模态交互统一架构的进展</h2></header><div class=entry-content><p>GITHUB 📑 Technical Report｜🤗 Hugging Face｜🤖 ModelScope 简介 Ming-Lite-Uni 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。
本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的多尺度可学习Token机制与多尺度表示对齐策略。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。
感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！
📌 更新日志 [2025.05.03] 🔥 我们的 技术报告 已在 arXiv 发布 [2025.05.03] 🔥 Ming-Lite-Uni 首个版本正式开源 为什么重要？ Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：
传统方法 Ming-Lite-Uni 的优势 模块化流程
（如 CLIP/SigLIP + 扩散模型） 端到端统一模型
理解与生成无缝融合 离散Token自回归
（视觉定位能力有限） 连续Token空间
原生支持细粒度视觉概念 固定分辨率处理
（上采样会产生伪影） 多尺度自适应
各分辨率下均保持一致的画质 编辑流程分离
（需要手动对齐） 对话驱动控制
自然语言指导像素级编辑 理解瓶颈
（视觉语义错位） 联合表示学习
理解与生成能力相互增强 核心增强点 统一的视觉理解与生成架构：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0....</p></div><footer class=entry-footer><span title='2025-05-07 00:00:03 +0800 +0800'>2025年5月7日</span>&nbsp;·&nbsp;3 分钟&nbsp;·&nbsp;489 字&nbsp;·&nbsp;inclusionAI, Ant Group</footer><a class=entry-link aria-label="post link to Ming-Lite-Uni：自然多模态交互统一架构的进展" href=https://inclusionai.github.io/zh/blog/ming-lite-uni/></a></article><footer class=page-footer><nav class=pagination><a class=next href=https://inclusionai.github.io/zh/blog/page/2/>下一页&nbsp;&nbsp;»</a></nav></footer></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Ming-Lite-Uni：自然多模态交互统一架构的进展 | INCLUSION AI</title><meta name=keywords content><meta name=description content="
        GITHUB 📑 Technical Report｜🤗 Hugging Face｜🤖 ModelScope

简介
Ming-Lite-Uni 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。
本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的多尺度可学习Token机制与多尺度表示对齐策略。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。
感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！
📌 更新日志

[2025.05.03] 🔥 我们的 技术报告 已在 arXiv 发布
[2025.05.03] 🔥 Ming-Lite-Uni 首个版本正式开源

为什么重要？
Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：

  
      
          传统方法
          Ming-Lite-Uni 的优势
      
  
  
      
          模块化流程（如 CLIP/SigLIP + 扩散模型）
          端到端统一模型理解与生成无缝融合
      
      
          离散Token自回归（视觉定位能力有限）
          连续Token空间原生支持细粒度视觉概念
      
      
          固定分辨率处理（上采样会产生伪影）
          多尺度自适应各分辨率下均保持一致的画质
      
      
          编辑流程分离（需要手动对齐）
          对话驱动控制自然语言指导像素级编辑
      
      
          理解瓶颈（视觉语义错位）
          联合表示学习理解与生成能力相互增强
      
  

核心增强点

统一的视觉理解与生成架构：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0.62 的得分，超过 SDXL (0.55)。
多尺度可学习Token：引入4×/8×/16×多尺度的分层Token，分别捕捉图像的整体布局（低分辨率）、物体结构（中分辨率）和细节纹理（高分辨率），GenEval得分提升3.5%。
多尺度表示对齐：设计了尺度一致性损失，通过原生分辨率优化确保各层级表示与最终结果的一致性，图像重建质量提升超过2dB PSNR，GenEval得分提升1.5%。
具备AGI能力的系统：支持“生成城堡 → 添加日落 → 调整视角”等链式指令，响应时间<1秒（RTX 4090测试）。系统支持指令驱动的生成与编辑，并已对齐 GPT-4o（2025年3月行业标杆）。

赋能多模态交互
Ming-Lite-Uni 是统一的多模态理解模型，突破传统NLP与视觉理解范畴，进一步支持图像生成、图像编辑与风格迁移等交互式生成任务。"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ming-lite-uni/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-uni/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-uni/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Ming-Lite-Uni：自然多模态交互统一架构的进展"><meta property="og:description" content="
        GITHUB 📑 Technical Report｜🤗 Hugging Face｜🤖 ModelScope

简介
Ming-Lite-Uni 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。
本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的多尺度可学习Token机制与多尺度表示对齐策略。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。
感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！
📌 更新日志

[2025.05.03] 🔥 我们的 技术报告 已在 arXiv 发布
[2025.05.03] 🔥 Ming-Lite-Uni 首个版本正式开源

为什么重要？
Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：

  
      
          传统方法
          Ming-Lite-Uni 的优势
      
  
  
      
          模块化流程（如 CLIP/SigLIP + 扩散模型）
          端到端统一模型理解与生成无缝融合
      
      
          离散Token自回归（视觉定位能力有限）
          连续Token空间原生支持细粒度视觉概念
      
      
          固定分辨率处理（上采样会产生伪影）
          多尺度自适应各分辨率下均保持一致的画质
      
      
          编辑流程分离（需要手动对齐）
          对话驱动控制自然语言指导像素级编辑
      
      
          理解瓶颈（视觉语义错位）
          联合表示学习理解与生成能力相互增强
      
  

核心增强点

统一的视觉理解与生成架构：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0.62 的得分，超过 SDXL (0.55)。
多尺度可学习Token：引入4×/8×/16×多尺度的分层Token，分别捕捉图像的整体布局（低分辨率）、物体结构（中分辨率）和细节纹理（高分辨率），GenEval得分提升3.5%。
多尺度表示对齐：设计了尺度一致性损失，通过原生分辨率优化确保各层级表示与最终结果的一致性，图像重建质量提升超过2dB PSNR，GenEval得分提升1.5%。
具备AGI能力的系统：支持“生成城堡 → 添加日落 → 调整视角”等链式指令，响应时间<1秒（RTX 4090测试）。系统支持指令驱动的生成与编辑，并已对齐 GPT-4o（2025年3月行业标杆）。

赋能多模态交互
Ming-Lite-Uni 是统一的多模态理解模型，突破传统NLP与视觉理解范畴，进一步支持图像生成、图像编辑与风格迁移等交互式生成任务。"><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ming-lite-uni/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-05-07T00:00:03+08:00"><meta property="article:modified_time" content="2025-05-07T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Ming-Lite-Uni：自然多模态交互统一架构的进展"><meta name=twitter:description content="
        GITHUB 📑 Technical Report｜🤗 Hugging Face｜🤖 ModelScope

简介
Ming-Lite-Uni 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。
本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的多尺度可学习Token机制与多尺度表示对齐策略。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。
感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！
📌 更新日志

[2025.05.03] 🔥 我们的 技术报告 已在 arXiv 发布
[2025.05.03] 🔥 Ming-Lite-Uni 首个版本正式开源

为什么重要？
Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：

  
      
          传统方法
          Ming-Lite-Uni 的优势
      
  
  
      
          模块化流程（如 CLIP/SigLIP + 扩散模型）
          端到端统一模型理解与生成无缝融合
      
      
          离散Token自回归（视觉定位能力有限）
          连续Token空间原生支持细粒度视觉概念
      
      
          固定分辨率处理（上采样会产生伪影）
          多尺度自适应各分辨率下均保持一致的画质
      
      
          编辑流程分离（需要手动对齐）
          对话驱动控制自然语言指导像素级编辑
      
      
          理解瓶颈（视觉语义错位）
          联合表示学习理解与生成能力相互增强
      
  

核心增强点

统一的视觉理解与生成架构：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0.62 的得分，超过 SDXL (0.55)。
多尺度可学习Token：引入4×/8×/16×多尺度的分层Token，分别捕捉图像的整体布局（低分辨率）、物体结构（中分辨率）和细节纹理（高分辨率），GenEval得分提升3.5%。
多尺度表示对齐：设计了尺度一致性损失，通过原生分辨率优化确保各层级表示与最终结果的一致性，图像重建质量提升超过2dB PSNR，GenEval得分提升1.5%。
具备AGI能力的系统：支持“生成城堡 → 添加日落 → 调整视角”等链式指令，响应时间<1秒（RTX 4090测试）。系统支持指令驱动的生成与编辑，并已对齐 GPT-4o（2025年3月行业标杆）。

赋能多模态交互
Ming-Lite-Uni 是统一的多模态理解模型，突破传统NLP与视觉理解范畴，进一步支持图像生成、图像编辑与风格迁移等交互式生成任务。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"Ming-Lite-Uni：自然多模态交互统一架构的进展","item":"https://inclusionai.github.io/zh/blog/ming-lite-uni/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Ming-Lite-Uni：自然多模态交互统一架构的进展","name":"Ming-Lite-Uni：自然多模态交互统一架构的进展","description":" GITHUB 📑 Technical Report｜🤗 Hugging Face｜🤖 ModelScope 简介 Ming-Lite-Uni 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。\n本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的多尺度可学习Token机制与多尺度表示对齐策略。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。\n感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！\n📌 更新日志 [2025.05.03] 🔥 我们的 技术报告 已在 arXiv 发布 [2025.05.03] 🔥 Ming-Lite-Uni 首个版本正式开源 为什么重要？ Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：\n传统方法 Ming-Lite-Uni 的优势 模块化流程\n（如 CLIP/SigLIP + 扩散模型） 端到端统一模型\n理解与生成无缝融合 离散Token自回归\n（视觉定位能力有限） 连续Token空间\n原生支持细粒度视觉概念 固定分辨率处理\n（上采样会产生伪影） 多尺度自适应\n各分辨率下均保持一致的画质 编辑流程分离\n（需要手动对齐） 对话驱动控制\n自然语言指导像素级编辑 理解瓶颈\n（视觉语义错位） 联合表示学习\n理解与生成能力相互增强 核心增强点 统一的视觉理解与生成架构：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0.62 的得分，超过 SDXL (0.55)。 多尺度可学习Token：引入4×/8×/16×多尺度的分层Token，分别捕捉图像的整体布局（低分辨率）、物体结构（中分辨率）和细节纹理（高分辨率），GenEval得分提升3.5%。 多尺度表示对齐：设计了尺度一致性损失，通过原生分辨率优化确保各层级表示与最终结果的一致性，图像重建质量提升超过2dB PSNR，GenEval得分提升1.5%。 具备AGI能力的系统：支持“生成城堡 → 添加日落 → 调整视角”等链式指令，响应时间\u0026lt;1秒（RTX 4090测试）。系统支持指令驱动的生成与编辑，并已对齐 GPT-4o（2025年3月行业标杆）。 赋能多模态交互 Ming-Lite-Uni 是统一的多模态理解模型，突破传统NLP与视觉理解范畴，进一步支持图像生成、图像编辑与风格迁移等交互式生成任务。\n","keywords":[],"articleBody":" GITHUB 📑 Technical Report｜🤗 Hugging Face｜🤖 ModelScope 简介 Ming-Lite-Uni 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。\n本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的多尺度可学习Token机制与多尺度表示对齐策略。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。\n感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！\n📌 更新日志 [2025.05.03] 🔥 我们的 技术报告 已在 arXiv 发布 [2025.05.03] 🔥 Ming-Lite-Uni 首个版本正式开源 为什么重要？ Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：\n传统方法 Ming-Lite-Uni 的优势 模块化流程\n（如 CLIP/SigLIP + 扩散模型） 端到端统一模型\n理解与生成无缝融合 离散Token自回归\n（视觉定位能力有限） 连续Token空间\n原生支持细粒度视觉概念 固定分辨率处理\n（上采样会产生伪影） 多尺度自适应\n各分辨率下均保持一致的画质 编辑流程分离\n（需要手动对齐） 对话驱动控制\n自然语言指导像素级编辑 理解瓶颈\n（视觉语义错位） 联合表示学习\n理解与生成能力相互增强 核心增强点 统一的视觉理解与生成架构：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0.62 的得分，超过 SDXL (0.55)。 多尺度可学习Token：引入4×/8×/16×多尺度的分层Token，分别捕捉图像的整体布局（低分辨率）、物体结构（中分辨率）和细节纹理（高分辨率），GenEval得分提升3.5%。 多尺度表示对齐：设计了尺度一致性损失，通过原生分辨率优化确保各层级表示与最终结果的一致性，图像重建质量提升超过2dB PSNR，GenEval得分提升1.5%。 具备AGI能力的系统：支持“生成城堡 → 添加日落 → 调整视角”等链式指令，响应时间\u003c1秒（RTX 4090测试）。系统支持指令驱动的生成与编辑，并已对齐 GPT-4o（2025年3月行业标杆）。 赋能多模态交互 Ming-Lite-Uni 是统一的多模态理解模型，突破传统NLP与视觉理解范畴，进一步支持图像生成、图像编辑与风格迁移等交互式生成任务。\n模型结构 Ming-Lite-Uni 是面向图像理解与高保真图像生成的统一多模态模型。其将图像表示压缩为连续视觉Token，并与文本Token一同输入自回归Transformer中进行处理；生成部分则由外部训练的扩散模型（SANA）执行，输入为Transformer生成的Token。\nBenchmark 评测 我们使用公开基准对 Ming-Lite-Uni 的多模态理解与文本生成图像能力进行了分别的定量评估。对于多模态理解，我们与传统的图文输入文本输出模型，以及具备视觉生成能力的最新模型进行了对比。对于多模态生成，我们在 GenEval 基准上评估了文本生成图像的表现。详细信息请参考我们的技术报告。\nMultimodal Understanding\nType Model Avg. MMB MMS MMMU MathV Hall AI2D MM-Vet Und. Only LLaVA-72B 68.0 84.5 65.8 56.6 68.4 47.9 86.2 60.6 Qwen2.5-VL-7B 76.2 87.8 71.1 67.9 70.8 58.8 88.2 76.7 Emu3-Chat - 58.5 - 31.6 - - - 37.2 InternVL2.5-78B 75.2 87.5 69.5 70 71.4 57.4 89.1 71.8 DeepSeek-VL2 66.4 81.2 61.0 50.7 59.4 51.5 84.5 60.0 GPT-4o-20241120 (closed) 72.0 84.3 65.1 70.7 59.9 56.2 84.9 74.5 Step-1o (closed) 77.7 87.3 69.3 69.9 74.7 55.8 89.1 82.8 Und. and Gen. TokenFlow-XL - 68.9 - 38.7 - - - 40.7 Janus-Pro-7B - 79.2 - 41.0 - - - 50.0 Ours (Ming-Lite-Uni) 69.7 80.7 60.5 51.2 68.3 51.8 84.5 72.3 Image Generation\nType Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Gen. Only LlamaGen 0.71 0.34 0.21 0.58 0.07 0.04 0.32 SDv2.1 0.98 0.51 0.44 0.85 0.07 0.17 0.50 Emu3-Gen 0.98 0.71 0.34 0.81 0.17 0.21 0.54 SDXL 0.98 0.74 0.39 0.85 0.15 0.23 0.55 DALL-E 3 0.96 0.87 0.47 0.83 0.43 0.45 0.67 SD3-Medium 0.99 0.94 0.72 0.89 0.33 0.60 0.74 Und. and Gen. Show-o 0.95 0.52 0.49 0.82 0.11 0.28 0.53 TokenFlow-XL 0.95 0.60 0.41 0.81 0.16 0.24 0.55 Janus-Pro-1B 0.98 0.82 0.51 0.89 0.65 0.56 0.73 Ours (Ming-Lite-Uni) 0.99 0.76 0.53 0.87 0.26 0.30 0.62 Example Usage System Requirements Python: \u003e= 3.8 PyTorch: \u003e= 2.4.1+cu12.2 (CUDA 12.2 compatible) flash-attn: \u003e= 2.6.3 Installation We recommend installing the following versions to set up your environment using pip:\npip install -r requirements.txt Usage Guided Below is an example of how to load and use the model:\nimport torch import os from Ming_Uni.MingUniInference import Ming_Uni_Inference from Ming_Uni.process import MyProcessor device = torch.cuda.current_device() device = torch.device(device) model_path='../Ming-Lite-Uni/' model = Ming_Uni_Inference(model_path) model.to(torch.bfloat16) model.to(device) model.eval() llm_model=os.path.join(model_path, 'qwen2_5_llm') my_proc=MyProcessor(llm_model) image_file = \"tests/cake.jpg\" prompt = \"add a candle on top of the cake\" inputs = my_proc.process(image_file=image_file, prompt=prompt, device=device) result = model.image_gen_generate(inputs, steps=30, seed=42, cfg=5.0, height=512, width=512)[1] result.save(\"result.png\") For more advanced usage, such as fine-tuning or generating images, refer to the documentation.\n致谢 该项目目前处于早期阶段。尽管一些初步结果令人鼓舞，但要实现理解与生成的无缝整合，还需取得较大进展。代码和模型都需要进一步打磨和优化，因此我们选择将项目开源。欢迎社区贡献力量，共同完善和发展该项目。如果您有任何建议或发现代码中的问题，请通过 Pull Requests 进行贡献。感谢您的支持和关注！\n开放协作 我们开源了 Ming-Lite-Uni，以加速向通用人工智能（AGI）迈进，特点包括：\n📂 完整模型权重与测试代码 🧩 模块化架构，方便扩展 📊 全面基准测试（对比 GPT-4V、SDXL 等） “2025 年 3 月 ChatGPT-4 同步发布图像生成功能，印证了我们关于统一多模态 AI 是下一范式的愿景。”\n联系方式 如果在使用本项目过程中需要帮助或遇到问题，请在 GitHub 提交 issue。\n许可与法律声明 Ming 遵循 MIT 许可证，法律声明见项目根目录下的 LEGAL.md 文件。\n引用 如果您觉得我们的工作对您有帮助，欢迎引用。\n@article{Mingunify2025, title = {Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction}, author = {Inclusion AI, Ant Group}, journal = {arXiv preprint}, year = {2025} } ","wordCount":"489","inLanguage":"zh","datePublished":"2025-05-07T00:00:03+08:00","dateModified":"2025-05-07T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ming-lite-uni/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>Ming-Lite-Uni：自然多模态交互统一架构的进展</h1><div class=post-meta>&lt;span title='2025-05-07 00:00:03 +0800 +0800'>2025年5月7日&lt;/span>&amp;nbsp;·&amp;nbsp;3 分钟&amp;nbsp;·&amp;nbsp;489 字&amp;nbsp;·&amp;nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ming-lite-uni/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p align=left><a href=https://github.com/inclusionAI/Ming/tree/Ming-Lite-Omni-Preview/Ming-unify class="btn external" target=_blank>GITHUB</a> 📑 <a href=https://arxiv.org/abs/2505.02471>Technical Report</a>｜🤗 <a href=https://huggingface.co/inclusionAI/Ming-Lite-Uni>Hugging Face</a>｜🤖 <a href=https://modelscope.cn/models/inclusionAI/Ming-Lite-Uni>ModelScope</a></p><h2 id=简介>简介<a hidden class=anchor aria-hidden=true href=#简介>#</a></h2><p><code>Ming-Lite-Uni</code> 是一个开源的多模态框架，包含一个全新设计的统一视觉生成器，以及一个原生多模态自回归模型，用于整合视觉与语言能力。</p><p>本项目提供了集成 MetaQueries 与 M2-omni 框架的开源实现，并引入了创新性的<strong>多尺度可学习Token机制</strong>与<strong>多尺度表示对齐策略</strong>。Ming-Lite-Uni 利用固定的MLLM与可训练的扩散模型，使原生多模态AR模型不仅支持文本生成图像（text-to-image），还支持基于指令的图像编辑，从而扩展其功能，不再局限于视觉理解。实验结果表明，Ming-Lite-Uni 具备强大的性能表现，并在交互体验上展现出高度流畅性。目前该项目处于alpha阶段，将持续优化中。</p><p>感谢大家的支持与关注！我们正在稳步推进项目，并取得了良好进展，更多更新即将到来，敬请期待！</p><h2 id=-更新日志>📌 更新日志<a hidden class=anchor aria-hidden=true href=#-更新日志>#</a></h2><ul><li>[2025.05.03] 🔥 我们的 <a href=https://arxiv.org/abs/2505.02471>技术报告</a> 已在 arXiv 发布</li><li>[2025.05.03] 🔥 <a href=https://github.com/inclusionAI/Ming>Ming-Lite-Uni</a> 首个版本正式开源</li></ul><h2 id=为什么重要>为什么重要？<a hidden class=anchor aria-hidden=true href=#为什么重要>#</a></h2><p>Ming-Lite-Uni 的统一架构克服了传统方法的根本性局限：</p><table><thead><tr><th>传统方法</th><th>Ming-Lite-Uni 的优势</th></tr></thead><tbody><tr><td><strong>模块化流程</strong><br>（如 CLIP/SigLIP + 扩散模型）</td><td><strong>端到端统一模型</strong><br>理解与生成无缝融合</td></tr><tr><td><strong>离散Token自回归</strong><br>（视觉定位能力有限）</td><td><strong>连续Token空间</strong><br>原生支持细粒度视觉概念</td></tr><tr><td><strong>固定分辨率处理</strong><br>（上采样会产生伪影）</td><td><strong>多尺度自适应</strong><br>各分辨率下均保持一致的画质</td></tr><tr><td><strong>编辑流程分离</strong><br>（需要手动对齐）</td><td><strong>对话驱动控制</strong><br>自然语言指导像素级编辑</td></tr><tr><td><strong>理解瓶颈</strong><br>（视觉语义错位）</td><td><strong>联合表示学习</strong><br>理解与生成能力相互增强</td></tr></tbody></table><h2 id=核心增强点>核心增强点<a hidden class=anchor aria-hidden=true href=#核心增强点>#</a></h2><ul><li><strong>统一的视觉理解与生成架构</strong>：Ming-Lite-Uni 在 OpenCompass 榜单中理解得分达 69.7，优于 DeepSeek-VL2 (66.4)；同时在 GenEval 图像生成基准上取得 0.62 的得分，超过 SDXL (0.55)。</li><li><strong>多尺度可学习Token</strong>：引入4×/8×/16×多尺度的分层Token，分别捕捉图像的整体布局（低分辨率）、物体结构（中分辨率）和细节纹理（高分辨率），GenEval得分提升3.5%。</li><li><strong>多尺度表示对齐</strong>：设计了<strong>尺度一致性损失</strong>，通过原生分辨率优化确保各层级表示与最终结果的一致性，图像重建质量提升超过2dB PSNR，GenEval得分提升1.5%。</li><li><strong>具备AGI能力的系统</strong>：支持“生成城堡 → 添加日落 → 调整视角”等链式指令，响应时间&lt;1秒（RTX 4090测试）。系统支持指令驱动的生成与编辑，并已对齐 GPT-4o（2025年3月行业标杆）。</li></ul><h2 id=赋能多模态交互>赋能多模态交互<a hidden class=anchor aria-hidden=true href=#赋能多模态交互>#</a></h2><p><strong>Ming-Lite-Uni</strong> 是统一的多模态理解模型，突破传统NLP与视觉理解范畴，进一步支持图像生成、图像编辑与风格迁移等交互式生成任务。</p><h2 id=模型结构>模型结构<a hidden class=anchor aria-hidden=true href=#模型结构>#</a></h2><p><strong>Ming-Lite-Uni</strong> 是面向图像理解与高保真图像生成的统一多模态模型。其将图像表示压缩为连续视觉Token，并与文本Token一同输入自回归Transformer中进行处理；生成部分则由外部训练的扩散模型（SANA）执行，输入为Transformer生成的Token。</p><img width=1034 alt=结构图 src=https://github.com/user-attachments/assets/927e090e-7cda-4f32-81de-774466973077><h2 id=benchmark-评测>Benchmark 评测<a hidden class=anchor aria-hidden=true href=#benchmark-评测>#</a></h2><p>我们使用公开基准对 Ming-Lite-Uni 的多模态理解与文本生成图像能力进行了分别的定量评估。对于多模态理解，我们与传统的图文输入文本输出模型，以及具备视觉生成能力的最新模型进行了对比。对于多模态生成，我们在 GenEval 基准上评估了文本生成图像的表现。详细信息请参考我们的技术报告。</p><p><strong>Multimodal Understanding</strong></p><table><thead><tr><th>Type</th><th>Model</th><th>Avg.</th><th>MMB</th><th>MMS</th><th>MMMU</th><th>MathV</th><th>Hall</th><th>AI2D</th><th>MM-Vet</th></tr></thead><tbody><tr><td><strong>Und. Only</strong></td><td>LLaVA-72B</td><td>68.0</td><td>84.5</td><td>65.8</td><td>56.6</td><td>68.4</td><td>47.9</td><td>86.2</td><td>60.6</td></tr><tr><td></td><td>Qwen2.5-VL-7B</td><td>76.2</td><td>87.8</td><td>71.1</td><td>67.9</td><td>70.8</td><td>58.8</td><td>88.2</td><td>76.7</td></tr><tr><td></td><td>Emu3-Chat</td><td>-</td><td>58.5</td><td>-</td><td>31.6</td><td>-</td><td>-</td><td>-</td><td>37.2</td></tr><tr><td></td><td>InternVL2.5-78B</td><td>75.2</td><td>87.5</td><td>69.5</td><td>70</td><td>71.4</td><td>57.4</td><td>89.1</td><td>71.8</td></tr><tr><td></td><td>DeepSeek-VL2</td><td>66.4</td><td>81.2</td><td>61.0</td><td>50.7</td><td>59.4</td><td>51.5</td><td>84.5</td><td>60.0</td></tr><tr><td></td><td>GPT-4o-20241120 (closed)</td><td>72.0</td><td>84.3</td><td>65.1</td><td>70.7</td><td>59.9</td><td>56.2</td><td>84.9</td><td>74.5</td></tr><tr><td></td><td>Step-1o (closed)</td><td>77.7</td><td>87.3</td><td>69.3</td><td>69.9</td><td>74.7</td><td>55.8</td><td>89.1</td><td>82.8</td></tr><tr><td><strong>Und. and Gen.</strong></td><td>TokenFlow-XL</td><td>-</td><td>68.9</td><td>-</td><td>38.7</td><td>-</td><td>-</td><td>-</td><td>40.7</td></tr><tr><td></td><td>Janus-Pro-7B</td><td>-</td><td>79.2</td><td>-</td><td>41.0</td><td>-</td><td>-</td><td>-</td><td>50.0</td></tr><tr><td></td><td><strong>Ours (Ming-Lite-Uni)</strong></td><td>69.7</td><td>80.7</td><td>60.5</td><td>51.2</td><td>68.3</td><td>51.8</td><td>84.5</td><td>72.3</td></tr></tbody></table><p><strong>Image Generation</strong></p><table><thead><tr><th>Type</th><th>Method</th><th>Single Obj.</th><th>Two Obj.</th><th>Counting</th><th>Colors</th><th>Position</th><th>Color Attri.</th><th>Overall</th></tr></thead><tbody><tr><td><strong>Gen. Only</strong></td><td>LlamaGen</td><td>0.71</td><td>0.34</td><td>0.21</td><td>0.58</td><td>0.07</td><td>0.04</td><td>0.32</td></tr><tr><td></td><td>SDv2.1</td><td>0.98</td><td>0.51</td><td>0.44</td><td>0.85</td><td>0.07</td><td>0.17</td><td>0.50</td></tr><tr><td></td><td>Emu3-Gen</td><td>0.98</td><td>0.71</td><td>0.34</td><td>0.81</td><td>0.17</td><td>0.21</td><td>0.54</td></tr><tr><td></td><td>SDXL</td><td>0.98</td><td>0.74</td><td>0.39</td><td>0.85</td><td>0.15</td><td>0.23</td><td>0.55</td></tr><tr><td></td><td>DALL-E 3</td><td>0.96</td><td>0.87</td><td>0.47</td><td>0.83</td><td>0.43</td><td>0.45</td><td>0.67</td></tr><tr><td></td><td>SD3-Medium</td><td>0.99</td><td>0.94</td><td>0.72</td><td>0.89</td><td>0.33</td><td>0.60</td><td>0.74</td></tr><tr><td><strong>Und. and Gen.</strong></td><td>Show-o</td><td>0.95</td><td>0.52</td><td>0.49</td><td>0.82</td><td>0.11</td><td>0.28</td><td>0.53</td></tr><tr><td></td><td>TokenFlow-XL</td><td>0.95</td><td>0.60</td><td>0.41</td><td>0.81</td><td>0.16</td><td>0.24</td><td>0.55</td></tr><tr><td></td><td>Janus-Pro-1B</td><td>0.98</td><td>0.82</td><td>0.51</td><td>0.89</td><td>0.65</td><td>0.56</td><td>0.73</td></tr><tr><td></td><td><strong>Ours (Ming-Lite-Uni)</strong></td><td>0.99</td><td>0.76</td><td>0.53</td><td>0.87</td><td>0.26</td><td>0.30</td><td>0.62</td></tr></tbody></table><h2 id=example-usage>Example Usage<a hidden class=anchor aria-hidden=true href=#example-usage>#</a></h2><h4 id=system-requirements>System Requirements<a hidden class=anchor aria-hidden=true href=#system-requirements>#</a></h4><ul><li><strong>Python:</strong> >= 3.8</li><li><strong>PyTorch:</strong> >= 2.4.1+cu12.2 (CUDA 12.2 compatible)</li><li><strong>flash-attn:</strong> >= 2.6.3</li></ul><h4 id=installation>Installation<a hidden class=anchor aria-hidden=true href=#installation>#</a></h4><p>We recommend installing the following versions to set up your environment using pip:</p><pre tabindex=0><code>pip install -r requirements.txt
</code></pre><ul><li><h3 id=usage-guided>Usage Guided<a hidden class=anchor aria-hidden=true href=#usage-guided>#</a></h3></li></ul><p>Below is an example of how to load and use the model:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>os</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>Ming_Uni.MingUniInference</span> <span class=kn>import</span> <span class=n>Ming_Uni_Inference</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>Ming_Uni.process</span> <span class=kn>import</span> <span class=n>MyProcessor</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cuda</span><span class=o>.</span><span class=n>current_device</span><span class=p>()</span>
</span></span><span class=line><span class=cl><span class=n>device</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>device</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>model_path</span><span class=o>=</span><span class=s1>&#39;../Ming-Lite-Uni/&#39;</span>
</span></span><span class=line><span class=cl><span class=n>model</span> <span class=o>=</span> <span class=n>Ming_Uni_Inference</span><span class=p>(</span><span class=n>model_path</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>bfloat16</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>to</span><span class=p>(</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>model</span><span class=o>.</span><span class=n>eval</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>llm_model</span><span class=o>=</span><span class=n>os</span><span class=o>.</span><span class=n>path</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>model_path</span><span class=p>,</span> <span class=s1>&#39;qwen2_5_llm&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>my_proc</span><span class=o>=</span><span class=n>MyProcessor</span><span class=p>(</span><span class=n>llm_model</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>image_file</span> <span class=o>=</span> <span class=s2>&#34;tests/cake.jpg&#34;</span>
</span></span><span class=line><span class=cl><span class=n>prompt</span> <span class=o>=</span> <span class=s2>&#34;add a candle on top of the cake&#34;</span>
</span></span><span class=line><span class=cl><span class=n>inputs</span> <span class=o>=</span> <span class=n>my_proc</span><span class=o>.</span><span class=n>process</span><span class=p>(</span><span class=n>image_file</span><span class=o>=</span><span class=n>image_file</span><span class=p>,</span> <span class=n>prompt</span><span class=o>=</span><span class=n>prompt</span><span class=p>,</span> <span class=n>device</span><span class=o>=</span><span class=n>device</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>result</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>image_gen_generate</span><span class=p>(</span><span class=n>inputs</span><span class=p>,</span> <span class=n>steps</span><span class=o>=</span><span class=mi>30</span><span class=p>,</span> <span class=n>seed</span><span class=o>=</span><span class=mi>42</span><span class=p>,</span> <span class=n>cfg</span><span class=o>=</span><span class=mf>5.0</span><span class=p>,</span> <span class=n>height</span><span class=o>=</span><span class=mi>512</span><span class=p>,</span> <span class=n>width</span><span class=o>=</span><span class=mi>512</span><span class=p>)[</span><span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=n>result</span><span class=o>.</span><span class=n>save</span><span class=p>(</span><span class=s2>&#34;result.png&#34;</span><span class=p>)</span>
</span></span></code></pre></div><p>For more advanced usage, such as fine-tuning or generating images, refer to the documentation.</p><h2 id=致谢>致谢<a hidden class=anchor aria-hidden=true href=#致谢>#</a></h2><p>该项目目前处于早期阶段。尽管一些初步结果令人鼓舞，但要实现理解与生成的无缝整合，还需取得较大进展。代码和模型都需要进一步打磨和优化，因此我们选择将项目开源。欢迎社区贡献力量，共同完善和发展该项目。如果您有任何建议或发现代码中的问题，请通过 Pull Requests 进行贡献。感谢您的支持和关注！</p><h2 id=开放协作>开放协作<a hidden class=anchor aria-hidden=true href=#开放协作>#</a></h2><p>我们开源了 Ming-Lite-Uni，以加速向通用人工智能（AGI）迈进，特点包括：</p><ul><li>📂 完整模型权重与测试代码</li><li>🧩 模块化架构，方便扩展</li><li>📊 全面基准测试（对比 GPT-4V、SDXL 等）</li></ul><p><em>&ldquo;2025 年 3 月 ChatGPT-4 同步发布图像生成功能，印证了我们关于统一多模态 AI 是下一范式的愿景。&rdquo;</em></p><h2 id=联系方式>联系方式<a hidden class=anchor aria-hidden=true href=#联系方式>#</a></h2><p>如果在使用本项目过程中需要帮助或遇到问题，请在 GitHub 提交 issue。</p><h2 id=许可与法律声明>许可与法律声明<a hidden class=anchor aria-hidden=true href=#许可与法律声明>#</a></h2><p>Ming 遵循 <a href=../LICENSE>MIT 许可证</a>，法律声明见项目根目录下的 <a href=../LEGAL.md>LEGAL.md 文件</a>。</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><p>如果您觉得我们的工作对您有帮助，欢迎引用。</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@article</span><span class=p>{</span><span class=nl>Mingunify2025</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>title</span>   <span class=p>=</span> <span class=s>{Ming-Lite-Uni: Advancements in Unified Architecture for Natural Multimodal Interaction}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>author</span>  <span class=p>=</span> <span class=s>{Inclusion AI, Ant Group}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>journal</span> <span class=p>=</span> <span class=s>{arXiv preprint}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=na>year</span>    <span class=p>=</span> <span class=s>{2025}</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
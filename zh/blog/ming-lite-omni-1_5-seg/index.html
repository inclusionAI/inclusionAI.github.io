<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 | INCLUSION AI</title><meta name=keywords content><meta name=description content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。
长久以来，我们追求着一个宏大目标：构建一个统一的多模态模型，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。
但现实却不尽人意。理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。
这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。
今天，我们想分享一个令人兴奋的发现。我们找到了这样一种催化剂，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，将经典的分割任务，重新定义为一次图像编辑，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。
困局：16%的分割得分与失控的生成 在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：生成式分割。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。
结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 16% 上下。
我们分析，根本原因在于数据分布的巨大鸿沟。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。
我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。
灵感迸发：让分割“穿上色彩的外衣” 我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？
答案显然是后者。
我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是将分割任务转换成一个色彩编辑任务。
例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。
这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。
对“理解”的促进：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。 对“创造”的释放：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。 “左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。
效果惊人：从16%到72.4%，以及更可控的编辑能力 当我们用这种新方法重新训练模型后，结果超出了所有人的预期。
1. SoTA级别的分割能力 首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 72.4%！这是一个超过 350% 的相对提升。
指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。
在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。
在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：
模型类别 模型名称 RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist
(专用视觉分割模型) VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂"><meta property="og:description" content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。
长久以来，我们追求着一个宏大目标：构建一个统一的多模态模型，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。
但现实却不尽人意。理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。
这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。
今天，我们想分享一个令人兴奋的发现。我们找到了这样一种催化剂，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，将经典的分割任务，重新定义为一次图像编辑，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。
困局：16%的分割得分与失控的生成 在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：生成式分割。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。
结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 16% 上下。
我们分析，根本原因在于数据分布的巨大鸿沟。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。
我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。
灵感迸发：让分割“穿上色彩的外衣” 我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？
答案显然是后者。
我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是将分割任务转换成一个色彩编辑任务。
例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。
这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。
对“理解”的促进：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。 对“创造”的释放：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。 “左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。
效果惊人：从16%到72.4%，以及更可控的编辑能力 当我们用这种新方法重新训练模型后，结果超出了所有人的预期。
1. SoTA级别的分割能力 首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 72.4%！这是一个超过 350% 的相对提升。
指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。
在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。
在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：
模型类别 模型名称 RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist
(专用视觉分割模型) VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-09-13T00:00:03+08:00"><meta property="article:modified_time" content="2025-09-13T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂"><meta name=twitter:description content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。
长久以来，我们追求着一个宏大目标：构建一个统一的多模态模型，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。
但现实却不尽人意。理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。
这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。
今天，我们想分享一个令人兴奋的发现。我们找到了这样一种催化剂，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，将经典的分割任务，重新定义为一次图像编辑，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。
困局：16%的分割得分与失控的生成 在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：生成式分割。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。
结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 16% 上下。
我们分析，根本原因在于数据分布的巨大鸿沟。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。
我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。
灵感迸发：让分割“穿上色彩的外衣” 我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？
答案显然是后者。
我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是将分割任务转换成一个色彩编辑任务。
例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。
这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。
对“理解”的促进：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。 对“创造”的释放：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。 “左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。
效果惊人：从16%到72.4%，以及更可控的编辑能力 当我们用这种新方法重新训练模型后，结果超出了所有人的预期。
1. SoTA级别的分割能力 首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 72.4%！这是一个超过 350% 的相对提升。
指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。
在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。
在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：
模型类别 模型名称 RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist
(专用视觉分割模型) VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂","item":"https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂","name":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂","description":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。\n长久以来，我们追求着一个宏大目标：构建一个统一的多模态模型，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。\n但现实却不尽人意。理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。\n这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。\n今天，我们想分享一个令人兴奋的发现。我们找到了这样一种催化剂，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，将经典的分割任务，重新定义为一次图像编辑，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。\n困局：16%的分割得分与失控的生成 在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：生成式分割。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。\n结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 16% 上下。\n我们分析，根本原因在于数据分布的巨大鸿沟。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。\n我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。\n灵感迸发：让分割“穿上色彩的外衣” 我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？\n答案显然是后者。\n我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是将分割任务转换成一个色彩编辑任务。\n例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。\n这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。\n对“理解”的促进：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。 对“创造”的释放：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。 “左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。\n效果惊人：从16%到72.4%，以及更可控的编辑能力 当我们用这种新方法重新训练模型后，结果超出了所有人的预期。\n1. SoTA级别的分割能力 首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 72.4%！这是一个超过 350% 的相对提升。\n指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。\n在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。\n在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：\n模型类别 模型名称 RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist\n(专用视觉分割模型) VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67.","keywords":[],"articleBody":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。\n长久以来，我们追求着一个宏大目标：构建一个统一的多模态模型，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。\n但现实却不尽人意。理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。\n这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。\n今天，我们想分享一个令人兴奋的发现。我们找到了这样一种催化剂，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，将经典的分割任务，重新定义为一次图像编辑，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。\n困局：16%的分割得分与失控的生成 在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：生成式分割。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。\n结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 16% 上下。\n我们分析，根本原因在于数据分布的巨大鸿沟。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。\n我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。\n灵感迸发：让分割“穿上色彩的外衣” 我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？\n答案显然是后者。\n我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是将分割任务转换成一个色彩编辑任务。\n例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。\n这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。\n对“理解”的促进：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。 对“创造”的释放：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。 “左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。\n效果惊人：从16%到72.4%，以及更可控的编辑能力 当我们用这种新方法重新训练模型后，结果超出了所有人的预期。\n1. SoTA级别的分割能力 首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 72.4%！这是一个超过 350% 的相对提升。\n指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。\n在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。\n在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：\n模型类别 模型名称 RefCOCO (val) RefCOCO+ (val) RefCOCOg (val) Vision Specialist\n(专用视觉分割模型) VLT 67.5 56.3 55.0 CRIS 70.5 62.3 59.9 LAVT 72.7 62.1 61.2 PolyFormer-B 74.8 67.6 67.8 MLLM + SAM\n(专用的分割模型) LISA-7B 74.1 62.4 66.4 PixelLM-7B 73.0 66.3 69.3 MLLM + DiT\n(生成式模型做分割) Nano-banana* 15.7 13.9 14.9 Qwen-Image-Edit* 30.3 28.8 34.0 Ming-Lite-Omni1.5 72.4 62.8 64.3 评估结果表明，我们的模型在分割任务中的表现已接近专为分割设计的专业模型。在评估过程中，Qwen-Image-Edit 和Nano-banana 在每个测试子集上随机采样 500 个样本进行测试，以降低计算开销，同时保证结果的统计趋势稳定。评估过程中我们发现，Nano-banana 在推理中经常无法准确把握图像分割的意图，因此评价指标相对较低，这可能与训练目标和数据侧重差异有关。\n2. 更精准、更可控的编辑能力 这个方法的魅力在于，它不仅治好了分割的“短板”，还反过来极大地增强了模型的通用编辑能力。\n因为模型在成千上万次“精确上色”的练习中，学会了对边界前所未有的尊重。这种对细粒度控制的“肌肉记忆”迁移到了所有编辑任务中。我们的编辑精度可控性指标，在背景改变、颜色修改和材质修改等子项上，均分从7.69提升到8.12。\n3. 身份的一致性保持 在人像编辑中，一个核心痛点是身份（ID）一致性。我们的模型在这方面也表现出色。无论是改变发型，还是调整表情，模型都能很好地保持人物的核心特征。\n指令：“头转向左侧”\nQwen（左）的ID、肤色存在不一致现象。 Nano-banana（中）人物额头与背景处的行人均发生了改变。 我们的模型（右）在转动头部的同时，很好地保持了主体和背景的一致性。 指令：“微笑”\nQwen（左）表情变化的同时人物ID也发生了改变。 Nano-banana（中）在换表情的同时手部动作出先畸变。 我们的模型（右）很好地遵循了指令，同时保持了ID一致性。 指令：“变换背景”\nQwen（左）的ID一致性明显下降，看起来像换了一个人。 Nano-banana（中）人物ID保持的不错，但画面结构产生了较大差异。 我们的模型（右）在准确地更换背景的同时，很好地保持了ID、外表的一致性。 更多一致性 Case： 诚实的审视：我们的不足与未来方向 尽管取得了令人鼓舞的进展，但我们深知模型仍有很大的提升空间。特别是在以下几个方面：\n大幅度的动作改变：实现从站立到奔跑这样的大姿态变换，仍然是一个巨大的挑战。 复杂指令的跟随能力：对于包含多个步骤或条件的复杂指令，模型的理解和执行能力还有待加强。 指令多样性的支持：扩展模型能理解和执行的指令类型，是我们下一步的重点工作。 结语：寻找下一个“催化剂” 从16%到72.4%，这个故事的核心并非某个复杂的网络结构或海量的新数据，而是一个关于**“任务设计”**的尝试。\n我们证明了，与其试图用“胶水”把AI的各种能力勉强粘合在一起，不如去寻找或设计那些本身就是“一体两面”的协同任务。这些任务就像催化剂，能让不同的能力在解决同一个问题的过程中，自然而然地相互促进、共同进化。\n“分割即编辑”只是第一个成功的尝试。我们相信，在3D理解、视频生成等更广阔的领域，还隐藏着更多这样的“催化剂”等待我们去发现。\nAI的“左手”与“右手”，终于学会了如何优雅地击掌。而这，仅仅是交响乐的序章。\n欢迎使用开源的 Ming-lite-omni 1.5 GitHub Page / Demo Page。\n","wordCount":"145","inLanguage":"zh","datePublished":"2025-09-13T00:00:03+08:00","dateModified":"2025-09-13T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ming-lite-omni-1_5-seg/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂</h1><div class=post-meta><span title='2025-09-13 00:00:03 +0800 +0800'>2025年9月13日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;145 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ming-lite-omni-1_5-seg/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><h1 id=编辑式图像分割ming-lite-omni-15-破解ai左右互搏的隐藏催化剂>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂<a hidden class=anchor aria-hidden=true href=#编辑式图像分割ming-lite-omni-15-破解ai左右互搏的隐藏催化剂>#</a></h1><p>最近，多模态AI领域风起云涌。从 Qwen-Image 的亮相到 Nano Banana 引发的交互式编辑热潮，图像编辑俨然已是下一个“兵家必争之地”。编辑既要明白“在哪里、是什么、怎么变”（理解图像），又要高质量地创造出结果（生成图像），其丰富的玩法和强交互性，吸引了大量用户和开发者参与讨论。然而，图像编辑除了好玩之外，还有被行业忽略的重要基础价值。</p><p>长久以来，我们追求着一个宏大目标：构建一个<strong>统一的多模态模型</strong>，它既能像科学家一样深刻理解世界（理解能力，如图像分割），又能像艺术家一样自由创造世界（生成能力，如图像编辑）。理想中，这两种能力应相辅相成，形成“理解越深，创造越好；创造越多，理解越透”的良性循环。</p><p>但现实却不尽人意。<strong>理解与生成，如同AI体内的“左手”和“右手”，往往无法协同工作。</strong> 训练模型识别一万张猫的图片，并不会直接提升它画猫的能力，反之亦然。更糟糕的是，在统一模型的训练中，两种任务常常因优化目标不同而陷入“左右互搏”的零和博弈：一次针对理解能力的优化，可能无意中损害了模型的生成质量。</p><p>这意味着，我们缺少一个关键的“催化剂”——一种能够促进“左手”与“右手”协同进化的任务机制。</p><p>今天，我们想分享一个令人兴奋的发现。<strong>我们找到了这样一种催化剂</strong>，一个简单而极其有效的任务转换，它不仅打破了僵局，还使模型的两项核心能力均实现了质的飞跃。这个秘诀就是：在统一模型的训练框架中，<strong>将经典的分割任务，重新定义为一次图像编辑</strong>，不仅让生成式分割能力达到 SOTA，还使编辑一致性实现了飞跃。</p><hr><h2 id=困局16的分割得分与失控的生成>困局：16%的分割得分与失控的生成<a hidden class=anchor aria-hidden=true href=#困局16的分割得分与失控的生成>#</a></h2><p>在找到这个方法之前，我们的统一模型在一个关键任务上举步维艰：<strong>生成式分割</strong>。我们希望模型能根据指令（如“分割出右上角那只香蕉”），直接“画”出分割掩码图。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*acrPSp-7qM8AAAAAgCAAAAgAevzJAQ/original alt=图示说明：根据指令进行分割。></p><p>结果是，模型在 RefCOCO-val 上的推理分割指标（cIoU）顽固地停留在 <strong>16%</strong> 上下。</p><p>我们分析，根本原因在于<strong>数据分布的巨大鸿沟</strong>。生成模型习惯了处理自然、连续的图像数据。而分割任务的目标（黑白掩码图）是一种极度抽象、非自然的数据分布。强迫一个“画家”去画黑白掩码图，无异于缘木求鱼。</p><p>我们意识到，必须找到一个任务，它既能满足“理解之手”对边界精度的要求，又能让“创造之手”在自己熟悉的领域内大展拳脚。</p><hr><h2 id=灵感迸发让分割穿上色彩的外衣>灵感迸发：让分割“穿上色彩的外衣”<a hidden class=anchor aria-hidden=true href=#灵感迸发让分割穿上色彩的外衣>#</a></h2><p>我们的“Ah‑ha moment”来源于一个简单的类比：如果想让孩子准确地圈出一个物体，是让他用铅笔画一个生硬的轮廓更容易，还是让他用彩笔把那个物体涂满颜色更容易？</p><p>答案显然是后者。</p><p>我们将这个想法应用到AI训练中。我们不再让模型生成抽象的黑白掩码，而是<strong>将分割任务转换成一个色彩编辑任务</strong>。</p><p>例如，对于“分割右上角的香蕉”这个指令，我们不再要求模型输出掩码，而是要求它直接在原图上执行一个新的指令：“把右上角的香蕉涂成紫色”、“把右上角的香蕉涂成红色”等等。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*-_O6RLOxXKcAAAAAgBAAAAgAevzJAQ/original alt=图示说明：左侧为原图香蕉，从生成抽象的黑白掩-码（中），到直接在原图上进行色彩编辑（右三）。这个转换让任务的数据分布回归到了自然图像领域。></p><p>这个看似微小的改动，却是那个我们梦寐以求的“催化剂”。</p><ul><li><strong>对“理解”的促进</strong>：为了准确地只给目标香蕉上色而不溢出，模型必须在内部先完成一次完美的、像素级的分割。分割能力从最终目标，变成了完成任务的必要前提。</li><li><strong>对“创造”的释放</strong>：模型不再处理奇怪的掩码图，而是在做它最擅长的事——图像到图像的编辑。它所有的生成能力，如光影、纹理、边缘融合，都能用来把颜色“涂”得更逼真、更准确。</li></ul><p>“左手”和“右手”终于有了一个共同的目标，它们的每一次努力都在互相加强。</p><hr><h2 id=效果惊人从16到724以及更可控的编辑能力>效果惊人：从16%到72.4%，以及更可控的编辑能力<a hidden class=anchor aria-hidden=true href=#效果惊人从16到724以及更可控的编辑能力>#</a></h2><p>当我们用这种新方法重新训练模型后，结果超出了所有人的预期。</p><h3 id=1-sota级别的分割能力>1. SoTA级别的分割能力<a hidden class=anchor aria-hidden=true href=#1-sota级别的分割能力>#</a></h3><p>首先，最直观的变化来自于分割指标。它从之前惨淡的16%，一跃飙升至 <strong>72.4%</strong>！这是一个超过 <strong>350%</strong> 的相对提升。</p><p>指标的背后，是肉眼可见的质变。在处理复杂的推理分割任务时，我们的模型展现出超越竞品的准确性和场景理解力。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*koynTZD5vO8AAAAAgDAAAAgAevzJAQ/original alt=图示说明：我们的模型（右）精准定位并分割了目标主体，Qwen-Image（左二）未能准确定位要分割的目标，Nano-banana（左三）则未能准确分割男士的头部，以及分割的边缘线不够贴合。></p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*5C7KTbk2WZ0AAAAAgBAAAAgAevzJAQ/original alt="图示说明：这个case的指令“please segment the girl with red mask”, 我们的模型（右）精准定位并分割了目标主体，Qwen-Image（左二）未能分割脚部，Nano-banana（左三）则改变了主体尺寸。"></p><p>在“分割女孩”的案例中，Qwen没有包含脚部，而Nano-banana改变了主体尺寸。在“分割拿雨伞的女人”这类需要推理的案例中，我们的模型能准确找到目标，而竞品则出现了主体识别错误或指令理解偏差。这证明，通过“上色”训练，模型的语义理解与视觉定位能力被深度绑定并共同强化了。</p><p>在推理分割指标评估过程中，依托于我们模型在非编辑区域的高度一致性，我们直接通过将涂色编辑结果与原图进行差分计算，获得分割掩码，示例如下：</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*UJX1RJJpu3cAAAAASyAAAAgAevzJAQ/original alt="Ming-Lite-Omni1.5, Qwen-Image-Edit, Nano-banana 差分对比"></p><table><thead><tr><th style=text-align:left>模型类别</th><th style=text-align:left>模型名称</th><th style=text-align:center>RefCOCO (val)</th><th style=text-align:center>RefCOCO+ (val)</th><th style=text-align:center>RefCOCOg (val)</th></tr></thead><tbody><tr><td style=text-align:left><strong>Vision Specialist</strong><br>(专用视觉分割模型)</td><td style=text-align:left>VLT</td><td style=text-align:center>67.5</td><td style=text-align:center>56.3</td><td style=text-align:center>55.0</td></tr><tr><td style=text-align:left></td><td style=text-align:left>CRIS</td><td style=text-align:center>70.5</td><td style=text-align:center>62.3</td><td style=text-align:center>59.9</td></tr><tr><td style=text-align:left></td><td style=text-align:left>LAVT</td><td style=text-align:center>72.7</td><td style=text-align:center>62.1</td><td style=text-align:center>61.2</td></tr><tr><td style=text-align:left></td><td style=text-align:left>PolyFormer-B</td><td style=text-align:center>74.8</td><td style=text-align:center>67.6</td><td style=text-align:center>67.8</td></tr><tr><td style=text-align:left><strong>MLLM + SAM</strong><br>(专用的分割模型)</td><td style=text-align:left>LISA-7B</td><td style=text-align:center>74.1</td><td style=text-align:center>62.4</td><td style=text-align:center>66.4</td></tr><tr><td style=text-align:left></td><td style=text-align:left>PixelLM-7B</td><td style=text-align:center>73.0</td><td style=text-align:center>66.3</td><td style=text-align:center>69.3</td></tr><tr><td style=text-align:left><strong>MLLM + DiT</strong><br>(生成式模型做分割)</td><td style=text-align:left>Nano-banana*</td><td style=text-align:center>15.7</td><td style=text-align:center>13.9</td><td style=text-align:center>14.9</td></tr><tr><td style=text-align:left></td><td style=text-align:left>Qwen-Image-Edit*</td><td style=text-align:center>30.3</td><td style=text-align:center>28.8</td><td style=text-align:center>34.0</td></tr><tr><td style=text-align:left></td><td style=text-align:left><strong>Ming-Lite-Omni1.5</strong></td><td style=text-align:center><strong>72.4</strong></td><td style=text-align:center><strong>62.8</strong></td><td style=text-align:center><strong>64.3</strong></td></tr></tbody></table><p>评估结果表明，我们的模型在分割任务中的表现已接近专为分割设计的专业模型。在评估过程中，Qwen-Image-Edit 和Nano-banana 在每个测试子集上随机采样 500 个样本进行测试，以降低计算开销，同时保证结果的统计趋势稳定。评估过程中我们发现，Nano-banana 在推理中经常无法准确把握图像分割的意图，因此评价指标相对较低，这可能与训练目标和数据侧重差异有关。</p><h3 id=2-更精准更可控的编辑能力>2. 更精准、更可控的编辑能力<a hidden class=anchor aria-hidden=true href=#2-更精准更可控的编辑能力>#</a></h3><p>这个方法的魅力在于，它不仅治好了分割的“短板”，还反过来极大地增强了模型的通用编辑能力。</p><p>因为模型在成千上万次“精确上色”的练习中，学会了对边界前所未有的尊重。这种对细粒度控制的“肌肉记忆”迁移到了所有编辑任务中。我们的编辑精度可控性指标，在背景改变、颜色修改和材质修改等子项上，均分从7.69提升到8.12。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*7PgiRpiJyScAAAAAgCAAAAgAevzJAQ/original alt=图示说明：指令为“消除图中最右侧男士的领结”。我们的模型（右）精准地移除了目标领结，同时保持了背景马匹等元素的一致性。Qwen（左二）错误地移除了多个领结，且马匹和老虎出现了不一致。Nano-banana（左三）同样在衣服款式的一致性和老虎斑纹的一致性上表现不佳。></p><h3 id=3-身份的一致性保持>3. 身份的一致性保持<a hidden class=anchor aria-hidden=true href=#3-身份的一致性保持>#</a></h3><p>在人像编辑中，一个核心痛点是身份（ID）一致性。我们的模型在这方面也表现出色。无论是改变发型，还是调整表情，模型都能很好地保持人物的核心特征。</p><p><strong>指令：“头转向左侧”</strong></p><ul><li>Qwen（左）的ID、肤色存在不一致现象。</li><li>Nano-banana（中）人物额头与背景处的行人均发生了改变。</li><li>我们的模型（右）在转动头部的同时，很好地保持了主体和背景的一致性。</li></ul><p><strong>指令：“微笑”</strong></p><ul><li>Qwen（左）表情变化的同时人物ID也发生了改变。</li><li>Nano-banana（中）在换表情的同时手部动作出先畸变。</li><li>我们的模型（右）很好地遵循了指令，同时保持了ID一致性。</li></ul><p><strong>指令：“变换背景”</strong></p><ul><li>Qwen（左）的ID一致性明显下降，看起来像换了一个人。</li><li>Nano-banana（中）人物ID保持的不错，但画面结构产生了较大差异。</li><li>我们的模型（右）在准确地更换背景的同时，很好地保持了ID、外表的一致性。</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_wp0xz6/afts/img/A*19ULQZrBWIAAAAAAd5AAAAgAevzJAQ/original alt=ID一致性对比图></p><p><strong>更多一致性 Case：</strong>
<video src=https://gw.alipayobjects.com/v/huamei_wp0xz6/afts/video/A*CcqdTbafkt8AAAAAgEAAAAgAevzJAQ width=704px height=740px controls></video></p><hr><h2 id=诚实的审视我们的不足与未来方向>诚实的审视：我们的不足与未来方向<a hidden class=anchor aria-hidden=true href=#诚实的审视我们的不足与未来方向>#</a></h2><p>尽管取得了令人鼓舞的进展，但我们深知模型仍有很大的提升空间。特别是在以下几个方面：</p><ul><li><strong>大幅度的动作改变</strong>：实现从站立到奔跑这样的大姿态变换，仍然是一个巨大的挑战。</li><li><strong>复杂指令的跟随能力</strong>：对于包含多个步骤或条件的复杂指令，模型的理解和执行能力还有待加强。</li><li><strong>指令多样性的支持</strong>：扩展模型能理解和执行的指令类型，是我们下一步的重点工作。</li></ul><hr><h2 id=结语寻找下一个催化剂>结语：寻找下一个“催化剂”<a hidden class=anchor aria-hidden=true href=#结语寻找下一个催化剂>#</a></h2><p>从16%到72.4%，这个故事的核心并非某个复杂的网络结构或海量的新数据，而是一个关于**“任务设计”**的尝试。</p><p>我们证明了，与其试图用“胶水”把AI的各种能力勉强粘合在一起，不如去寻找或设计那些本身就是“一体两面”的协同任务。这些任务就像催化剂，能让不同的能力在解决同一个问题的过程中，自然而然地相互促进、共同进化。</p><p>“分割即编辑”只是第一个成功的尝试。我们相信，在3D理解、视频生成等更广阔的领域，还隐藏着更多这样的“催化剂”等待我们去发现。</p><p><strong>AI的“左手”与“右手”，终于学会了如何优雅地击掌。而这，仅仅是交响乐的序章。</strong></p><p>欢迎使用开源的 <strong>Ming-lite-omni 1.5</strong> <a href=https://github.com/inclusionAI/Ming/blob/main/cookbook.ipynb><strong>GitHub Page / Demo Page</strong></a>。</p></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
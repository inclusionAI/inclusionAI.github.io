<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂 | INCLUSION AI</title><meta name=keywords content><meta name=description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-UniVision：基于共享连续表征的统一图像理解与生成 🚀 技术亮点 业界首个连续统一的视觉令牌化器： MingTok 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。 首个采用连续视觉 Token 的 NTP 式自回归 MLLM： 基于 MingTok，Ming-UniVision 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。 缓解表征竞争 → 实现 3.5 倍收敛加速： 统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。 单一特征空间内的多轮上下文学习： 所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。 挑战：‘看’与‘画’的逆向天性 自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉理解（看懂图像）与视觉生成（画出图像）也纳入这个统一的序列预测框架。
然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。
理解： 像素 → 高维、抽象的语义概念 生成： 概念 → 精细、高保真的像素细节 这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。
为何现有方案存在不足 现有模型尝试通过两种有限的策略来统一它们：
非对称设计： 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。 共享离散令牌： 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。 我们的解决方案：Ming-UniVision 与 MingTok 为了打破这一僵局，我们推出了 Ming-UniVision，一个构建于颠覆性创新 MingTok 之上的新一代自回归视觉语言模型。
MingTok 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。
核心设计：三段式架构，调和表征竞争 Ming-UniVision 的核心是 MingTok 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。
图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3."><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/mingtok/><link crossorigin=anonymous href=/assets/css/stylesheet.fa92d8da4b04eca0dd3a888f432d2e4fabc3dec8bebb65e348f9ce26d866c08e.css integrity="sha256-+pLY2ksE7KDdOoiPQy0uT6vD3si+u2XjSPnOJthmwI4=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/mingtok/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/mingtok/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90",{anonymize_ip:!1})}</script><meta property="og:title" content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂"><meta property="og:description" content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-UniVision：基于共享连续表征的统一图像理解与生成 🚀 技术亮点 业界首个连续统一的视觉令牌化器： MingTok 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。 首个采用连续视觉 Token 的 NTP 式自回归 MLLM： 基于 MingTok，Ming-UniVision 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。 缓解表征竞争 → 实现 3.5 倍收敛加速： 统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。 单一特征空间内的多轮上下文学习： 所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。 挑战：‘看’与‘画’的逆向天性 自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉理解（看懂图像）与视觉生成（画出图像）也纳入这个统一的序列预测框架。
然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。
理解： 像素 → 高维、抽象的语义概念 生成： 概念 → 精细、高保真的像素细节 这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。
为何现有方案存在不足 现有模型尝试通过两种有限的策略来统一它们：
非对称设计： 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。 共享离散令牌： 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。 我们的解决方案：Ming-UniVision 与 MingTok 为了打破这一僵局，我们推出了 Ming-UniVision，一个构建于颠覆性创新 MingTok 之上的新一代自回归视觉语言模型。
MingTok 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。
核心设计：三段式架构，调和表征竞争 Ming-UniVision 的核心是 MingTok 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。
图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3."><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/mingtok/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-09-13T00:00:03+08:00"><meta property="article:modified_time" content="2025-09-13T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂"><meta name=twitter:description content="GITHUB 🤗 Hugging Face｜ 🤖 ModelScope
Ming-UniVision：基于共享连续表征的统一图像理解与生成 🚀 技术亮点 业界首个连续统一的视觉令牌化器： MingTok 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。 首个采用连续视觉 Token 的 NTP 式自回归 MLLM： 基于 MingTok，Ming-UniVision 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。 缓解表征竞争 → 实现 3.5 倍收敛加速： 统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。 单一特征空间内的多轮上下文学习： 所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。 挑战：‘看’与‘画’的逆向天性 自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉理解（看懂图像）与视觉生成（画出图像）也纳入这个统一的序列预测框架。
然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。
理解： 像素 → 高维、抽象的语义概念 生成： 概念 → 精细、高保真的像素细节 这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。
为何现有方案存在不足 现有模型尝试通过两种有限的策略来统一它们：
非对称设计： 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。 共享离散令牌： 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。 我们的解决方案：Ming-UniVision 与 MingTok 为了打破这一僵局，我们推出了 Ming-UniVision，一个构建于颠覆性创新 MingTok 之上的新一代自回归视觉语言模型。
MingTok 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。
核心设计：三段式架构，调和表征竞争 Ming-UniVision 的核心是 MingTok 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。
图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂","item":"https://inclusionai.github.io/zh/blog/mingtok/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂","name":"编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂","description":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\nMing-UniVision：基于共享连续表征的统一图像理解与生成 🚀 技术亮点 业界首个连续统一的视觉令牌化器： MingTok 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。 首个采用连续视觉 Token 的 NTP 式自回归 MLLM： 基于 MingTok，Ming-UniVision 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。 缓解表征竞争 → 实现 3.5 倍收敛加速： 统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。 单一特征空间内的多轮上下文学习： 所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。 挑战：‘看’与‘画’的逆向天性 自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉理解（看懂图像）与视觉生成（画出图像）也纳入这个统一的序列预测框架。\n然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。\n理解： 像素 → 高维、抽象的语义概念 生成： 概念 → 精细、高保真的像素细节 这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。\n为何现有方案存在不足 现有模型尝试通过两种有限的策略来统一它们：\n非对称设计： 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。 共享离散令牌： 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。 我们的解决方案：Ming-UniVision 与 MingTok 为了打破这一僵局，我们推出了 Ming-UniVision，一个构建于颠覆性创新 MingTok 之上的新一代自回归视觉语言模型。\nMingTok 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。\n核心设计：三段式架构，调和表征竞争 Ming-UniVision 的核心是 MingTok 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。\n图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3.","keywords":[],"articleBody":"GITHUB 🤗 Hugging Face｜ 🤖 ModelScope\nMing-UniVision：基于共享连续表征的统一图像理解与生成 🚀 技术亮点 业界首个连续统一的视觉令牌化器： MingTok 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。 首个采用连续视觉 Token 的 NTP 式自回归 MLLM： 基于 MingTok，Ming-UniVision 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。 缓解表征竞争 → 实现 3.5 倍收敛加速： 统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。 单一特征空间内的多轮上下文学习： 所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。 挑战：‘看’与‘画’的逆向天性 自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉理解（看懂图像）与视觉生成（画出图像）也纳入这个统一的序列预测框架。\n然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。\n理解： 像素 → 高维、抽象的语义概念 生成： 概念 → 精细、高保真的像素细节 这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。\n为何现有方案存在不足 现有模型尝试通过两种有限的策略来统一它们：\n非对称设计： 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。 共享离散令牌： 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。 我们的解决方案：Ming-UniVision 与 MingTok 为了打破这一僵局，我们推出了 Ming-UniVision，一个构建于颠覆性创新 MingTok 之上的新一代自回归视觉语言模型。\nMingTok 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。\n核心设计：三段式架构，调和表征竞争 Ming-UniVision 的核心是 MingTok 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。\n图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3.5 倍的训练收敛加速。\n低维编码器 (Low-level Encoder)： 将输入图像映射为一串紧凑、连续的潜码，为高效的自回归生成进行优化。 语义解码器 (Semantic Decoder)： 将潜码自回归地“精炼”为与 CLIP 等顶级理解模型对齐的高维语义特征。 像素解码器 (Pixel Decoder)： 作为质量保证模块，确保可以从语义特征中高保真地重建原始图像，保证表征过程的高保真度。 关键创新： MingTok 创造了一个统一、可微的接口。用于理解的高维特征可以直接作为下一轮生成或编辑任务的条件，彻底消除了代价高昂的“像素空间绕行”。\n突破：效率的根本性飞跃 通过集成 MingTok，Ming-UniVision 在理解和生成任务上均取得了极具竞争力的结果。其共享的连续潜空间从两个层面实现了效率的根本性提升，解决了困扰以往架构的瓶颈。\n图 2：在通用识别任务上，我们的方法性能接近分离表征模型，并显著优于其他统一表征模型。在生成方面，我们的模型在颜色、位置等细粒度控制上表现出明显优势。\n1. 训练效率革命：超过 3.5 倍的收敛加速 传统方法在对齐异构表征时会产生“任务竞争”，拖慢学习速度。MingTok 从根本上解决了这个问题。\n协同增强： 我们的实验证明，统一表征不仅避免了性能权衡，反而促进了理解与生成能力的协同增强。 \u003e3.5倍加速： 由于避免了低效的对齐工作，模型可以将全部精力用于核心任务学习，从而将达到同等性能水平的时间缩短为原来的不到三分之一。 图3：在使用统一的 MingTok 表征进行联合训练时，其性能与纯生成训练的差距极小，证明了统一方案的优越性。\n2. 交互效率革命：告别“像素往返” 多轮交互（如“生成→编辑→再生成”）的效率取决于“理解-生成”循环的速度。这正是传统架构的症结所在。\n架构类型 多轮交互能力 核心瓶颈 交互路径 效率与保真度 DiT-based Models ❌ 原生不支持 非自回归、无状态 不适用 (需完全重启) 低 混合/分离架构 ⚠️ 支持，但低效 空间不统一 潜码 → 像素 → 特征 低、复杂、有信息衰减 Ming-UniVision ✅ 原生且高效 统一连续空间 特征 → 特征 高，且高保真 如上表所示，任何分离式架构都无法摆脱低效的 潜码 → 像素 → 特征 往返宿命。这种“像素绕行”不仅延迟巨大，还会导致上下文信息在多轮传递中不断衰减。\nMing-UniVision 实现了 特征 → 特征 的直接闭环。来自理解任务的高维特征可以直接被下一轮生成任务无缝利用，解锁了真正连贯的多模态序列建模。这使得过去需要多个专用模型才能完成的任务，如今可以在一个统一框架内自然涌现：\n迭代式图像增强： 先执行超分辨率，然后直接在结果之上继续上色或去噪。 生成式思维链： 先执行一个理解任务（如“分割出图中的汽车”），然后直接对该区域应用编辑指令。 图4：“超分→上色”和“分割→编辑”等多轮任务，现在可以在一个无缝的流程中完成。\n理解、生成与编辑，不再是孤立的管道，而是被编织在一场连续的视觉对话之中。\n总结与展望 我们相信，像 MingTok 这样统一、连续的视觉表征，为构建更灵活、更直观、更接近人类认知方式的多模态交互系统开辟了新的可能性。\n我们深知这只是漫长探索中的一步。我们已将代码和初步的模型权重开源，希望能为社区提供一个可用的基石，并激发更多关于统一表征的讨论。我们期待与业界同仁交流学习，共同推动多模态人工智能的发展。\n项目链接 GitHub: [链接地址] 技术报告: [链接地址] 在线 Demo: [链接地址] ","wordCount":"175","inLanguage":"zh","datePublished":"2025-09-13T00:00:03+08:00","dateModified":"2025-09-13T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/mingtok/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;<svg fill="none" shape-rendering="geometricPrecision" stroke="currentcolor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container><div class=hero><h1 class=post-title>编辑式图像分割：Ming-lite-omni 1.5 破解AI“左右互搏”的隐藏催化剂</h1><div class=post-meta><span title='2025-09-13 00:00:03 +0800 +0800'>2025年9月13日</span>&nbsp;·&nbsp;1 分钟&nbsp;·&nbsp;175 字&nbsp;·&nbsp;inclusionAI, Ant Group&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/mingtok/>English</a></li></ul></div></div></div><main class=main><article class=post-single><div class=post-content><p><a href=https://github.com/inclusionAI/Ming class="btn external" target=_blank>GITHUB</a> 🤗 <a href=https://huggingface.co/inclusionAI/Ming-UniVision>Hugging Face</a>｜ 🤖 <a href=https://www.modelscope.cn/models/inclusionAI/Ming-UniVision>ModelScope</a></p><h1 id=ming-univision基于共享连续表征的统一图像理解与生成>Ming-UniVision：基于共享连续表征的统一图像理解与生成<a hidden class=anchor aria-hidden=true href=#ming-univision基于共享连续表征的统一图像理解与生成>#</a></h1><h2 id=video-srchttpsgwalipayobjectscomvhuamei_qlf8jcaftsvideoazbkgtruoxa4aaaaagyaaaagaehi-aq-width768px-height580px-controlsvideo><video src=https://gw.alipayobjects.com/v/huamei_qlf8jc/afts/video/A*ZBkgTruOxA4AAAAAgyAAAAgAehi-AQ width=768px height=580px controls></video><a hidden class=anchor aria-hidden=true href=#video-srchttpsgwalipayobjectscomvhuamei_qlf8jcaftsvideoazbkgtruoxa4aaaaagyaaaagaehi-aq-width768px-height580px-controlsvideo>#</a></h2><h3 id=-技术亮点>🚀 技术亮点<a hidden class=anchor aria-hidden=true href=#-技术亮点>#</a></h3><ol><li><strong>业界首个连续统一的视觉令牌化器：</strong>
<strong>MingTok</strong> 在单一连续潜空间内无缝支持图像理解与生成，彻底消除了量化过程，并有效打通了不同模态。</li><li><strong>首个采用连续视觉 Token 的 NTP 式自回归 MLLM：</strong>
基于 MingTok，<strong>Ming-UniVision</strong> 在一个共享的“下一词元预测 (NTP)”框架下统一了视觉与语言，实现了对多种视觉任务的端到端自回归建模。</li><li><strong>缓解表征竞争 → 实现 3.5 倍收敛加速：</strong>
统一的连续表征协同了语义理解与生成的目标，在不牺牲性能的前提下，显著加速了模型的联合训练过程。</li><li><strong>单一特征空间内的多轮上下文学习：</strong>
所有操作（理解、生成、编辑）均在同一个连续空间内完成，彻底避免了代价高昂的跨空间转换，使得训练与推理过程更简洁、更高效。</li></ol><h2 id=挑战看与画的逆向天性>挑战：‘看’与‘画’的逆向天性<a hidden class=anchor aria-hidden=true href=#挑战看与画的逆向天性>#</a></h2><p>自回归（Autoregression），这种通过“预测下一个 token”来建模世界的强大范式，已经成功统一了语言、音频等多种模态。下一个前沿领域，是将视觉<strong>理解</strong>（看懂图像）与视觉<strong>生成</strong>（画出图像）也纳入这个统一的序列预测框架。</p><p>然而，这一宏伟目标面临一个深层的挑战：在很多方面，理解与生成是互为逆向的任务。</p><ul><li><strong>理解：</strong> 像素 → 高维、抽象的语义概念</li><li><strong>生成：</strong> 概念 → 精细、高保真的像素细节</li></ul><p>这两种任务对底层视觉表征有着截然不同，甚至是相互竞争的偏好。</p><h3 id=为何现有方案存在不足>为何现有方案存在不足<a hidden class=anchor aria-hidden=true href=#为何现有方案存在不足>#</a></h3><p>现有模型尝试通过两种有限的策略来统一它们：</p><ol><li><strong>非对称设计：</strong> 为每个任务使用不同的、异构的特征空间。这导致在多轮交互中，模型必须在不同空间之间进行低效的“往返”，从而引入延迟和工程复杂性。</li><li><strong>共享离散令牌：</strong> 统一了令牌空间，但引入了量化误差。这既损害了生成图像的保真度，也削弱了其理解能力。</li></ol><h3 id=我们的解决方案ming-univision-与-mingtok>我们的解决方案：Ming-UniVision 与 MingTok<a hidden class=anchor aria-hidden=true href=#我们的解决方案ming-univision-与-mingtok>#</a></h3><p>为了打破这一僵局，我们推出了 <strong>Ming-UniVision</strong>，一个构建于颠覆性创新 <strong>MingTok</strong> 之上的新一代自回归视觉语言模型。</p><p><strong>MingTok</strong> 是首个基于连续潜空间的视觉令牌化器。它提供了一个真正统一且高效的表征，构成了 Ming-UniVision 统一“下一词元预测 (NTP)”框架的基石——在一个统一的上下文学习多模态闭环中，将图像理解、生成和编辑融为一体。</p><h2 id=核心设计三段式架构调和表征竞争>核心设计：三段式架构，调和表征竞争<a hidden class=anchor aria-hidden=true href=#核心设计三段式架构调和表征竞争>#</a></h2><p>Ming-UniVision 的核心是 <strong>MingTok</strong> 令牌化器，它是一个三段式序列架构，旨在优雅地调和理解与生成对表征的竞争性需求。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*VVx0SJQR5K4AAAAARBAAAAgAehi-AQ/original alt="Figure 1: Architecture Comparison">
<em>图1：(a) 现有模型使用分离的视觉表征。(b) MingTok 使用统一方案生成语义与生成表征。(c) 这种统一方法带来了超过 3.5 倍的训练收敛加速。</em></p><ol><li><strong>低维编码器 (Low-level Encoder)：</strong> 将输入图像映射为一串紧凑、连续的潜码，为高效的自回归生成进行优化。</li><li><strong>语义解码器 (Semantic Decoder)：</strong> 将潜码自回归地“精炼”为与 CLIP 等顶级理解模型对齐的高维语义特征。</li><li><strong>像素解码器 (Pixel Decoder)：</strong> 作为质量保证模块，确保可以从语义特征中高保真地重建原始图像，保证表征过程的高保真度。</li></ol><blockquote><p><strong>关键创新：</strong> MingTok 创造了一个统一、可微的接口。用于理解的高维特征可以直接作为下一轮生成或编辑任务的条件，<strong>彻底消除了代价高昂的“像素空间绕行”</strong>。</p></blockquote><h2 id=突破效率的根本性飞跃>突破：效率的根本性飞跃<a hidden class=anchor aria-hidden=true href=#突破效率的根本性飞跃>#</a></h2><p>通过集成 MingTok，Ming-UniVision 在理解和生成任务上均取得了极具竞争力的结果。其共享的连续潜空间从两个层面实现了效率的根本性提升，解决了困扰以往架构的瓶颈。</p><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*oi4-RqyoAvIAAAAARPAAAAgAehi-AQ/original alt="Figure 2: Benchmark Results">
<em>图 2：在通用识别任务上，我们的方法性能接近分离表征模型，并显著优于其他统一表征模型。在生成方面，我们的模型在颜色、位置等细粒度控制上表现出明显优势。</em></p><h3 id=1-训练效率革命超过-35-倍的收敛加速>1. 训练效率革命：超过 3.5 倍的收敛加速<a hidden class=anchor aria-hidden=true href=#1-训练效率革命超过-35-倍的收敛加速>#</a></h3><p>传统方法在对齐异构表征时会产生“任务竞争”，拖慢学习速度。MingTok 从根本上解决了这个问题。</p><ul><li><strong>协同增强：</strong> 我们的实验证明，统一表征不仅避免了性能权衡，反而促进了理解与生成能力的协同增强。</li><li><strong>>3.5倍加速：</strong> 由于避免了低效的对齐工作，模型可以将全部精力用于核心任务学习，从而将达到同等性能水平的时间缩短为原来的不到三分之一。</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*dkPxS4hNZx8AAAAARAAAAAgAehi-AQ/original alt="Figure 3: Pre-training Performance">
<em>图3：在使用统一的 MingTok 表征进行联合训练时，其性能与纯生成训练的差距极小，证明了统一方案的优越性。</em></p><h3 id=2-交互效率革命告别像素往返>2. 交互效率革命：告别“像素往返”<a hidden class=anchor aria-hidden=true href=#2-交互效率革命告别像素往返>#</a></h3><p>多轮交互（如“生成→编辑→再生成”）的效率取决于“理解-生成”循环的速度。这正是传统架构的症结所在。</p><table><thead><tr><th style=text-align:left>架构类型</th><th style=text-align:left>多轮交互能力</th><th style=text-align:left>核心瓶颈</th><th style=text-align:left>交互路径</th><th style=text-align:left>效率与保真度</th></tr></thead><tbody><tr><td style=text-align:left>DiT-based Models</td><td style=text-align:left>❌ 原生不支持</td><td style=text-align:left>非自回归、无状态</td><td style=text-align:left>不适用 (需完全重启)</td><td style=text-align:left>低</td></tr><tr><td style=text-align:left>混合/分离架构</td><td style=text-align:left>⚠️ 支持，但低效</td><td style=text-align:left>空间不统一</td><td style=text-align:left><code>潜码 → 像素 → 特征</code></td><td style=text-align:left>低、复杂、有信息衰减</td></tr><tr><td style=text-align:left><strong>Ming-UniVision</strong></td><td style=text-align:left>✅ <strong>原生且高效</strong></td><td style=text-align:left><strong>统一连续空间</strong></td><td style=text-align:left><strong><code>特征 → 特征</code></strong></td><td style=text-align:left><strong>高，且高保真</strong></td></tr></tbody></table><p>如上表所示，任何分离式架构都无法摆脱低效的 <code>潜码 → 像素 → 特征</code> 往返宿命。这种“像素绕行”不仅延迟巨大，还会导致上下文信息在多轮传递中不断衰减。</p><p><strong>Ming-UniVision</strong> 实现了 <strong><code>特征 → 特征</code> 的直接闭环</strong>。来自理解任务的高维特征可以直接被下一轮生成任务无缝利用，解锁了真正连贯的多模态序列建模。这使得过去需要多个专用模型才能完成的任务，如今可以在一个统一框架内自然涌现：</p><ul><li><strong>迭代式图像增强：</strong> 先执行超分辨率，然后直接在结果之上继续上色或去噪。</li><li><strong>生成式思维链：</strong> 先执行一个理解任务（如“分割出图中的汽车”），然后直接对该区域应用编辑指令。</li></ul><p><img loading=lazy src=https://mdn.alipayobjects.com/huamei_qlf8jc/afts/img/A*B3ckSaNK1cMAAAAARzAAAAgAehi-AQ/original alt="Figure 4: Multi-turn Interaction Demo">
<em>图4：“超分→上色”和“分割→编辑”等多轮任务，现在可以在一个无缝的流程中完成。</em></p><p>理解、生成与编辑，不再是孤立的管道，而是被编织在一场<strong>连续的视觉对话</strong>之中。</p><hr><h2 id=总结与展望>总结与展望<a hidden class=anchor aria-hidden=true href=#总结与展望>#</a></h2><p>我们相信，像 MingTok 这样统一、连续的视觉表征，为构建更灵活、更直观、更接近人类认知方式的多模态交互系统开辟了新的可能性。</p><p>我们深知这只是漫长探索中的一步。我们已将代码和初步的模型权重开源，希望能为社区提供一个可用的基石，并激发更多关于统一表征的讨论。我们期待与业界同仁交流学习，共同推动多模态人工智能的发展。</p><h3 id=项目链接>项目链接<a hidden class=anchor aria-hidden=true href=#项目链接>#</a></h3><ul><li><strong>GitHub:</strong> [链接地址]</li><li><strong>技术报告:</strong> [链接地址]</li><li><strong>在线 Demo:</strong> [链接地址]</li></ul></div></article></main><footer class=footer><span>&copy; 2025 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 8" fill="currentcolor"><path d="M12 8H0l6-8z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>
<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Blog | INCLUSION AI</title><meta name=keywords content><meta name=description content="Blog - INCLUSION AI"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/><link crossorigin=anonymous href=/assets/css/stylesheet.419cab9a4e041985806269ccd5fb7e29179062491b1e90454454dcf1af0c9022.css integrity="sha256-QZyrmk4EGYWAYmnM1ft+KReQYkkbHpBFRFTc8a8MkCI=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate type=application/rss+xml href=https://inclusionai.github.io/zh/blog/index.xml><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="Blog"><meta property="og:description" content="inclusionAI"><meta property="og:type" content="website"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Blog"><meta name=twitter:description content="inclusionAI"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"}]}</script></head><body class=list id=top><script>const hasHeaderBg=!0</script><header class=header><div class="nav-container nav-background"><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-entry><header class=entry-header><h2>Ming-Lite-Omni-Preview: MOEæ¶æ„çš„å¤šæ¨¡æ€å¤§æ¨¡å‹</h2></header><div class=entry-content><p>GITHUB ğŸ¤— Hugging Face | ğŸ¤– ModelScope
ç®€ä»‹ Ming-Lite-Omni-Preview æ„å»ºè‡ª Ling-Liteï¼Œå®ƒæ˜¯ä¸€ä¸ª MoEï¼ˆä¸“å®¶æ··åˆï¼‰æ¨¡å‹ï¼Œèƒ½å¤Ÿæ„ŸçŸ¥æ–‡æœ¬ã€å›¾åƒã€éŸ³é¢‘å’Œè§†é¢‘ç­‰å¤šç§æ¨¡æ€ï¼Œå¹¶ä»¥æµå¼æ–¹å¼ç”Ÿæˆæ–‡æœ¬å’Œè‡ªç„¶è¯­éŸ³ã€‚ ä¸ºäº†æ›´è‡ªç„¶åœ°å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼Œæˆ‘ä»¬å¯¹ Ling-Lite è¿›è¡Œäº†å¢å¼ºï¼Œä¸ºæ¯ç§æ¨¡æ€å¼•å…¥äº†ä¸“ç”¨è·¯ç”±æ¨¡å—ã€‚ å› æ­¤ï¼ŒMing-Omni åœ¨å¤„ç†å¤šæ¨¡æ€ä¿¡æ¯æ–¹é¢è¡¨ç°ä¼˜å¼‚ï¼Œå¹¶å…·æœ‰å¾ˆå¼ºçš„å¯æ‰©å±•æ€§ã€‚
ä¸»è¦ç‰¹æ€§ Omni and Novel MoE Architecture: ä¸€ç§åŸºäºä¸“å®¶æ··åˆï¼ˆMoEï¼‰çš„åˆ›æ–°å‹ Omni æ¶æ„ï¼Œåœ¨å¤šä¸ªå¤šæ¨¡æ€è¯„æµ‹ä¸­å–å¾—äº†é¢†å…ˆæ€§èƒ½ã€‚
Video understanding: æ”¯æŒè§†è§‰ Token çš„ KV-Cache åŠ¨æ€å‹ç¼©æœºåˆ¶ï¼Œæ—¢èƒ½ç†è§£æ•°å°æ—¶çš„é•¿è§†é¢‘ï¼Œä¹Ÿèƒ½å¯¹å‡ ç§’é’Ÿçš„çŸ­è§†é¢‘è¿›è¡Œç²¾ç»†åˆ†æã€‚
Natural Speech Generation and Fine-grained Voice Dialogue: æ”¯æŒç«¯åˆ°ç«¯å¯¹è¯ä¸­çš„æ–¹è¨€ç†è§£ä¸ç”Ÿæˆï¼Œå…·å¤‡ä¸€æ¬¡æ€§è¯­éŸ³å…‹éš†èƒ½åŠ›ï¼Œå¹¶é€šè¿‡éŸ³é¢‘åˆ†è¯å™¨å‹ç¼©æå‡è¯­è°ƒè¡¨ç°åŠ›ã€‚
è¯„æµ‹ç»“æœ Image benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct InternVL2.5-8B-MPO AI2D 83.84 83.9 84.5 HallusionBench 54.68 51.9 51.7 MMBench_TEST_V11 79.63 84.3 82.0 MMMU 57.0 58.6 54.8 MMStar 62.0 63.9 65.2 MMVet 73.6 67.1 68.1 MathVista 69.0 68.2 67.9 OCRBench 87.9 86.4 88.2 Average 70.96 70.5 70.3 Object Recognition Object Recognition Ming-Lite-Omni-Preview Qwen2.5-VL-7B InternVL-2.5-8B Plants 52.1 55.3 32.8 Animals 52.6 54.8 36.5 Home appliances & furniture 93.5 97.4 90.9 Personal Electronics 96.1 95.1 93.2 Food & Ingredients 57.5 60.0 48.7 Tableware 96.6 94.9 88.1 Vehicles 31.9 40.9 31.9 Average 68.6 71.2 60.3 Video benchmark Benchmarks Ming-Lite-Omni-Preview Qwen2.5VL-7B VideoMME wo/w sub. 63.9/67.6 65.1/71.6 MVBench 67.0 72.0 Video-MMMU 45.4 47.44 LongVideoBench 53.7 60.0 Audio benchmark SpeechQA Model AlpacaEval CommonEval SD-QA MMSU OpenBookQA IFEval AdvBench Qwen2-Audio-chat 3.69 3.40 35.35 35.43 49.01 22.57 98.85 Baichuan-Audio 4.00 3.39 49.64 48.80 63.30 41.32 86.73 GLM-4-Voice 4.06 3.48 43.31 40.11 52.97 24.91 88.08 Kimi-Audio 4.46 3.97 63.12 62.17 83.52 61.10 100.00 Qwen2.5-Omni 4.49 3.93 55.71 61.32 81.10 52.87 99.42 Ming-Lite-Omni-Preview 4.25 3.88 58.95 46.06 60.00 46.71 96.53 ASR Model Aishell-1 Aishell-2 ios Wenetspeech test-net Wenet test-meeting Librispeech test-clean Librispeech test-other Whisper Large-v3 5.14 4.76 9.68 18.54 1.9 3.65 Qwen2-Audio 1.53 3.06 7.72 8.4 1.6 3.6 GLM-4-voice Base 2.46 - - - 2.82 7.66 Baichuan-Omni-1.5 - - 6.9 8.4 - - Qwen2.5-Omni 1.18 2.36 5.9 7.7 1.8 3.4 Ming-Lite-Omni-Preview 1.62 2.82 6.23 6.9 2.34 5.74 Knowledge Model InfoSeek_H-mean InfoSeek_unseen_question InfoSeek_unseen_entity GPT-4o 36.05 - - PaLI-X 22.06 23.5 20.8 Qwen2.5-vl-32B 19.35 20.55 18.28 Ming-Lite-Omni-Preview 27.3 28.9 25.9 OCR&amp;GUI Model Ming-Lite-Omni-Preview Qwen2.5-VL-7B-Instruct ChartQA_TEST 85.2 87.3 DocVQA_TEST 93.2 95.7 OCRBenchV2_en/zh 52.2/51.6 56.3/57.2 OmniDocBenchâ†“ 34.7/34.5 30.8/39.8 TextVQA_VAL 82.36 84.9 ScreenSpot 79.3 84.7 æ¨¡å‹ä¸‹è½½ ä½ å¯ä»¥ä» Huggingface å’Œ ModelScope ä¸¤ä¸ªå¹³å°ä¸‹è½½æœ¬æ¨¡å‹ã€‚
...</p></div><footer class=entry-footer><span class=post-date title="2025-05-05 00:00:03 +0800 +0800">2025å¹´5æœˆ5æ—¥</span>
<span class=post-word-count>944 å­—</span>
<span class=post-author>inclusionAI, Ant Group</span></footer><a class=entry-link aria-label="post link to Ming-Lite-Omni-Preview: MOEæ¶æ„çš„å¤šæ¨¡æ€å¤§æ¨¡å‹" href=https://inclusionai.github.io/zh/blog/ming-lite-omni-preview/></a></article><footer class=page-footer><nav class=pagination><a class=prev href=https://inclusionai.github.io/zh/blog/page/2/>Â«&nbsp;ä¸Šä¸€é¡µ&nbsp;</a></nav></footer></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script></body></html>
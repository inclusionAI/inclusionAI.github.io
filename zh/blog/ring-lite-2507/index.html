<!doctype html><html lang=zh dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡 | INCLUSION AI</title><meta name=keywords content><meta name=description content="📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope
概述
我们推出了Ring-lite-2507，该模型是在我们之前发布的轻量级推理模型Ring-lite-2506上的一次全面升级！Ring-lite-2507是一个激活参数为2.75B，总参数为16.8B的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，Ring-lite-2507相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。
我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。
亮点

🚀 多项任务中的卓越表现: Ring-lite-2507在推理和通用任务上均表现出卓越的性能；
🔥 仅激活2.75B模型参数: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数；
⛓️‍💥 算法-系统协同设计: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性；
🔍 公开可用: 我们的训练数据和模型权重均已公开。

模型评测
我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 & 智能体，以及对齐任务。
知识理解

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          MMLU-Pro (EM)
          72.50
          63.44
          72.56
      
      
          GPQA-Diamond (Pass@1)
          69.35
          63.51
          62.00
      
      
          SuperGPQA (EM)
          40.05
          13.97
          40.36
      
      
          Phybench (Pass@1)
          28.51
          29.19
          22.14
      
  

数学

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          MATH-500 (Pass@1)
          97.95
          96.80
          97.30
      
      
          CNMO 2024 (Pass@1)
          75.09
          77.26
          74.57
      
      
          AIME 2024 (Pass@1)
          79.79
          79.00
          74.90
      
      
          AIME 2025 (Pass@1)
          72.92
          69.50
          67.19
      
      
          LiveMathBench (Pass@1)
          83.37
          85.08
          81.90
      
      
          TheoremQA (Pass@1)
          70.00
          70.19
          68.81
      
      
          OlympiadBench (math) (Pass@1)
          80.64
          82.86
          80.20
      
  

代码

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          LiveCodeBench(2408-2505) (Pass@1)
          60.35
          59.53
          55.12
      
      
          Codeforces(Percentile) (Pass@1)
          1830
          1673
          1580
      
      
          Codeforces(Rating)
          92.16
          88.00
          79.44
      
  

推理 & 智能体

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          DROP (zero-shot F1)
          89.27
          60.21
          87.13
      
      
          BBH (EM)
          88.65
          50.84
          87.30
      
      
          ARCPrize (Pass@1)
          19.00
          3.12
          3.88
      
      
          MuSR (EM)
          77.19
          66.77
          76.92
      
      
          BFCL_Live (Pass@1)
          74.81
          66.76
          75.99
      
  

对齐

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          IFEval (Prompt Strict)
          84.66
          54.34
          85.40
      
      
          AlignBench v1.1(gpt-4.1)
          80.90
          69.60
          74.70
      
      
          FoFo (gpt-4-turbo)
          85.02
          67.81
          81.93
      
      
          ArenaHard (gpt-4.1)
          88.85
          56.12
          86.14
      
  

Constrained Contextual Computation Policy Optimization(C3PO)
我们提出了一个创新性的token级强化学习训练算法，Constrained Contextual Computation Policy Optimization（C3PO），该算法旨在通过稳定训练过程中token的吞吐量，从而提升强化学习训练的鲁棒性。与样本级的筛选不同，C3PO在token级别通过采样来形成一个全局固定的token批次，每个训练步骤都保证输入给优化器的token数量一致，从而降低了梯度方差，使得训练更加稳定。"><meta name=author content="inclusionAI, Ant Group"><link rel=canonical href=https://inclusionai.github.io/zh/blog/ring-lite-2507/><link crossorigin=anonymous href=/assets/css/stylesheet.419cab9a4e041985806269ccd5fb7e29179062491b1e90454454dcf1af0c9022.css integrity="sha256-QZyrmk4EGYWAYmnM1ft+KReQYkkbHpBFRFTc8a8MkCI=" rel="preload stylesheet" as=style><link rel=icon href=https://inclusionai.github.io/favicon.png><link rel=apple-touch-icon href=https://inclusionai.github.io/favicon.png><link rel=manifest href=https://inclusionai.github.io/site.webmanifest><meta name=theme-color content="#615CED"><link rel=alternate hreflang=en href=https://inclusionai.github.io/blog/ring-lite-2507/><link rel=alternate hreflang=zh href=https://inclusionai.github.io/zh/blog/ring-lite-2507/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><script defer crossorigin=anonymous src=/js/custom.e9080c0a180dc80cf80d5fef7c857effb14f65c998e22134feb9896034b1b81a.js integrity="sha256-6QgMChgNyAz4DV/vfIV+/7FPZcmY4iE0/rmJYDSxuBo="></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.css integrity=sha384-Juol1FqnotbkyZUT5Z7gUPjQ9gzlwCENvUZTpQBAPxtusdwFLRy382PSDx5UUJ4/ crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/katex.min.js integrity=sha384-97gW6UIJxnlKemYavrqDHSX3SiygeOwIZhwyOKRfSaf0JWKRVj9hLASHgFTzT+0O crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.3/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous></script><script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-NMEMBZ8R90"></script><script>var doNotTrack=!1,dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes";if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-NMEMBZ8R90")}</script><meta property="og:title" content="欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡"><meta property="og:description" content="📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope
概述
我们推出了Ring-lite-2507，该模型是在我们之前发布的轻量级推理模型Ring-lite-2506上的一次全面升级！Ring-lite-2507是一个激活参数为2.75B，总参数为16.8B的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，Ring-lite-2507相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。
我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。
亮点

🚀 多项任务中的卓越表现: Ring-lite-2507在推理和通用任务上均表现出卓越的性能；
🔥 仅激活2.75B模型参数: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数；
⛓️‍💥 算法-系统协同设计: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性；
🔍 公开可用: 我们的训练数据和模型权重均已公开。

模型评测
我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 & 智能体，以及对齐任务。
知识理解

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          MMLU-Pro (EM)
          72.50
          63.44
          72.56
      
      
          GPQA-Diamond (Pass@1)
          69.35
          63.51
          62.00
      
      
          SuperGPQA (EM)
          40.05
          13.97
          40.36
      
      
          Phybench (Pass@1)
          28.51
          29.19
          22.14
      
  

数学

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          MATH-500 (Pass@1)
          97.95
          96.80
          97.30
      
      
          CNMO 2024 (Pass@1)
          75.09
          77.26
          74.57
      
      
          AIME 2024 (Pass@1)
          79.79
          79.00
          74.90
      
      
          AIME 2025 (Pass@1)
          72.92
          69.50
          67.19
      
      
          LiveMathBench (Pass@1)
          83.37
          85.08
          81.90
      
      
          TheoremQA (Pass@1)
          70.00
          70.19
          68.81
      
      
          OlympiadBench (math) (Pass@1)
          80.64
          82.86
          80.20
      
  

代码

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          LiveCodeBench(2408-2505) (Pass@1)
          60.35
          59.53
          55.12
      
      
          Codeforces(Percentile) (Pass@1)
          1830
          1673
          1580
      
      
          Codeforces(Rating)
          92.16
          88.00
          79.44
      
  

推理 & 智能体

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          DROP (zero-shot F1)
          89.27
          60.21
          87.13
      
      
          BBH (EM)
          88.65
          50.84
          87.30
      
      
          ARCPrize (Pass@1)
          19.00
          3.12
          3.88
      
      
          MuSR (EM)
          77.19
          66.77
          76.92
      
      
          BFCL_Live (Pass@1)
          74.81
          66.76
          75.99
      
  

对齐

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          IFEval (Prompt Strict)
          84.66
          54.34
          85.40
      
      
          AlignBench v1.1(gpt-4.1)
          80.90
          69.60
          74.70
      
      
          FoFo (gpt-4-turbo)
          85.02
          67.81
          81.93
      
      
          ArenaHard (gpt-4.1)
          88.85
          56.12
          86.14
      
  

Constrained Contextual Computation Policy Optimization(C3PO)
我们提出了一个创新性的token级强化学习训练算法，Constrained Contextual Computation Policy Optimization（C3PO），该算法旨在通过稳定训练过程中token的吞吐量，从而提升强化学习训练的鲁棒性。与样本级的筛选不同，C3PO在token级别通过采样来形成一个全局固定的token批次，每个训练步骤都保证输入给优化器的token数量一致，从而降低了梯度方差，使得训练更加稳定。"><meta property="og:type" content="article"><meta property="og:url" content="https://inclusionai.github.io/zh/blog/ring-lite-2507/"><meta property="og:image" content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-08-05T00:00:03+08:00"><meta property="article:modified_time" content="2025-08-05T00:00:03+08:00"><meta property="og:site_name" content="inclusionAI"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://inclusionai.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡"><meta name=twitter:description content="📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope
概述
我们推出了Ring-lite-2507，该模型是在我们之前发布的轻量级推理模型Ring-lite-2506上的一次全面升级！Ring-lite-2507是一个激活参数为2.75B，总参数为16.8B的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，Ring-lite-2507相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。
我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。
亮点

🚀 多项任务中的卓越表现: Ring-lite-2507在推理和通用任务上均表现出卓越的性能；
🔥 仅激活2.75B模型参数: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数；
⛓️‍💥 算法-系统协同设计: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性；
🔍 公开可用: 我们的训练数据和模型权重均已公开。

模型评测
我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 & 智能体，以及对齐任务。
知识理解

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          MMLU-Pro (EM)
          72.50
          63.44
          72.56
      
      
          GPQA-Diamond (Pass@1)
          69.35
          63.51
          62.00
      
      
          SuperGPQA (EM)
          40.05
          13.97
          40.36
      
      
          Phybench (Pass@1)
          28.51
          29.19
          22.14
      
  

数学

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          MATH-500 (Pass@1)
          97.95
          96.80
          97.30
      
      
          CNMO 2024 (Pass@1)
          75.09
          77.26
          74.57
      
      
          AIME 2024 (Pass@1)
          79.79
          79.00
          74.90
      
      
          AIME 2025 (Pass@1)
          72.92
          69.50
          67.19
      
      
          LiveMathBench (Pass@1)
          83.37
          85.08
          81.90
      
      
          TheoremQA (Pass@1)
          70.00
          70.19
          68.81
      
      
          OlympiadBench (math) (Pass@1)
          80.64
          82.86
          80.20
      
  

代码

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          LiveCodeBench(2408-2505) (Pass@1)
          60.35
          59.53
          55.12
      
      
          Codeforces(Percentile) (Pass@1)
          1830
          1673
          1580
      
      
          Codeforces(Rating)
          92.16
          88.00
          79.44
      
  

推理 & 智能体

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          DROP (zero-shot F1)
          89.27
          60.21
          87.13
      
      
          BBH (EM)
          88.65
          50.84
          87.30
      
      
          ARCPrize (Pass@1)
          19.00
          3.12
          3.88
      
      
          MuSR (EM)
          77.19
          66.77
          76.92
      
      
          BFCL_Live (Pass@1)
          74.81
          66.76
          75.99
      
  

对齐

  
      
          Benchmark
          Ring-lite-2507
          Ring-lite-2506
          Qwen3-8B-Thinking
      
  
  
      
          IFEval (Prompt Strict)
          84.66
          54.34
          85.40
      
      
          AlignBench v1.1(gpt-4.1)
          80.90
          69.60
          74.70
      
      
          FoFo (gpt-4-turbo)
          85.02
          67.81
          81.93
      
      
          ArenaHard (gpt-4.1)
          88.85
          56.12
          86.14
      
  

Constrained Contextual Computation Policy Optimization(C3PO)
我们提出了一个创新性的token级强化学习训练算法，Constrained Contextual Computation Policy Optimization（C3PO），该算法旨在通过稳定训练过程中token的吞吐量，从而提升强化学习训练的鲁棒性。与样本级的筛选不同，C3PO在token级别通过采样来形成一个全局固定的token批次，每个训练步骤都保证输入给优化器的token数量一致，从而降低了梯度方差，使得训练更加稳定。"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blog","item":"https://inclusionai.github.io/zh/blog/"},{"@type":"ListItem","position":2,"name":"欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡","item":"https://inclusionai.github.io/zh/blog/ring-lite-2507/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡","name":"欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡","description":"📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope\n概述 我们推出了Ring-lite-2507，该模型是在我们之前发布的轻量级推理模型Ring-lite-2506上的一次全面升级！Ring-lite-2507是一个激活参数为2.75B，总参数为16.8B的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，Ring-lite-2507相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。\n我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。\n亮点\n🚀 多项任务中的卓越表现: Ring-lite-2507在推理和通用任务上均表现出卓越的性能； 🔥 仅激活2.75B模型参数: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数； ⛓️‍💥 算法-系统协同设计: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性； 🔍 公开可用: 我们的训练数据和模型权重均已公开。 模型评测 我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 \u0026amp; 智能体，以及对齐任务。\n知识理解 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MMLU-Pro (EM) 72.50 63.44 72.56 GPQA-Diamond (Pass@1) 69.35 63.51 62.00 SuperGPQA (EM) 40.05 13.97 40.36 Phybench (Pass@1) 28.51 29.19 22.14 数学 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MATH-500 (Pass@1) 97.95 96.80 97.30 CNMO 2024 (Pass@1) 75.09 77.26 74.57 AIME 2024 (Pass@1) 79.79 79.00 74.90 AIME 2025 (Pass@1) 72.92 69.50 67.19 LiveMathBench (Pass@1) 83.37 85.08 81.90 TheoremQA (Pass@1) 70.00 70.19 68.81 OlympiadBench (math) (Pass@1) 80.64 82.86 80.20 代码 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking LiveCodeBench(2408-2505) (Pass@1) 60.35 59.53 55.12 Codeforces(Percentile) (Pass@1) 1830 1673 1580 Codeforces(Rating) 92.16 88.00 79.44 推理 \u0026amp; 智能体 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking DROP (zero-shot F1) 89.27 60.21 87.13 BBH (EM) 88.65 50.84 87.30 ARCPrize (Pass@1) 19.00 3.12 3.88 MuSR (EM) 77.19 66.77 76.92 BFCL_Live (Pass@1) 74.81 66.76 75.99 对齐 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking IFEval (Prompt Strict) 84.66 54.34 85.40 AlignBench v1.1(gpt-4.1) 80.90 69.60 74.70 FoFo (gpt-4-turbo) 85.02 67.81 81.93 ArenaHard (gpt-4.1) 88.85 56.12 86.14 Constrained Contextual Computation Policy Optimization(C3PO) 我们提出了一个创新性的token级强化学习训练算法，Constrained Contextual Computation Policy Optimization（C3PO），该算法旨在通过稳定训练过程中token的吞吐量，从而提升强化学习训练的鲁棒性。与样本级的筛选不同，C3PO在token级别通过采样来形成一个全局固定的token批次，每个训练步骤都保证输入给优化器的token数量一致，从而降低了梯度方差，使得训练更加稳定。\n","keywords":[],"articleBody":"📖 Technical Report | 🤗 Hugging Face｜ 🤖 ModelScope\n概述 我们推出了Ring-lite-2507，该模型是在我们之前发布的轻量级推理模型Ring-lite-2506上的一次全面升级！Ring-lite-2507是一个激活参数为2.75B，总参数为16.8B的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，Ring-lite-2507相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。\n我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。\n亮点\n🚀 多项任务中的卓越表现: Ring-lite-2507在推理和通用任务上均表现出卓越的性能； 🔥 仅激活2.75B模型参数: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数； ⛓️‍💥 算法-系统协同设计: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性； 🔍 公开可用: 我们的训练数据和模型权重均已公开。 模型评测 我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 \u0026 智能体，以及对齐任务。\n知识理解 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MMLU-Pro (EM) 72.50 63.44 72.56 GPQA-Diamond (Pass@1) 69.35 63.51 62.00 SuperGPQA (EM) 40.05 13.97 40.36 Phybench (Pass@1) 28.51 29.19 22.14 数学 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking MATH-500 (Pass@1) 97.95 96.80 97.30 CNMO 2024 (Pass@1) 75.09 77.26 74.57 AIME 2024 (Pass@1) 79.79 79.00 74.90 AIME 2025 (Pass@1) 72.92 69.50 67.19 LiveMathBench (Pass@1) 83.37 85.08 81.90 TheoremQA (Pass@1) 70.00 70.19 68.81 OlympiadBench (math) (Pass@1) 80.64 82.86 80.20 代码 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking LiveCodeBench(2408-2505) (Pass@1) 60.35 59.53 55.12 Codeforces(Percentile) (Pass@1) 1830 1673 1580 Codeforces(Rating) 92.16 88.00 79.44 推理 \u0026 智能体 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking DROP (zero-shot F1) 89.27 60.21 87.13 BBH (EM) 88.65 50.84 87.30 ARCPrize (Pass@1) 19.00 3.12 3.88 MuSR (EM) 77.19 66.77 76.92 BFCL_Live (Pass@1) 74.81 66.76 75.99 对齐 Benchmark Ring-lite-2507 Ring-lite-2506 Qwen3-8B-Thinking IFEval (Prompt Strict) 84.66 54.34 85.40 AlignBench v1.1(gpt-4.1) 80.90 69.60 74.70 FoFo (gpt-4-turbo) 85.02 67.81 81.93 ArenaHard (gpt-4.1) 88.85 56.12 86.14 Constrained Contextual Computation Policy Optimization(C3PO) 我们提出了一个创新性的token级强化学习训练算法，Constrained Contextual Computation Policy Optimization（C3PO），该算法旨在通过稳定训练过程中token的吞吐量，从而提升强化学习训练的鲁棒性。与样本级的筛选不同，C3PO在token级别通过采样来形成一个全局固定的token批次，每个训练步骤都保证输入给优化器的token数量一致，从而降低了梯度方差，使得训练更加稳定。\nC3PO\n在蒸馏和强化学习之间平衡token效率 尽管蒸馏训练在很多情况下表现出优秀的性能，但我们发现，与RL训练相比，它通常需要更多的训练迭代才能达到相当的性能。此外，我们观察到，蒸馏模型的训练迭代次数会显著影响RL训练中熵损失的走势，进而影响了RL的探索空间。从实验上来看，模型经过过多的SFT训练轮次可能会导致RL熵迅速崩溃，另一方面，SFT阶段训练不足也不可避免地会导致性能下降。为了系统性地量化最优SFT训练的程度，我们提出采用token效率指标来动态指导更适合强化学习的起始模型。\n训练数据 为构建一个高质量的强化学习训练数据集，我们搭建了一个全面而细致的数据处理链路。这个链路包括多个关键阶段，如数据清洗、答案验证和数据标注等，从而确保我们的训练数据免于数据污染，同时具有信息量，更适用于强化学习训练。\nData Pipeline\n训练流程 Training Pipeline\n推理强化学习 与之前发布的Ring-lite-2506相比，我们通过整合更具挑战性的数学、代码和科学数据来扩展了我们的推理数据集。具体来说，我们采用了67K个数学问题、32K个代码问题和9.9K个科学问题用于推理强化学习训练。此外，我们还通过引入超过19K个逻辑游戏，如ARC-AGI、Countdown、数独、AlphaMaze等，来进一步扩充我们的推理数据集。对于每种类型的问题，我们专门设计了适合的奖励函数，以确保我们的训练数据均是可被验证的。\n通用强化学习 除了推理任务之外，我们的Ring-lite-2507在训练过程中广泛补充了用于RL训练的通用数据集。我们的通用RL训练不仅没有牺牲在推理任务上的强劲表现，同时，它在广泛的通用榜单评测中均展现了强大的文本理解以及通用任务能力。\n我们的通用强化学习训练数据中包括了指令遵循、问答、文本摘要等多个不同任务。对于开放式问题，我们采用了一个通用奖励模型来计算奖励分数。此外，我们还集成了一个基于规则的验证器来处理易于验证的通用问题，例如指令遵循任务等。\n引用 @misc{lingteam2025ringlitescalablereasoningc3postabilized, title={Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs}, author={Ling Team and Bin Hu and Cai Chen and Deng Zhao and Ding Liu and Dingnan Jin and Feng Zhu and Hao Dai and Hongzhi Luan and Jia Guo and Jiaming Liu and Jiewei Wu and Jun Mei and Jun Zhou and Junbo Zhao and Junwu Xiong and Kaihong Zhang and Kuan Xu and Lei Liang and Liang Jiang and Liangcheng Fu and Longfei Zheng and Qiang Gao and Qing Cui and Quan Wan and Shaomian Zheng and Shuaicheng Li and Tongkai Yang and Wang Ren and Xiaodong Yan and Xiaopei Wan and Xiaoyun Feng and Xin Zhao and Xinxing Yang and Xinyu Kong and Xuemin Yang and Yang Li and Yingting Wu and Yongkang Liu and Zhankai Xu and Zhenduo Zhang and Zhenglei Zhou and Zhenyu Huang and Zhiqiang Zhang and Zihao Wang and Zujie Wen}, year={2025}, eprint={2506.14731}, archivePrefix={arXiv}, primaryClass={cs.CL}, url={https://arxiv.org/abs/2506.14731}, } ","wordCount":"359","inLanguage":"zh","datePublished":"2025-08-05T00:00:03+08:00","dateModified":"2025-08-05T00:00:03+08:00","author":{"@type":"Person","name":"inclusionAI, Ant Group"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://inclusionai.github.io/zh/blog/ring-lite-2507/"},"publisher":{"@type":"Organization","name":"INCLUSION AI","logo":{"@type":"ImageObject","url":"https://inclusionai.github.io/favicon.png"}}}</script></head><body id=top><script>const hasHeaderBg=!1</script><header class=header><div class=nav-container><nav class=nav><div class=logo><a href=/ accesskey=h title="inclusionAI (Alt + H)"><img src=https://inclusionai.github.io/img/logo_head.png alt aria-label=logo height=50></a></div><ul id=menu><li><a href=/blog/ title=Blog><span>Blog</span></a></li><li><a href=/publication title=Publication><span>Publication</span></a></li><li><a href=/about title=About><span>About</span></a></li><li><a href=https://github.com/inclusionAI title="Try Ling & Ming"><span>Try Ling & Ming</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li></ul></nav></div></header><div class=hero-container></div><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=/>Home</a>&nbsp;»&nbsp;<a href=https://inclusionai.github.io/zh/blog/>Blog</a></div><h1 class=post-title>欢迎我们的新成员—Ring-lite！它推理更有深度，能力更均衡</h1><div class=post-meta><span class=post-date title="2025-08-05 00:00:03 +0800 +0800">2025年8月5日</span>
<span class=post-word-count>359 字</span>
<span class=post-author>inclusionAI, Ant Group</span>
&nbsp;|&nbsp;语言:<ul class=i18n_list><li><a href=https://inclusionai.github.io/blog/ring-lite-2507/>English</a></li></ul></div></header><div class=post-content><p>📖 <a href=https://arxiv.org/abs/2506.14731>Technical Report</a> | 🤗 <a href=https://huggingface.co/inclusionAI/Ring-lite-2507>Hugging Face</a>｜ 🤖 <a href=https://modelscope.cn/models/inclusionAI/Ring-lite-2507>ModelScope</a></p><h2 id=概述>概述<a hidden class=anchor aria-hidden=true href=#概述>#</a></h2><p>我们推出了<strong>Ring-lite-2507</strong>，该模型是在我们之前发布的轻量级推理模型<strong>Ring-lite-2506</strong>上的一次全面升级！<strong>Ring-lite-2507</strong>是一个激活参数为<strong>2.75B</strong>，总参数为<strong>16.8B</strong>的MoE大语言模型。此次升级，我们的模型不仅进一步提升了在数学、代码和逻辑任务上的推理能力，同时在知识理解、对齐和智能体等多个广泛使用的通用类评测榜单中取得了卓越的表现。通过我们提出的创新性强化学习算法和多阶段强化学习训练流程，<strong>Ring-lite-2507</strong>相比最新10B参数以下的Dense推理模型，在仅激活其1/3参数规模的情况下，在各项任务中达到了相当或更具竞争力的性能。</p><p>我们提出了一种创新的强化学习训练算法，即Constrained Contextual Computation Policy Optimization（C3PO），旨在解决MoE强化学习训练过程中的不稳定性问题。通过算法-系统协同设计，我们的方法同时提高了训练稳定性和计算吞吐量。此外，我们系统性地研究了长思维链SFT和RL训练之间的动态关系，并提出使用token效率指标来帮助我们探索选择更适合RL训练的微调模型，从而实现了RL训练过程中的性能和效率的双平衡。此外，我们还采用了新型两阶段强化学习的训练范式，以平衡多领域融合数据的训练效果，在增强推理能力的同时，更有效地提升各种下游通用任务的表现。</p><p><strong>亮点</strong></p><ul><li>🚀 <strong>多项任务中的卓越表现</strong>: Ring-lite-2507在推理和通用任务上均表现出卓越的性能；</li><li>🔥 <strong>仅激活2.75B模型参数</strong>: Ring-lite-2507是一个基于MoE的大语言模型，仅激活了2.75B模型参数；</li><li>⛓️‍💥 <strong>算法-系统协同设计</strong>: 我们创新性地提出了C3PO训练方法，并采用token效率来平衡RL训练的稳定性和有效性；</li><li>🔍 <strong>公开可用</strong>: 我们的训练数据和模型权重均已公开。</li></ul><h2 id=模型评测>模型评测<a hidden class=anchor aria-hidden=true href=#模型评测>#</a></h2><p>我们在两个主要领域对模型进行了全面评估：推理和通用。我们使用了一系列公开评测榜单来衡量模型能力，包括：知识理解、数学、代码、推理 & 智能体，以及对齐任务。</p><h3 id=知识理解>知识理解<a hidden class=anchor aria-hidden=true href=#知识理解>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>MMLU-Pro (EM)</td><td style=text-align:center>72.50</td><td style=text-align:center>63.44</td><td style=text-align:center><strong>72.56</strong></td></tr><tr><td style=text-align:center>GPQA-Diamond (Pass@1)</td><td style=text-align:center><strong>69.35</strong></td><td style=text-align:center>63.51</td><td style=text-align:center>62.00</td></tr><tr><td style=text-align:center>SuperGPQA (EM)</td><td style=text-align:center>40.05</td><td style=text-align:center>13.97</td><td style=text-align:center><strong>40.36</strong></td></tr><tr><td style=text-align:center>Phybench (Pass@1)</td><td style=text-align:center>28.51</td><td style=text-align:center><strong>29.19</strong></td><td style=text-align:center>22.14</td></tr></tbody></table><h3 id=数学>数学<a hidden class=anchor aria-hidden=true href=#数学>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>MATH-500 (Pass@1)</td><td style=text-align:center><strong>97.95</strong></td><td style=text-align:center>96.80</td><td style=text-align:center>97.30</td></tr><tr><td style=text-align:center>CNMO 2024 (Pass@1)</td><td style=text-align:center>75.09</td><td style=text-align:center><strong>77.26</strong></td><td style=text-align:center>74.57</td></tr><tr><td style=text-align:center>AIME 2024 (Pass@1)</td><td style=text-align:center><strong>79.79</strong></td><td style=text-align:center>79.00</td><td style=text-align:center>74.90</td></tr><tr><td style=text-align:center>AIME 2025 (Pass@1)</td><td style=text-align:center><strong>72.92</strong></td><td style=text-align:center>69.50</td><td style=text-align:center>67.19</td></tr><tr><td style=text-align:center>LiveMathBench (Pass@1)</td><td style=text-align:center>83.37</td><td style=text-align:center><strong>85.08</strong></td><td style=text-align:center>81.90</td></tr><tr><td style=text-align:center>TheoremQA (Pass@1)</td><td style=text-align:center>70.00</td><td style=text-align:center><strong>70.19</strong></td><td style=text-align:center>68.81</td></tr><tr><td style=text-align:center>OlympiadBench (math) (Pass@1)</td><td style=text-align:center>80.64</td><td style=text-align:center><strong>82.86</strong></td><td style=text-align:center>80.20</td></tr></tbody></table><h3 id=代码>代码<a hidden class=anchor aria-hidden=true href=#代码>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>LiveCodeBench(2408-2505) (Pass@1)</td><td style=text-align:center><strong>60.35</strong></td><td style=text-align:center>59.53</td><td style=text-align:center>55.12</td></tr><tr><td style=text-align:center>Codeforces(Percentile) (Pass@1)</td><td style=text-align:center><strong>1830</strong></td><td style=text-align:center>1673</td><td style=text-align:center>1580</td></tr><tr><td style=text-align:center>Codeforces(Rating)</td><td style=text-align:center><strong>92.16</strong></td><td style=text-align:center>88.00</td><td style=text-align:center>79.44</td></tr></tbody></table><h3 id=推理--智能体>推理 & 智能体<a hidden class=anchor aria-hidden=true href=#推理--智能体>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>DROP (zero-shot F1)</td><td style=text-align:center><strong>89.27</strong></td><td style=text-align:center>60.21</td><td style=text-align:center>87.13</td></tr><tr><td style=text-align:center>BBH (EM)</td><td style=text-align:center><strong>88.65</strong></td><td style=text-align:center>50.84</td><td style=text-align:center>87.30</td></tr><tr><td style=text-align:center>ARCPrize (Pass@1)</td><td style=text-align:center><strong>19.00</strong></td><td style=text-align:center>3.12</td><td style=text-align:center>3.88</td></tr><tr><td style=text-align:center>MuSR (EM)</td><td style=text-align:center><strong>77.19</strong></td><td style=text-align:center>66.77</td><td style=text-align:center>76.92</td></tr><tr><td style=text-align:center>BFCL_Live (Pass@1)</td><td style=text-align:center>74.81</td><td style=text-align:center>66.76</td><td style=text-align:center><strong>75.99</strong></td></tr></tbody></table><h3 id=对齐>对齐<a hidden class=anchor aria-hidden=true href=#对齐>#</a></h3><table><thead><tr><th style=text-align:center><strong>Benchmark</strong></th><th style=text-align:center><strong>Ring-lite-2507</strong></th><th style=text-align:center><strong>Ring-lite-2506</strong></th><th style=text-align:center><strong>Qwen3-8B-Thinking</strong></th></tr></thead><tbody><tr><td style=text-align:center>IFEval (Prompt Strict)</td><td style=text-align:center>84.66</td><td style=text-align:center>54.34</td><td style=text-align:center><strong>85.40</strong></td></tr><tr><td style=text-align:center>AlignBench v1.1(gpt-4.1)</td><td style=text-align:center><strong>80.90</strong></td><td style=text-align:center>69.60</td><td style=text-align:center>74.70</td></tr><tr><td style=text-align:center>FoFo (gpt-4-turbo)</td><td style=text-align:center><strong>85.02</strong></td><td style=text-align:center>67.81</td><td style=text-align:center>81.93</td></tr><tr><td style=text-align:center>ArenaHard (gpt-4.1)</td><td style=text-align:center><strong>88.85</strong></td><td style=text-align:center>56.12</td><td style=text-align:center>86.14</td></tr></tbody></table><h2 id=constrained-contextual-computation-policy-optimizationc3po>Constrained Contextual Computation Policy Optimization(C3PO)<a hidden class=anchor aria-hidden=true href=#constrained-contextual-computation-policy-optimizationc3po>#</a></h2><p>我们提出了一个创新性的token级强化学习训练算法，<u>C</u>onstrained <u>C</u>ontextual <u>C</u>omputation <u>P</u>olicy <u>O</u>ptimization（C3PO），该算法旨在通过稳定训练过程中token的吞吐量，从而提升强化学习训练的鲁棒性。与样本级的筛选不同，C3PO在token级别通过采样来形成一个全局固定的token批次，每个训练步骤都保证输入给优化器的token数量一致，从而降低了梯度方差，使得训练更加稳定。</p><div style=text-align:center;margin:auto;width:100%><img src=./assets/C3PO_overview_formal.png alt="Image description"><p style=font-size:14px;color:gray>C3PO</p></div><h2 id=在蒸馏和强化学习之间平衡token效率>在蒸馏和强化学习之间平衡token效率<a hidden class=anchor aria-hidden=true href=#在蒸馏和强化学习之间平衡token效率>#</a></h2><p>尽管蒸馏训练在很多情况下表现出优秀的性能，但我们发现，与RL训练相比，它通常需要更多的训练迭代才能达到相当的性能。此外，我们观察到，蒸馏模型的训练迭代次数会显著影响RL训练中熵损失的走势，进而影响了RL的探索空间。从实验上来看，模型经过过多的SFT训练轮次可能会导致RL熵迅速崩溃，另一方面，SFT阶段训练不足也不可避免地会导致性能下降。为了系统性地量化最优SFT训练的程度，我们提出采用token效率指标来动态指导更适合强化学习的起始模型。</p><h2 id=训练数据>训练数据<a hidden class=anchor aria-hidden=true href=#训练数据>#</a></h2><p>为构建一个高质量的强化学习训练数据集，我们搭建了一个全面而细致的数据处理链路。这个链路包括多个关键阶段，如数据清洗、答案验证和数据标注等，从而确保我们的训练数据免于数据污染，同时具有信息量，更适用于强化学习训练。</p><div style=text-align:center;margin:auto;width:100%><img src=./assets/data-pipeline.png alt="Image description"><p style=font-size:14px;color:gray>Data Pipeline</p></div><h2 id=训练流程>训练流程<a hidden class=anchor aria-hidden=true href=#训练流程>#</a></h2><div style=text-align:center;margin:auto;width:100%><img src=./assets/0731-pipeline.png alt="Image description"><p style=font-size:14px;color:gray>Training Pipeline</p></div><h3 id=推理强化学习>推理强化学习<a hidden class=anchor aria-hidden=true href=#推理强化学习>#</a></h3><p>与之前发布的Ring-lite-2506相比，我们通过整合更具挑战性的数学、代码和科学数据来扩展了我们的推理数据集。具体来说，我们采用了67K个数学问题、32K个代码问题和9.9K个科学问题用于推理强化学习训练。此外，我们还通过引入超过19K个逻辑游戏，如ARC-AGI、Countdown、数独、AlphaMaze等，来进一步扩充我们的推理数据集。对于每种类型的问题，我们专门设计了适合的奖励函数，以确保我们的训练数据均是可被验证的。</p><h3 id=通用强化学习>通用强化学习<a hidden class=anchor aria-hidden=true href=#通用强化学习>#</a></h3><p>除了推理任务之外，我们的Ring-lite-2507在训练过程中广泛补充了用于RL训练的通用数据集。我们的通用RL训练不仅没有牺牲在推理任务上的强劲表现，同时，它在广泛的通用榜单评测中均展现了强大的文本理解以及通用任务能力。</p><p>我们的通用强化学习训练数据中包括了指令遵循、问答、文本摘要等多个不同任务。对于开放式问题，我们采用了一个通用奖励模型来计算奖励分数。此外，我们还集成了一个基于规则的验证器来处理易于验证的通用问题，例如指令遵循任务等。</p><h2 id=引用>引用<a hidden class=anchor aria-hidden=true href=#引用>#</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-bibtex data-lang=bibtex><span class=line><span class=cl><span class=nc>@misc</span><span class=p>{</span><span class=nl>lingteam2025ringlitescalablereasoningc3postabilized</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>title</span><span class=p>=</span><span class=s>{Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning for LLMs}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>      <span class=na>author</span><span class=p>=</span><span class=s>{Ling Team and Bin Hu and Cai Chen and Deng Zhao and Ding Liu and Dingnan Jin and Feng Zhu and Hao Dai and Hongzhi Luan and Jia Guo and Jiaming Liu and Jiewei Wu and Jun Mei and Jun Zhou and Junbo Zhao and Junwu Xiong and Kaihong Zhang and Kuan Xu and Lei Liang and Liang Jiang and Liangcheng Fu and Longfei Zheng and Qiang Gao and Qing Cui and Quan Wan and Shaomian Zheng and Shuaicheng Li and Tongkai Yang and Wang Ren and Xiaodong Yan and Xiaopei Wan and Xiaoyun Feng and Xin Zhao and Xinxing Yang and Xinyu Kong and Xuemin Yang and Yang Li and Yingting Wu and Yongkang Liu and Zhankai Xu and Zhenduo Zhang and Zhenglei Zhou and Zhenyu Huang and Zhiqiang Zhang and Zihao Wang and Zujie Wen}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>year</span><span class=p>=</span><span class=s>{2025}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>eprint</span><span class=p>=</span><span class=s>{2506.14731}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>archivePrefix</span><span class=p>=</span><span class=s>{arXiv}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>primaryClass</span><span class=p>=</span><span class=s>{cs.CL}</span><span class=p>,</span>
</span></span><span class=line><span class=cl>      <span class=na>url</span><span class=p>=</span><span class=s>{https://arxiv.org/abs/2506.14731}</span><span class=p>,</span> 
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div></div></article></main><footer class=footer><span>&copy; 2026 <a href=https://inclusionai.github.io/zh/>INCLUSION AI</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 8" fill="currentColor"><path d="M12 8H0l6-8z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")},mybutton.oncontextmenu=e=>{e.preventDefault(),document.querySelectorAll(".example-container").forEach(e=>{e.style.backgroundColor="unset"}),document.querySelectorAll(".example-content").forEach(e=>{e.style.display="block",e.style.backgroundColor="var(--code-bg)",e.style.marginBottom="var(--modal-gap)",e.classList.remove("scroll")}),document.querySelectorAll(".next-button").forEach(e=>{e.style.display="none"})}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="复制";function s(){t.innerHTML="已复制！",setTimeout(()=>{t.innerHTML="复制"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>